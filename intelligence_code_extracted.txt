================================================================================
INTELLIGENCE FOLDER CODE EXTRACTION
================================================================================
Extracted on: 2025-06-20 14:18:16
Source directory: /Users/oudiantebi/Session42 Dropbox/Oudi Antebi/Mac (3)/Documents/MyCode/COS1/NEW-COS/intelligence
================================================================================


================================================================================
FILE: intelligence/__init__.py
SIZE: 65 bytes
LINES: 2
================================================================================

# intelligence/__init__.py
# Intelligence package initialization

================================================================================
FILE: intelligence/claude_analysis.py
SIZE: 75,054 bytes
LINES: 1,586
================================================================================

# File: intelligence/claude_analysis.py
"""
Specialized Claude Opus 4 Analysts for Knowledge Tree Construction
================================================================
Multiple specialized AI analysts that process emails in parallel
"""

import asyncio
import anthropic
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import json
import logging
from config.settings import ANTHROPIC_API_KEY

logger = logging.getLogger(__name__)

@dataclass
class AnalysisResult:
    """Result from a specialized Claude analyst"""
    analyst_type: str
    insights: Dict
    confidence: float
    evidence: List[Dict]
    relationships: List[Dict]
    topics: List[str]
    entities: List[Dict]

class BaseClaudeAnalyst:
    """Base class for specialized Claude analysts"""
    
    def __init__(self, analyst_type: str, model: str = None):
        self.analyst_type = analyst_type
        self.model = model or "claude-3-opus-20240229"
        self.client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        
    async def analyze_emails(self, emails: List[Dict], context: Dict = None) -> AnalysisResult:
        """Analyze a batch of emails with specialized focus"""
        raise NotImplementedError("Subclasses must implement analyze_emails")
    
    def _prepare_email_context(self, emails: List[Dict]) -> str:
        """Prepare email context for Claude"""
        context_parts = []
        for email in emails[:50]:  # Limit to prevent token overflow
            context_parts.append(f"""
Email ID: {email.get('id')}
Date: {email.get('email_date')}
From: {email.get('sender')} 
To: {email.get('recipients')}
Subject: {email.get('subject')}
Body: {email.get('body_text', '')[:1000]}...
---
""")
        return "\n".join(context_parts)

class BusinessStrategyAnalyst(BaseClaudeAnalyst):
    """Analyzes emails for strategic business decisions and rationale"""
    
    def __init__(self):
        super().__init__("business_strategy")
        self.analysis_prompt = """You are an expert in strategic thinking and business worldview modeling. 

Your task is NOT to simply categorize business information, but to understand HOW this person thinks about business strategy and what their unique strategic philosophy reveals about their worldview.

Analyze these emails to understand:

1. STRATEGIC MENTAL MODELS: What frameworks do they use to think about business decisions?
2. VALUE CREATION PHILOSOPHY: How do they conceptualize value creation and business success?
3. RISK AND OPPORTUNITY LENS: How do they perceive and evaluate risks vs opportunities?
4. DECISION ARCHITECTURE: What factors consistently drive their strategic decisions?
5. COMPETITIVE WORLDVIEW: How do they think about competition, differentiation, and market positioning?
6. STAKEHOLDER PHILOSOPHY: How do they conceptualize relationships with customers, partners, employees?

Focus on understanding their UNIQUE perspective on business - the mental models and frameworks that make them distinctive. Look for the "why behind the why" in their strategic thinking.

For each insight, provide:
- The underlying belief or framework driving their approach
- How this manifests in their communications and decisions  
- What this reveals about their unique worldview
- Evidence from specific emails
- Confidence level and strategic significance

This should feel revelatory - like you understand their strategic DNA in a way that even they might find insightful."""

    async def analyze_emails(self, emails: List[Dict], context: Dict = None) -> AnalysisResult:
        """Extract strategic business intelligence"""
        try:
            email_context = self._prepare_email_context(emails)
            
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.model,
                max_tokens=4000,
                temperature=0.3,
                messages=[{
                    "role": "user",
                    "content": f"{self.analysis_prompt}\n\nEmails to analyze:\n{email_context}"
                }]
            )
            
            # Parse Claude's response
            result = self._parse_analysis_response(response.content[0].text)
            
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights=result,
                confidence=0.85,
                evidence=self._extract_evidence(result, emails),
                relationships=result.get('key_relationships', []),
                topics=self._extract_topics(result),
                entities=self._extract_entities(result)
            )
            
        except Exception as e:
            logger.error(f"Business strategy analysis error: {str(e)}")
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights={},
                confidence=0.0,
                evidence=[],
                relationships=[],
                topics=[],
                entities=[]
            )
    
    def _parse_analysis_response(self, response_text: str) -> Dict:
        """Parse Claude's JSON response"""
        try:
            # Extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            
            # If no JSON, create structured response from text
            return {
                "strategic_mental_models": self._extract_section(response_text, "STRATEGIC MENTAL MODELS"),
                "value_creation_philosophy": self._extract_section(response_text, "VALUE CREATION PHILOSOPHY"),
                "risk_opportunity_lens": self._extract_section(response_text, "RISK AND OPPORTUNITY LENS"),
                "decision_architecture": self._extract_section(response_text, "DECISION ARCHITECTURE"),
                "competitive_worldview": self._extract_section(response_text, "COMPETITIVE WORLDVIEW"),
                "stakeholder_philosophy": self._extract_section(response_text, "STAKEHOLDER PHILOSOPHY"),
                "raw_analysis": response_text
            }
        except Exception as e:
            logger.error(f"Failed to parse business analysis response: {str(e)}")
            return {"raw_analysis": response_text}
    
    def _extract_section(self, text: str, section_name: str) -> str:
        """Extract a specific section from the response"""
        lines = text.split('\n')
        in_section = False
        section_content = []
        
        for line in lines:
            if section_name.lower() in line.lower():
                in_section = True
                continue
            elif in_section and any(keyword in line.lower() for keyword in ['mental models', 'philosophy', 'lens', 'architecture', 'worldview']):
                break
            elif in_section:
                section_content.append(line)
        
        return '\n'.join(section_content).strip()
    
    def _extract_evidence(self, insights: Dict, emails: List[Dict]) -> List[Dict]:
        """Extract supporting evidence from emails"""
        evidence = []
        
        # Extract key terms from insights for searching
        search_terms = []
        for category, content in insights.items():
            if isinstance(content, str):
                # Extract significant words (length > 3, not common words)
                words = [word.strip('.,!?;:').lower() for word in content.split() 
                        if len(word) > 3 and word.lower() not in ['this', 'that', 'with', 'from', 'they', 'them', 'will', 'have', 'been', 'were', 'was']]
                search_terms.extend(words[:5])  # Top 5 words per category
        
        # Search for evidence in emails
        for email in emails[:20]:  # Check first 20 emails for performance
            email_content = f"{email.get('subject', '')} {email.get('body_text', '')}"
            email_content_lower = email_content.lower()
            
            # Count matches
            matches = sum(1 for term in search_terms if term in email_content_lower)
            
            if matches > 0:
                # Extract relevant excerpt
                sentences = email_content.split('.')
                relevant_sentences = []
                for sentence in sentences:
                    if any(term in sentence.lower() for term in search_terms):
                        relevant_sentences.append(sentence.strip())
                        if len(relevant_sentences) >= 2:  # Max 2 sentences
                            break
                
                evidence.append({
                    'email_id': email.get('id'),
                    'date': email.get('email_date'),
                    'sender': email.get('sender'),
                    'subject': email.get('subject'),
                    'excerpt': '. '.join(relevant_sentences)[:200] + '...' if len('. '.join(relevant_sentences)) > 200 else '. '.join(relevant_sentences),
                    'relevance_score': matches / len(search_terms) if search_terms else 0,
                    'match_count': matches
                })
        
        # Sort by relevance and return top 5
        evidence.sort(key=lambda x: x['relevance_score'], reverse=True)
        return evidence[:5]
    
    def _extract_topics(self, insights: Dict) -> List[str]:
        """Extract key topics from insights"""
        topics = set()
        
        for category, content in insights.items():
            # Add category as topic
            topics.add(category.replace('_', ' ').title())
            
            if isinstance(content, str):
                # Extract potential topics from content
                words = content.split()
                for i, word in enumerate(words):
                    word = word.strip('.,!?;:').lower()
                    if len(word) > 4:  # Longer words more likely to be topics
                        topics.add(word.title())
                    
                    # Look for capitalized words/phrases that might be topics
                    if word[0].isupper() and len(word) > 3:
                        topics.add(word)
                        
                    # Look for phrases (adjacent capitalized words)
                    if i < len(words) - 1:
                        next_word = words[i + 1].strip('.,!?;:')
                        if word[0].isupper() and next_word and next_word[0].isupper():
                            topics.add(f"{word} {next_word}")
            
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        for key, value in item.items():
                            topics.add(key.replace('_', ' ').title())
                            if isinstance(value, str) and len(value.split()) <= 3:
                                topics.add(value)
                    elif isinstance(item, str):
                        topics.add(item)
        
        # Filter out very common words and return list
        common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among'}
        filtered_topics = [topic for topic in topics if topic.lower() not in common_words and len(topic) > 2]
        
        return list(set(filtered_topics))[:20]  # Return top 20 unique topics
    
    def _extract_entities(self, insights: Dict) -> List[Dict]:
        """Extract entities (people, companies, projects) from insights"""
        entities = []
        
        for category, content in insights.items():
            if isinstance(content, str):
                # Look for patterns that might be entities
                words = content.split()
                
                for i, word in enumerate(words):
                    word_clean = word.strip('.,!?;:')
                    
                    # Look for capitalized words (potential proper nouns)
                    if word_clean and word_clean[0].isupper() and len(word_clean) > 2:
                        entity_type = "unknown"
                        
                        # Try to classify entity type based on context
                        context = ' '.join(words[max(0, i-2):i+3]).lower()
                        
                        if any(indicator in context for indicator in ['company', 'corp', 'inc', 'ltd', 'llc', 'organization']):
                            entity_type = "company"
                        elif any(indicator in context for indicator in ['project', 'initiative', 'program', 'system', 'platform']):
                            entity_type = "project"
                        elif any(indicator in context for indicator in ['person', 'team', 'manager', 'director', 'ceo', 'cto']):
                            entity_type = "person"
                        elif word_clean.endswith(('.com', '.org', '.net', '.io')):
                            entity_type = "website"
                        elif '@' in word_clean:
                            entity_type = "email"
                        
                        entities.append({
                            'name': word_clean,
                            'type': entity_type,
                            'context': context,
                            'source_category': category,
                            'confidence': 0.7 if entity_type != "unknown" else 0.3
                        })
                
                # Look for email addresses
                import re
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails_found = re.findall(email_pattern, content)
                for email in emails_found:
                    entities.append({
                        'name': email,
                        'type': 'email',
                        'context': content[max(0, content.find(email)-50):content.find(email)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
                
                # Look for URLs
                url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                urls_found = re.findall(url_pattern, content)
                for url in urls_found:
                    entities.append({
                        'name': url,
                        'type': 'url',
                        'context': content[max(0, content.find(url)-50):content.find(url)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
        
        # Deduplicate entities by name and return top 10
        seen_names = set()
        unique_entities = []
        for entity in entities:
            if entity['name'].lower() not in seen_names:
                seen_names.add(entity['name'].lower())
                unique_entities.append(entity)
                if len(unique_entities) >= 10:
                    break
        
        return unique_entities

class RelationshipDynamicsAnalyst(BaseClaudeAnalyst):
    """Maps relationship dynamics and influence patterns"""
    
    def __init__(self):
        super().__init__("relationship_dynamics")
        self.analysis_prompt = """You are an expert in interpersonal dynamics and relationship intelligence. 

Your task is NOT to simply map who talks to whom, but to understand this person's RELATIONSHIP PHILOSOPHY and how they think about human connections.

Analyze these emails to understand:

1. RELATIONSHIP MENTAL MODEL: How do they conceptualize professional relationships and their purpose?
2. INFLUENCE PHILOSOPHY: How do they think about influence, persuasion, and social dynamics?
3. TRUST ARCHITECTURE: How do they build, maintain, and leverage trust?
4. COMMUNICATION PATTERNS: What do their communication styles reveal about their relationship approach?
5. NETWORK STRATEGY: How do they think about building and maintaining their professional network?
6. COLLABORATION WORLDVIEW: What are their beliefs about teamwork, leadership, and collaboration?

Focus on understanding their unique approach to relationships - the underlying beliefs and frameworks that guide how they connect with others.

Look for:
- Patterns in how they build rapport and establish connections
- Their approach to managing different types of relationships
- How they handle conflict, disagreement, or difficult conversations
- What they prioritize in professional relationships
- How they leverage relationships for mutual value creation

This should reveal their "relationship DNA" - the core principles that guide how they think about and manage human connections."""

    async def analyze_emails(self, emails: List[Dict], context: Dict = None) -> AnalysisResult:
        """Map relationship dynamics and influence patterns"""
        try:
            email_context = self._prepare_email_context(emails)
            
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.model,
                max_tokens=4000,
                temperature=0.25,
                messages=[{
                    "role": "user",
                    "content": f"{self.analysis_prompt}\n\nEmails to analyze:\n{email_context}"
                }]
            )
            
            # Parse Claude's response
            result = self._parse_relationship_response(response.content[0].text)
            
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights=result,
                confidence=0.85,
                evidence=self._extract_evidence(result, emails),
                relationships=result.get('relationship_networks', []),
                topics=self._extract_topics(result),
                entities=self._extract_entities(result)
            )
            
        except Exception as e:
            logger.error(f"Relationship dynamics analysis error: {str(e)}")
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights={},
                confidence=0.0,
                evidence=[],
                relationships=[],
                topics=[],
                entities=[]
            )
    
    def _parse_relationship_response(self, response_text: str) -> Dict:
        """Parse relationship analysis response"""
        return {
            "relationship_mental_model": self._extract_section(response_text, "RELATIONSHIP MENTAL MODEL"),
            "influence_philosophy": self._extract_section(response_text, "INFLUENCE PHILOSOPHY"),
            "trust_architecture": self._extract_section(response_text, "TRUST ARCHITECTURE"),
            "communication_patterns": self._extract_section(response_text, "COMMUNICATION PATTERNS"),
            "network_strategy": self._extract_section(response_text, "NETWORK STRATEGY"),
            "collaboration_worldview": self._extract_section(response_text, "COLLABORATION WORLDVIEW"),
            "raw_analysis": response_text
        }
    
    def _extract_section(self, text: str, section_name: str) -> str:
        """Extract a specific section from the response"""
        lines = text.split('\n')
        in_section = False
        section_content = []
        
        for line in lines:
            if section_name.lower() in line.lower():
                in_section = True
                continue
            elif in_section and any(keyword in line.lower() for keyword in ['mental model', 'philosophy', 'architecture', 'patterns', 'strategy', 'worldview']):
                break
            elif in_section:
                section_content.append(line)
        
        return '\n'.join(section_content).strip()
    
    def _extract_evidence(self, insights: Dict, emails: List[Dict]) -> List[Dict]:
        """Extract supporting evidence from emails"""
        evidence = []
        
        # Extract key terms from insights for searching
        search_terms = []
        for category, content in insights.items():
            if isinstance(content, str):
                # Extract significant words (length > 3, not common words)
                words = [word.strip('.,!?;:').lower() for word in content.split() 
                        if len(word) > 3 and word.lower() not in ['this', 'that', 'with', 'from', 'they', 'them', 'will', 'have', 'been', 'were', 'was']]
                search_terms.extend(words[:5])  # Top 5 words per category
        
        # Search for evidence in emails
        for email in emails[:20]:  # Check first 20 emails for performance
            email_content = f"{email.get('subject', '')} {email.get('body_text', '')}"
            email_content_lower = email_content.lower()
            
            # Count matches
            matches = sum(1 for term in search_terms if term in email_content_lower)
            
            if matches > 0:
                # Extract relevant excerpt
                sentences = email_content.split('.')
                relevant_sentences = []
                for sentence in sentences:
                    if any(term in sentence.lower() for term in search_terms):
                        relevant_sentences.append(sentence.strip())
                        if len(relevant_sentences) >= 2:  # Max 2 sentences
                            break
                
                evidence.append({
                    'email_id': email.get('id'),
                    'date': email.get('email_date'),
                    'sender': email.get('sender'),
                    'subject': email.get('subject'),
                    'excerpt': '. '.join(relevant_sentences)[:200] + '...' if len('. '.join(relevant_sentences)) > 200 else '. '.join(relevant_sentences),
                    'relevance_score': matches / len(search_terms) if search_terms else 0,
                    'match_count': matches
                })
        
        # Sort by relevance and return top 5
        evidence.sort(key=lambda x: x['relevance_score'], reverse=True)
        return evidence[:5]
    
    def _extract_topics(self, insights: Dict) -> List[str]:
        """Extract key topics from insights"""
        topics = set()
        
        for category, content in insights.items():
            # Add category as topic
            topics.add(category.replace('_', ' ').title())
            
            if isinstance(content, str):
                # Extract potential topics from content
                words = content.split()
                for i, word in enumerate(words):
                    word = word.strip('.,!?;:').lower()
                    if len(word) > 4:  # Longer words more likely to be topics
                        topics.add(word.title())
                    
                    # Look for capitalized words/phrases that might be topics
                    if word[0].isupper() and len(word) > 3:
                        topics.add(word)
                        
                    # Look for phrases (adjacent capitalized words)
                    if i < len(words) - 1:
                        next_word = words[i + 1].strip('.,!?;:')
                        if word[0].isupper() and next_word and next_word[0].isupper():
                            topics.add(f"{word} {next_word}")
            
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        for key, value in item.items():
                            topics.add(key.replace('_', ' ').title())
                            if isinstance(value, str) and len(value.split()) <= 3:
                                topics.add(value)
                    elif isinstance(item, str):
                        topics.add(item)
        
        # Filter out very common words and return list
        common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among'}
        filtered_topics = [topic for topic in topics if topic.lower() not in common_words and len(topic) > 2]
        
        return list(set(filtered_topics))[:20]  # Return top 20 unique topics
    
    def _extract_entities(self, insights: Dict) -> List[Dict]:
        """Extract entities (people, companies, projects) from insights"""
        entities = []
        
        for category, content in insights.items():
            if isinstance(content, str):
                # Look for patterns that might be entities
                words = content.split()
                
                for i, word in enumerate(words):
                    word_clean = word.strip('.,!?;:')
                    
                    # Look for capitalized words (potential proper nouns)
                    if word_clean and word_clean[0].isupper() and len(word_clean) > 2:
                        entity_type = "unknown"
                        
                        # Try to classify entity type based on context
                        context = ' '.join(words[max(0, i-2):i+3]).lower()
                        
                        if any(indicator in context for indicator in ['company', 'corp', 'inc', 'ltd', 'llc', 'organization']):
                            entity_type = "company"
                        elif any(indicator in context for indicator in ['project', 'initiative', 'program', 'system', 'platform']):
                            entity_type = "project"
                        elif any(indicator in context for indicator in ['person', 'team', 'manager', 'director', 'ceo', 'cto']):
                            entity_type = "person"
                        elif word_clean.endswith(('.com', '.org', '.net', '.io')):
                            entity_type = "website"
                        elif '@' in word_clean:
                            entity_type = "email"
                        
                        entities.append({
                            'name': word_clean,
                            'type': entity_type,
                            'context': context,
                            'source_category': category,
                            'confidence': 0.7 if entity_type != "unknown" else 0.3
                        })
                
                # Look for email addresses
                import re
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails_found = re.findall(email_pattern, content)
                for email in emails_found:
                    entities.append({
                        'name': email,
                        'type': 'email',
                        'context': content[max(0, content.find(email)-50):content.find(email)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
                
                # Look for URLs
                url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                urls_found = re.findall(url_pattern, content)
                for url in urls_found:
                    entities.append({
                        'name': url,
                        'type': 'url',
                        'context': content[max(0, content.find(url)-50):content.find(url)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
        
        # Deduplicate entities by name and return top 10
        seen_names = set()
        unique_entities = []
        for entity in entities:
            if entity['name'].lower() not in seen_names:
                seen_names.add(entity['name'].lower())
                unique_entities.append(entity)
                if len(unique_entities) >= 10:
                    break
        
        return unique_entities

class TechnicalEvolutionAnalyst(BaseClaudeAnalyst):
    """Tracks technical decisions and architecture evolution"""
    
    def __init__(self):
        super().__init__("technical_evolution")
        self.analysis_prompt = """You are an expert in technical thinking and technology philosophy.

Your task is NOT to simply catalog technical decisions, but to understand this person's TECHNICAL WORLDVIEW and how they think about technology's role in solving problems.

Analyze these emails to understand:

1. TECHNICAL PHILOSOPHY: What are their core beliefs about technology and its role?
2. PROBLEM-SOLVING APPROACH: How do they approach technical challenges and solution design?
3. INNOVATION MINDSET: How do they think about new technologies, adoption, and risk?
4. ARCHITECTURE THINKING: What principles guide their technical architecture decisions?
5. TECHNOLOGY LEADERSHIP: How do they think about leading technical teams and decisions?
6. TECHNICAL STRATEGY: How do they connect technical choices to business outcomes?

Focus on understanding their unique technical perspective - the mental models and principles that make their technical approach distinctive.

Look for:
- Patterns in how they evaluate and adopt new technologies
- Their approach to balancing innovation with stability
- How they think about technical debt and long-term architecture
- Their philosophy on technical team management and culture
- How they connect technical decisions to business strategy

This should reveal their "technical DNA" - the core frameworks that guide their technology thinking."""

    async def analyze_emails(self, emails: List[Dict], context: Dict = None) -> AnalysisResult:
        """Extract technical decisions and evolution patterns"""
        try:
            email_context = self._prepare_email_context(emails)
            
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.model,
                max_tokens=4000,
                temperature=0.2,
                messages=[{
                    "role": "user",
                    "content": f"{self.analysis_prompt}\n\nEmails to analyze:\n{email_context}"
                }]
            )
            
            # Parse Claude's response
            result = self._parse_technical_response(response.content[0].text)
            
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights=result,
                confidence=0.85,
                evidence=self._extract_evidence(result, emails),
                relationships=result.get('technical_relationships', []),
                topics=self._extract_topics(result),
                entities=self._extract_entities(result)
            )
            
        except Exception as e:
            logger.error(f"Technical evolution analysis error: {str(e)}")
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights={},
                confidence=0.0,
                evidence=[],
                relationships=[],
                topics=[],
                entities=[]
            )
    
    def _parse_technical_response(self, response_text: str) -> Dict:
        """Parse technical analysis response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "technical_philosophy": self._extract_section(response_text, "TECHNICAL PHILOSOPHY"),
            "problem_solving_approach": self._extract_section(response_text, "PROBLEM-SOLVING APPROACH"),
            "innovation_mindset": self._extract_section(response_text, "INNOVATION MINDSET"),
            "architecture_thinking": self._extract_section(response_text, "ARCHITECTURE THINKING"),
            "technology_leadership": self._extract_section(response_text, "TECHNOLOGY LEADERSHIP"),
            "technical_strategy": self._extract_section(response_text, "TECHNICAL STRATEGY"),
            "raw_analysis": response_text
        }
    
    def _extract_section(self, text: str, section_name: str) -> str:
        """Extract a specific section from the response"""
        lines = text.split('\n')
        in_section = False
        section_content = []
        
        for line in lines:
            if section_name.lower() in line.lower():
                in_section = True
                continue
            elif in_section and any(keyword in line.lower() for keyword in ['philosophy', 'approach', 'mindset', 'thinking', 'leadership', 'strategy']):
                break
            elif in_section:
                section_content.append(line)
        
        return '\n'.join(section_content).strip()
    
    def _extract_evidence(self, insights: Dict, emails: List[Dict]) -> List[Dict]:
        """Extract supporting evidence from emails"""
        evidence = []
        
        # Extract key terms from insights for searching
        search_terms = []
        for category, content in insights.items():
            if isinstance(content, str):
                # Extract significant words (length > 3, not common words)
                words = [word.strip('.,!?;:').lower() for word in content.split() 
                        if len(word) > 3 and word.lower() not in ['this', 'that', 'with', 'from', 'they', 'them', 'will', 'have', 'been', 'were', 'was']]
                search_terms.extend(words[:5])  # Top 5 words per category
        
        # Search for evidence in emails
        for email in emails[:20]:  # Check first 20 emails for performance
            email_content = f"{email.get('subject', '')} {email.get('body_text', '')}"
            email_content_lower = email_content.lower()
            
            # Count matches
            matches = sum(1 for term in search_terms if term in email_content_lower)
            
            if matches > 0:
                # Extract relevant excerpt
                sentences = email_content.split('.')
                relevant_sentences = []
                for sentence in sentences:
                    if any(term in sentence.lower() for term in search_terms):
                        relevant_sentences.append(sentence.strip())
                        if len(relevant_sentences) >= 2:  # Max 2 sentences
                            break
                
                evidence.append({
                    'email_id': email.get('id'),
                    'date': email.get('email_date'),
                    'sender': email.get('sender'),
                    'subject': email.get('subject'),
                    'excerpt': '. '.join(relevant_sentences)[:200] + '...' if len('. '.join(relevant_sentences)) > 200 else '. '.join(relevant_sentences),
                    'relevance_score': matches / len(search_terms) if search_terms else 0,
                    'match_count': matches
                })
        
        # Sort by relevance and return top 5
        evidence.sort(key=lambda x: x['relevance_score'], reverse=True)
        return evidence[:5]
    
    def _extract_topics(self, insights: Dict) -> List[str]:
        """Extract key topics from insights"""
        topics = set()
        
        for category, content in insights.items():
            # Add category as topic
            topics.add(category.replace('_', ' ').title())
            
            if isinstance(content, str):
                # Extract potential topics from content
                words = content.split()
                for i, word in enumerate(words):
                    word = word.strip('.,!?;:').lower()
                    if len(word) > 4:  # Longer words more likely to be topics
                        topics.add(word.title())
                    
                    # Look for capitalized words/phrases that might be topics
                    if word[0].isupper() and len(word) > 3:
                        topics.add(word)
                        
                    # Look for phrases (adjacent capitalized words)
                    if i < len(words) - 1:
                        next_word = words[i + 1].strip('.,!?;:')
                        if word[0].isupper() and next_word and next_word[0].isupper():
                            topics.add(f"{word} {next_word}")
            
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        for key, value in item.items():
                            topics.add(key.replace('_', ' ').title())
                            if isinstance(value, str) and len(value.split()) <= 3:
                                topics.add(value)
                    elif isinstance(item, str):
                        topics.add(item)
        
        # Filter out very common words and return list
        common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among'}
        filtered_topics = [topic for topic in topics if topic.lower() not in common_words and len(topic) > 2]
        
        return list(set(filtered_topics))[:20]  # Return top 20 unique topics
    
    def _extract_entities(self, insights: Dict) -> List[Dict]:
        """Extract entities (people, companies, projects) from insights"""
        entities = []
        
        for category, content in insights.items():
            if isinstance(content, str):
                # Look for patterns that might be entities
                words = content.split()
                
                for i, word in enumerate(words):
                    word_clean = word.strip('.,!?;:')
                    
                    # Look for capitalized words (potential proper nouns)
                    if word_clean and word_clean[0].isupper() and len(word_clean) > 2:
                        entity_type = "unknown"
                        
                        # Try to classify entity type based on context
                        context = ' '.join(words[max(0, i-2):i+3]).lower()
                        
                        if any(indicator in context for indicator in ['company', 'corp', 'inc', 'ltd', 'llc', 'organization']):
                            entity_type = "company"
                        elif any(indicator in context for indicator in ['project', 'initiative', 'program', 'system', 'platform']):
                            entity_type = "project"
                        elif any(indicator in context for indicator in ['person', 'team', 'manager', 'director', 'ceo', 'cto']):
                            entity_type = "person"
                        elif word_clean.endswith(('.com', '.org', '.net', '.io')):
                            entity_type = "website"
                        elif '@' in word_clean:
                            entity_type = "email"
                        
                        entities.append({
                            'name': word_clean,
                            'type': entity_type,
                            'context': context,
                            'source_category': category,
                            'confidence': 0.7 if entity_type != "unknown" else 0.3
                        })
                
                # Look for email addresses
                import re
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails_found = re.findall(email_pattern, content)
                for email in emails_found:
                    entities.append({
                        'name': email,
                        'type': 'email',
                        'context': content[max(0, content.find(email)-50):content.find(email)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
                
                # Look for URLs
                url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                urls_found = re.findall(url_pattern, content)
                for url in urls_found:
                    entities.append({
                        'name': url,
                        'type': 'url',
                        'context': content[max(0, content.find(url)-50):content.find(url)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
        
        # Deduplicate entities by name and return top 10
        seen_names = set()
        unique_entities = []
        for entity in entities:
            if entity['name'].lower() not in seen_names:
                seen_names.add(entity['name'].lower())
                unique_entities.append(entity)
                if len(unique_entities) >= 10:
                    break
        
        return unique_entities

class MarketIntelligenceAnalyst(BaseClaudeAnalyst):
    """Identifies market signals and competitive intelligence"""
    
    def __init__(self):
        super().__init__("market_intelligence")
        self.analysis_prompt = """You are an expert in market thinking and competitive worldview modeling.

Your task is NOT to simply identify market trends, but to understand this person's MARKET PHILOSOPHY and how they think about competitive dynamics and opportunity creation.

Analyze these emails to understand:

1. MARKET WORLDVIEW: How do they conceptualize markets, competition, and value creation?
2. OPPORTUNITY RECOGNITION: What patterns do they use to identify and evaluate opportunities?
3. COMPETITIVE INTELLIGENCE: How do they think about competitors and competitive advantage?
4. CUSTOMER PHILOSOPHY: What are their beliefs about customer needs and behavior?
5. TIMING AND MARKET DYNAMICS: How do they think about market timing and momentum?
6. STRATEGIC POSITIONING: How do they approach market positioning and differentiation?

Focus on understanding their unique market perspective - the mental models and frameworks that guide their market thinking.

Look for:
- Patterns in how they identify and evaluate market opportunities
- Their approach to competitive analysis and strategy
- How they think about customer segments and needs
- Their philosophy on market timing and entry strategies
- How they connect market insights to strategic decisions

This should reveal their "market DNA" - the core frameworks that guide their market intelligence and strategic thinking."""

    async def analyze_emails(self, emails: List[Dict], context: Dict = None) -> AnalysisResult:
        """Extract market intelligence and opportunities"""
        try:
            email_context = self._prepare_email_context(emails)
            
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.model,
                max_tokens=4000,
                temperature=0.2,
                messages=[{
                    "role": "user",
                    "content": f"{self.analysis_prompt}\n\nEmails to analyze:\n{email_context}"
                }]
            )
            
            # Parse Claude's response
            result = self._parse_market_response(response.content[0].text)
            
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights=result,
                confidence=0.85,
                evidence=self._extract_evidence(result, emails),
                relationships=result.get('market_relationships', []),
                topics=self._extract_topics(result),
                entities=self._extract_entities(result)
            )
            
        except Exception as e:
            logger.error(f"Market intelligence analysis error: {str(e)}")
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights={},
                confidence=0.0,
                evidence=[],
                relationships=[],
                topics=[],
                entities=[]
            )
    
    def _parse_market_response(self, response_text: str) -> Dict:
        """Parse market analysis response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "market_worldview": self._extract_section(response_text, "MARKET WORLDVIEW"),
            "opportunity_recognition": self._extract_section(response_text, "OPPORTUNITY RECOGNITION"),
            "competitive_intelligence": self._extract_section(response_text, "COMPETITIVE INTELLIGENCE"),
            "customer_philosophy": self._extract_section(response_text, "CUSTOMER PHILOSOPHY"),
            "timing_market_dynamics": self._extract_section(response_text, "TIMING AND MARKET DYNAMICS"),
            "strategic_positioning": self._extract_section(response_text, "STRATEGIC POSITIONING"),
            "raw_analysis": response_text
        }
    
    def _extract_section(self, text: str, section_name: str) -> str:
        """Extract a specific section from the response"""
        lines = text.split('\n')
        in_section = False
        section_content = []
        
        for line in lines:
            if section_name.lower() in line.lower():
                in_section = True
                continue
            elif in_section and any(keyword in line.lower() for keyword in ['worldview', 'recognition', 'intelligence', 'philosophy', 'dynamics', 'positioning']):
                break
            elif in_section:
                section_content.append(line)
        
        return '\n'.join(section_content).strip()
    
    def _extract_evidence(self, insights: Dict, emails: List[Dict]) -> List[Dict]:
        """Extract supporting evidence from emails"""
        evidence = []
        
        # Extract key terms from insights for searching
        search_terms = []
        for category, content in insights.items():
            if isinstance(content, str):
                # Extract significant words (length > 3, not common words)
                words = [word.strip('.,!?;:').lower() for word in content.split() 
                        if len(word) > 3 and word.lower() not in ['this', 'that', 'with', 'from', 'they', 'them', 'will', 'have', 'been', 'were', 'was']]
                search_terms.extend(words[:5])  # Top 5 words per category
        
        # Search for evidence in emails
        for email in emails[:20]:  # Check first 20 emails for performance
            email_content = f"{email.get('subject', '')} {email.get('body_text', '')}"
            email_content_lower = email_content.lower()
            
            # Count matches
            matches = sum(1 for term in search_terms if term in email_content_lower)
            
            if matches > 0:
                # Extract relevant excerpt
                sentences = email_content.split('.')
                relevant_sentences = []
                for sentence in sentences:
                    if any(term in sentence.lower() for term in search_terms):
                        relevant_sentences.append(sentence.strip())
                        if len(relevant_sentences) >= 2:  # Max 2 sentences
                            break
                
                evidence.append({
                    'email_id': email.get('id'),
                    'date': email.get('email_date'),
                    'sender': email.get('sender'),
                    'subject': email.get('subject'),
                    'excerpt': '. '.join(relevant_sentences)[:200] + '...' if len('. '.join(relevant_sentences)) > 200 else '. '.join(relevant_sentences),
                    'relevance_score': matches / len(search_terms) if search_terms else 0,
                    'match_count': matches
                })
        
        # Sort by relevance and return top 5
        evidence.sort(key=lambda x: x['relevance_score'], reverse=True)
        return evidence[:5]
    
    def _extract_topics(self, insights: Dict) -> List[str]:
        """Extract key topics from insights"""
        topics = set()
        
        for category, content in insights.items():
            # Add category as topic
            topics.add(category.replace('_', ' ').title())
            
            if isinstance(content, str):
                # Extract potential topics from content
                words = content.split()
                for i, word in enumerate(words):
                    word = word.strip('.,!?;:').lower()
                    if len(word) > 4:  # Longer words more likely to be topics
                        topics.add(word.title())
                    
                    # Look for capitalized words/phrases that might be topics
                    if word[0].isupper() and len(word) > 3:
                        topics.add(word)
                        
                    # Look for phrases (adjacent capitalized words)
                    if i < len(words) - 1:
                        next_word = words[i + 1].strip('.,!?;:')
                        if word[0].isupper() and next_word and next_word[0].isupper():
                            topics.add(f"{word} {next_word}")
            
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        for key, value in item.items():
                            topics.add(key.replace('_', ' ').title())
                            if isinstance(value, str) and len(value.split()) <= 3:
                                topics.add(value)
                    elif isinstance(item, str):
                        topics.add(item)
        
        # Filter out very common words and return list
        common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among'}
        filtered_topics = [topic for topic in topics if topic.lower() not in common_words and len(topic) > 2]
        
        return list(set(filtered_topics))[:20]  # Return top 20 unique topics
    
    def _extract_entities(self, insights: Dict) -> List[Dict]:
        """Extract entities (people, companies, projects) from insights"""
        entities = []
        
        for category, content in insights.items():
            if isinstance(content, str):
                # Look for patterns that might be entities
                words = content.split()
                
                for i, word in enumerate(words):
                    word_clean = word.strip('.,!?;:')
                    
                    # Look for capitalized words (potential proper nouns)
                    if word_clean and word_clean[0].isupper() and len(word_clean) > 2:
                        entity_type = "unknown"
                        
                        # Try to classify entity type based on context
                        context = ' '.join(words[max(0, i-2):i+3]).lower()
                        
                        if any(indicator in context for indicator in ['company', 'corp', 'inc', 'ltd', 'llc', 'organization']):
                            entity_type = "company"
                        elif any(indicator in context for indicator in ['project', 'initiative', 'program', 'system', 'platform']):
                            entity_type = "project"
                        elif any(indicator in context for indicator in ['person', 'team', 'manager', 'director', 'ceo', 'cto']):
                            entity_type = "person"
                        elif word_clean.endswith(('.com', '.org', '.net', '.io')):
                            entity_type = "website"
                        elif '@' in word_clean:
                            entity_type = "email"
                        
                        entities.append({
                            'name': word_clean,
                            'type': entity_type,
                            'context': context,
                            'source_category': category,
                            'confidence': 0.7 if entity_type != "unknown" else 0.3
                        })
                
                # Look for email addresses
                import re
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails_found = re.findall(email_pattern, content)
                for email in emails_found:
                    entities.append({
                        'name': email,
                        'type': 'email',
                        'context': content[max(0, content.find(email)-50):content.find(email)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
                
                # Look for URLs
                url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                urls_found = re.findall(url_pattern, content)
                for url in urls_found:
                    entities.append({
                        'name': url,
                        'type': 'url',
                        'context': content[max(0, content.find(url)-50):content.find(url)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
        
        # Deduplicate entities by name and return top 10
        seen_names = set()
        unique_entities = []
        for entity in entities:
            if entity['name'].lower() not in seen_names:
                seen_names.add(entity['name'].lower())
                unique_entities.append(entity)
                if len(unique_entities) >= 10:
                    break
        
        return unique_entities

class PredictiveAnalyst(BaseClaudeAnalyst):
    """Analyzes patterns to predict future outcomes and opportunities"""
    
    def __init__(self):
        super().__init__("predictive_analysis")
        self.analysis_prompt = """You are an expert in predictive thinking and future-oriented worldview modeling.

Your task is NOT to simply make predictions, but to understand this person's PREDICTIVE PHILOSOPHY and how they think about the future and uncertainty.

Analyze these emails to understand:

1. FUTURE THINKING: How do they conceptualize the future and plan for uncertainty?
2. PATTERN RECOGNITION: What patterns do they use to anticipate future developments?
3. RISK ASSESSMENT: How do they think about and prepare for potential risks?
4. OPPORTUNITY ANTICIPATION: How do they identify and position for future opportunities?
5. DECISION TIMING: How do they think about timing decisions and market moves?
6. SCENARIO PLANNING: How do they approach planning for multiple possible futures?

Focus on understanding their unique predictive perspective - the mental models and frameworks that guide their future thinking.

Look for:
- Patterns in how they anticipate and prepare for change
- Their approach to managing uncertainty and unknown risks
- How they think about timing and strategic positioning
- Their philosophy on scenario planning and contingency thinking
- How they balance short-term execution with long-term vision

This should reveal their "predictive DNA" - the core frameworks that guide their forward-looking strategic thinking."""

    async def analyze_emails(self, emails: List[Dict], context: Dict = None) -> AnalysisResult:
        """Generate predictions based on patterns"""
        try:
            email_context = self._prepare_email_context(emails)
            
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.model,
                max_tokens=4000,
                temperature=0.3,
                messages=[{
                    "role": "user",
                    "content": f"{self.analysis_prompt}\n\nEmails to analyze:\n{email_context}"
                }]
            )
            
            # Parse Claude's response
            result = self._parse_predictive_response(response.content[0].text)
            
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights=result,
                confidence=0.85,
                evidence=self._extract_evidence(result, emails),
                relationships=result.get('predictive_relationships', []),
                topics=self._extract_topics(result),
                entities=self._extract_entities(result)
            )
            
        except Exception as e:
            logger.error(f"Predictive analysis error: {str(e)}")
            return AnalysisResult(
                analyst_type=self.analyst_type,
                insights={},
                confidence=0.0,
                evidence=[],
                relationships=[],
                topics=[],
                entities=[]
            )
    
    def _parse_predictive_response(self, response_text: str) -> Dict:
        """Parse predictive analysis response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "future_thinking": self._extract_section(response_text, "FUTURE THINKING"),
            "pattern_recognition": self._extract_section(response_text, "PATTERN RECOGNITION"),
            "risk_assessment": self._extract_section(response_text, "RISK ASSESSMENT"),
            "opportunity_anticipation": self._extract_section(response_text, "OPPORTUNITY ANTICIPATION"),
            "decision_timing": self._extract_section(response_text, "DECISION TIMING"),
            "scenario_planning": self._extract_section(response_text, "SCENARIO PLANNING"),
            "raw_analysis": response_text
        }
    
    def _extract_section(self, text: str, section_name: str) -> str:
        """Extract a specific section from the response"""
        lines = text.split('\n')
        in_section = False
        section_content = []
        
        for line in lines:
            if section_name.lower() in line.lower():
                in_section = True
                continue
            elif in_section and any(keyword in line.lower() for keyword in ['thinking', 'recognition', 'assessment', 'anticipation', 'timing', 'planning']):
                break
            elif in_section:
                section_content.append(line)
        
        return '\n'.join(section_content).strip()
    
    def _extract_evidence(self, insights: Dict, emails: List[Dict]) -> List[Dict]:
        """Extract supporting evidence from emails"""
        evidence = []
        
        # Extract key terms from insights for searching
        search_terms = []
        for category, content in insights.items():
            if isinstance(content, str):
                # Extract significant words (length > 3, not common words)
                words = [word.strip('.,!?;:').lower() for word in content.split() 
                        if len(word) > 3 and word.lower() not in ['this', 'that', 'with', 'from', 'they', 'them', 'will', 'have', 'been', 'were', 'was']]
                search_terms.extend(words[:5])  # Top 5 words per category
        
        # Search for evidence in emails
        for email in emails[:20]:  # Check first 20 emails for performance
            email_content = f"{email.get('subject', '')} {email.get('body_text', '')}"
            email_content_lower = email_content.lower()
            
            # Count matches
            matches = sum(1 for term in search_terms if term in email_content_lower)
            
            if matches > 0:
                # Extract relevant excerpt
                sentences = email_content.split('.')
                relevant_sentences = []
                for sentence in sentences:
                    if any(term in sentence.lower() for term in search_terms):
                        relevant_sentences.append(sentence.strip())
                        if len(relevant_sentences) >= 2:  # Max 2 sentences
                            break
                
                evidence.append({
                    'email_id': email.get('id'),
                    'date': email.get('email_date'),
                    'sender': email.get('sender'),
                    'subject': email.get('subject'),
                    'excerpt': '. '.join(relevant_sentences)[:200] + '...' if len('. '.join(relevant_sentences)) > 200 else '. '.join(relevant_sentences),
                    'relevance_score': matches / len(search_terms) if search_terms else 0,
                    'match_count': matches
                })
        
        # Sort by relevance and return top 5
        evidence.sort(key=lambda x: x['relevance_score'], reverse=True)
        return evidence[:5]
    
    def _extract_topics(self, insights: Dict) -> List[str]:
        """Extract key topics from insights"""
        topics = set()
        
        for category, content in insights.items():
            # Add category as topic
            topics.add(category.replace('_', ' ').title())
            
            if isinstance(content, str):
                # Extract potential topics from content
                words = content.split()
                for i, word in enumerate(words):
                    word = word.strip('.,!?;:').lower()
                    if len(word) > 4:  # Longer words more likely to be topics
                        topics.add(word.title())
                    
                    # Look for capitalized words/phrases that might be topics
                    if word[0].isupper() and len(word) > 3:
                        topics.add(word)
                        
                    # Look for phrases (adjacent capitalized words)
                    if i < len(words) - 1:
                        next_word = words[i + 1].strip('.,!?;:')
                        if word[0].isupper() and next_word and next_word[0].isupper():
                            topics.add(f"{word} {next_word}")
            
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        for key, value in item.items():
                            topics.add(key.replace('_', ' ').title())
                            if isinstance(value, str) and len(value.split()) <= 3:
                                topics.add(value)
                    elif isinstance(item, str):
                        topics.add(item)
        
        # Filter out very common words and return list
        common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among'}
        filtered_topics = [topic for topic in topics if topic.lower() not in common_words and len(topic) > 2]
        
        return list(set(filtered_topics))[:20]  # Return top 20 unique topics
    
    def _extract_entities(self, insights: Dict) -> List[Dict]:
        """Extract entities (people, companies, projects) from insights"""
        entities = []
        
        for category, content in insights.items():
            if isinstance(content, str):
                # Look for patterns that might be entities
                words = content.split()
                
                for i, word in enumerate(words):
                    word_clean = word.strip('.,!?;:')
                    
                    # Look for capitalized words (potential proper nouns)
                    if word_clean and word_clean[0].isupper() and len(word_clean) > 2:
                        entity_type = "unknown"
                        
                        # Try to classify entity type based on context
                        context = ' '.join(words[max(0, i-2):i+3]).lower()
                        
                        if any(indicator in context for indicator in ['company', 'corp', 'inc', 'ltd', 'llc', 'organization']):
                            entity_type = "company"
                        elif any(indicator in context for indicator in ['project', 'initiative', 'program', 'system', 'platform']):
                            entity_type = "project"
                        elif any(indicator in context for indicator in ['person', 'team', 'manager', 'director', 'ceo', 'cto']):
                            entity_type = "person"
                        elif word_clean.endswith(('.com', '.org', '.net', '.io')):
                            entity_type = "website"
                        elif '@' in word_clean:
                            entity_type = "email"
                        
                        entities.append({
                            'name': word_clean,
                            'type': entity_type,
                            'context': context,
                            'source_category': category,
                            'confidence': 0.7 if entity_type != "unknown" else 0.3
                        })
                
                # Look for email addresses
                import re
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails_found = re.findall(email_pattern, content)
                for email in emails_found:
                    entities.append({
                        'name': email,
                        'type': 'email',
                        'context': content[max(0, content.find(email)-50):content.find(email)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
                
                # Look for URLs
                url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                urls_found = re.findall(url_pattern, content)
                for url in urls_found:
                    entities.append({
                        'name': url,
                        'type': 'url',
                        'context': content[max(0, content.find(url)-50):content.find(url)+50],
                        'source_category': category,
                        'confidence': 0.9
                    })
        
        # Deduplicate entities by name and return top 10
        seen_names = set()
        unique_entities = []
        for entity in entities:
            if entity['name'].lower() not in seen_names:
                seen_names.add(entity['name'].lower())
                unique_entities.append(entity)
                if len(unique_entities) >= 10:
                    break
        
        return unique_entities

class KnowledgeTreeBuilder:
    """Builds comprehensive knowledge tree from email analysis"""
    
    # Make this available as a singleton for easy import in routes
    _instance = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def __init__(self):
        self.analysts = {
            'business_strategy': BusinessStrategyAnalyst(),
            'relationship_dynamics': RelationshipDynamicsAnalyst(),
            'technical_evolution': TechnicalEvolutionAnalyst(),
            'market_intelligence': MarketIntelligenceAnalyst(),
            'predictive': PredictiveAnalyst()
        }
        
    async def build_knowledge_tree(self, user_id: int, time_window_days: int = 30) -> Dict:
        """Build comprehensive knowledge tree from emails"""
        from models.database import get_db_manager
        
        try:
            db_manager = get_db_manager()
            
            # Get emails for time window
            emails = self._get_emails_for_window(user_id, time_window_days)
            
            if not emails:
                logger.warning(f"No emails found for user {user_id} in {time_window_days} day window")
                return {'status': 'no_data'}
            
            logger.info(f"Building knowledge tree from {len(emails)} emails")
            
            # Run all analysts in parallel
            analysis_tasks = []
            for analyst_name, analyst in self.analysts.items():
                task = analyst.analyze_emails(emails)
                analysis_tasks.append(task)
            
            # Wait for all analyses to complete
            results = await asyncio.gather(*analysis_tasks)
            
            # Merge results into knowledge tree
            knowledge_tree = self._merge_analysis_results(results)
            
            # Add temporal context
            knowledge_tree['time_window'] = {
                'days': time_window_days,
                'email_count': len(emails),
                'analysis_timestamp': datetime.utcnow().isoformat()
            }
            
            # Save to database
            self._save_knowledge_tree(user_id, knowledge_tree)
            
            return knowledge_tree
            
        except Exception as e:
            logger.error(f"Knowledge tree building error: {str(e)}")
            return {'status': 'error', 'error': str(e)}
    
    def _get_emails_for_window(self, user_id: int, days: int) -> List[Dict]:
        """Get emails within time window - using real emails, not mock data"""
        from models.database import get_db_manager
        from datetime import datetime, timedelta
        
        db_manager = get_db_manager()
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        # Get emails - this should now use real data since we fixed the OAuth issue
        emails = db_manager.get_user_emails(user_id, limit=1000)
        
        # Filter by date
        filtered_emails = []
        for email in emails:
            if email.email_date and email.email_date > cutoff_date:
                filtered_emails.append(email.to_dict())
        
        logger.info(f"Retrieved {len(filtered_emails)} real emails for knowledge tree analysis")
        return filtered_emails
    
    def _merge_analysis_results(self, results: List[AnalysisResult]) -> Dict:
        """Merge results from all analysts into unified knowledge tree"""
        knowledge_tree = {
            'insights': {},
            'relationships': [],
            'topics': set(),
            'entities': [],
            'predictions': [],
            'evidence_map': {},
            'worldview_synthesis': {}
        }
        
        for result in results:
            # Merge insights by analyst type
            knowledge_tree['insights'][result.analyst_type] = result.insights
            
            # Aggregate relationships
            knowledge_tree['relationships'].extend(result.relationships)
            
            # Collect all topics
            knowledge_tree['topics'].update(result.topics)
            
            # Aggregate entities
            knowledge_tree['entities'].extend(result.entities)
            
            # Map evidence to insights
            for evidence in result.evidence:
                insight_id = evidence.get('insight_id')
                if insight_id:
                    knowledge_tree['evidence_map'][insight_id] = evidence
        
        # Convert set to list for JSON serialization
        knowledge_tree['topics'] = list(knowledge_tree['topics'])
        
        # Deduplicate and rank entities
        knowledge_tree['entities'] = self._deduplicate_entities(knowledge_tree['entities'])
        
        # Create worldview synthesis from all insights
        knowledge_tree['worldview_synthesis'] = self._synthesize_worldview_insights(knowledge_tree['insights'])
        
        return knowledge_tree
    
    def _synthesize_worldview_insights(self, insights: Dict) -> Dict:
        """Synthesize insights from all analysts into coherent worldview"""
        synthesis = {
            "core_philosophies": {},
            "cross_domain_patterns": [],
            "strategic_frameworks": {},
            "unique_perspectives": [],
            "synthesis_confidence": 0.8
        }
        
        # Extract core philosophies from each domain
        for analyst_type, insight_data in insights.items():
            if isinstance(insight_data, dict):
                philosophies = {}
                for key, value in insight_data.items():
                    if 'philosophy' in key.lower() or 'worldview' in key.lower() or 'thinking' in key.lower():
                        philosophies[key] = value
                synthesis["core_philosophies"][analyst_type] = philosophies
        
        return synthesis
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """Deduplicate and merge entity information"""
        entity_map = {}
        
        for entity in entities:
            key = (entity.get('type'), entity.get('name'))
            if key in entity_map:
                # Merge information
                existing = entity_map[key]
                existing['mentions'] = existing.get('mentions', 0) + 1
                existing['contexts'].extend(entity.get('contexts', []))
            else:
                entity['mentions'] = 1
                entity['contexts'] = entity.get('contexts', [])
                entity_map[key] = entity
        
        return list(entity_map.values())
    
    def _save_knowledge_tree(self, user_id: int, knowledge_tree: Dict):
        """Save knowledge tree to database"""
        try:
            from storage.storage_manager import StorageManager
            storage_manager = StorageManager()
            
            # Store as JSON in the database
            storage_manager.save_knowledge_tree(user_id, knowledge_tree)
            
            # Also index the knowledge tree in vector database for semantic search
            knowledge_tree_str = json.dumps(knowledge_tree)
            storage_manager.index_knowledge_tree(user_id, knowledge_tree_str)
            
            logger.info(f"Knowledge tree saved for user {user_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to save knowledge tree: {str(e)}")
            return False

================================================================================
FILE: intelligence/claude_intelligent_augmentation.py
SIZE: 22,472 bytes
LINES: 548
================================================================================

"""
Claude Opus 4 Intelligent Web Intelligence System
=============================================== 
Revolutionary approach using Claude Opus 4 as the orchestrating brain to:
1. Create intelligent domain navigation plans
2. Extract comprehensive company intelligence from websites
3. Use company context for targeted person searches
4. Avoid rate limiting through smart strategies
"""

import asyncio
import aiohttp
import json
import re
from typing import Dict, Optional, List, Tuple
from urllib.parse import quote, urljoin, urlparse
import time
import anthropic
from dataclasses import dataclass
import random

from utils.logging import structured_logger as logger
from config.settings import ANTHROPIC_API_KEY

@dataclass
class DomainIntelligencePlan:
    """Claude-generated plan for domain intelligence gathering"""
    domain: str
    primary_urls: List[str]
    fallback_urls: List[str]
    expected_company_info: Dict[str, str]
    navigation_strategy: str
    scraping_priorities: List[str]

@dataclass
class CompanyIntelligence:
    """Comprehensive company intelligence extracted by Claude"""
    name: str
    industry: str
    description: str
    size: Optional[str]
    founded: Optional[str]
    location: Optional[str]
    funding: Optional[str]
    leadership: List[Dict]
    key_people: List[Dict]
    products: List[str]
    technologies: List[str]
    recent_news: List[Dict]
    confidence_score: float

@dataclass
class PersonIntelligence:
    """Person intelligence enhanced with company context"""
    name: str
    title: str
    company_context: Dict
    professional_background: Dict
    connections: Dict
    confidence_score: float

class ClaudeIntelligentAugmentation:
    """Claude Opus 4 powered intelligent augmentation system"""
    
    def __init__(self):
        self.claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None
        self.session = None
        self.rate_limiter = {}
        
    async def initialize(self):
        """Initialize HTTP session with intelligent headers"""
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=2)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        
        # Rotate user agents intelligently
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=headers
        )
        
        logger.info(" Claude Opus 4 Intelligent Augmentation initialized")

    async def create_domain_intelligence_plan(self, domain: str) -> DomainIntelligencePlan:
        """Use Claude to create an intelligent plan for domain exploration"""
        
        if not self.claude_client:
            logger.warning("Claude API not available, using fallback strategy")
            return self._create_fallback_plan(domain)
        
        prompt = f"""
        You are an expert web intelligence analyst. Create a comprehensive plan to gather intelligence about the company associated with domain: {domain}

        Analyze this domain and provide a strategic plan with:

        1. PRIMARY_URLS: Most likely URLs to contain company information (about, team, company, leadership pages)
        2. FALLBACK_URLS: Alternative URLs if primary ones fail
        3. EXPECTED_INFO: What types of company information we should expect to find
        4. NAVIGATION_STRATEGY: How to intelligently navigate the site
        5. SCRAPING_PRIORITIES: Order of information importance

        Consider:
        - Common website structures and patterns
        - Where companies typically place key information
        - How to avoid rate limiting and detection
        - Most efficient navigation paths

        Respond in JSON format:
        {{
            "primary_urls": ["/about", "/team", "/company"],
            "fallback_urls": ["/about-us", "/leadership", "/our-team"],
            "expected_company_info": {{
                "name": "likely found in header/about section",
                "industry": "about page or product descriptions",
                "description": "mission statement on about page"
            }},
            "navigation_strategy": "Start with about page, then team, prioritize official company info",
            "scraping_priorities": ["company_name", "description", "team", "products", "location"]
        }}
        """
        
        try:
            response = await self._call_claude(prompt)
            plan_data = json.loads(response)
            
            return DomainIntelligencePlan(
                domain=domain,
                primary_urls=[urljoin(f"https://{domain}", url) for url in plan_data.get('primary_urls', [])],
                fallback_urls=[urljoin(f"https://{domain}", url) for url in plan_data.get('fallback_urls', [])],
                expected_company_info=plan_data.get('expected_company_info', {}),
                navigation_strategy=plan_data.get('navigation_strategy', ''),
                scraping_priorities=plan_data.get('scraping_priorities', [])
            )
            
        except Exception as e:
            logger.error(f"Failed to create Claude intelligence plan: {e}")
            return self._create_fallback_plan(domain)

    async def extract_company_intelligence(self, domain: str) -> CompanyIntelligence:
        """Use Claude to intelligently extract comprehensive company information"""
        
        logger.info(f" Starting intelligent company analysis for {domain}")
        
        # Step 1: Create intelligent navigation plan
        plan = await self.create_domain_intelligence_plan(domain)
        logger.info(f" Created intelligence plan with {len(plan.primary_urls)} primary targets")
        
        # Step 2: Intelligently gather raw content
        content_map = await self._intelligent_content_gathering(plan)
        
        # Step 3: Use Claude to extract structured intelligence
        company_intel = await self._claude_company_analysis(domain, content_map, plan)
        
        return company_intel

    async def _intelligent_content_gathering(self, plan: DomainIntelligencePlan) -> Dict[str, str]:
        """Intelligently gather content using Claude's navigation strategy"""
        
        content_map = {}
        
        # Rate limiting strategy
        await self._intelligent_rate_limit(plan.domain)
        
        # Try primary URLs first
        for url in plan.primary_urls[:3]:  # Limit to top 3 to avoid detection
            try:
                logger.info(f" Intelligently fetching: {url}")
                
                content = await self._smart_fetch_content(url)
                if content and len(content.strip()) > 100:
                    content_map[url] = content
                    logger.info(f" Successfully gathered {len(content)} chars from {url}")
                    
                    # Intelligent delay between requests
                    await asyncio.sleep(random.uniform(2, 5))
                else:
                    logger.warning(f" No useful content from {url}")
                    
            except Exception as e:
                logger.warning(f"Failed to fetch {url}: {e}")
                continue
        
        # If primary URLs failed, try fallbacks
        if not content_map and plan.fallback_urls:
            logger.info(" Primary URLs failed, trying fallback strategy")
            for url in plan.fallback_urls[:2]:
                try:
                    content = await self._smart_fetch_content(url)
                    if content:
                        content_map[url] = content
                        break
                except Exception as e:
                    continue
        
        logger.info(f" Gathered content from {len(content_map)} URLs")
        return content_map

    async def _smart_fetch_content(self, url: str) -> Optional[str]:
        """Intelligently fetch content with anti-detection measures"""
        
        try:
            # Smart headers for this specific request
            headers = {
                'Referer': f"https://www.google.com/",
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            async with self.session.get(url, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # Basic content cleaning
                    cleaned_content = self._clean_html_content(content)
                    return cleaned_content
                    
                elif response.status == 429:
                    logger.warning(f"Rate limited on {url}, backing off")
                    await asyncio.sleep(random.uniform(10, 20))
                    return None
                else:
                    logger.warning(f"HTTP {response.status} for {url}")
                    return None
                    
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return None

    def _clean_html_content(self, html: str) -> str:
        """Extract meaningful content from HTML"""
        
        # Remove scripts and styles
        html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
        
        # Remove HTML tags but keep text
        text = re.sub(r'<[^>]+>', ' ', html)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Limit content size for Claude analysis
        return text[:8000] if text else ""

    async def _claude_company_analysis(self, domain: str, content_map: Dict[str, str], plan: DomainIntelligencePlan) -> CompanyIntelligence:
        """Use Claude to extract structured company intelligence from gathered content"""
        
        if not self.claude_client:
            return self._create_fallback_company_intel(domain)
        
        # Combine all content for analysis
        combined_content = "\n\n".join([f"=== {url} ===\n{content}" for url, content in content_map.items()])
        
        prompt = f"""
        You are an expert company intelligence analyst. Analyze the following website content from {domain} and extract comprehensive company intelligence.

        WEBSITE CONTENT:
        {combined_content[:15000]}

        Extract and structure the following information:

        1. COMPANY_NAME: Official company name
        2. INDUSTRY: Primary industry/sector
        3. DESCRIPTION: Company description and mission
        4. SIZE: Employee count or size category
        5. FOUNDED: Founding year or date
        6. LOCATION: Headquarters location
        7. FUNDING: Funding status, investors, amounts
        8. LEADERSHIP: Key executives and leadership team
        9. KEY_PEOPLE: Important team members
        10. PRODUCTS: Main products or services
        11. TECHNOLOGIES: Technologies they use/develop
        12. RECENT_NEWS: Any recent developments or news
        13. CONFIDENCE: Your confidence in this analysis (0-1)

        Respond in JSON format:
        {{
            "name": "Company Name",
            "industry": "Industry",
            "description": "Description",
            "size": "Size",
            "founded": "Year",
            "location": "Location",
            "funding": "Funding info",
            "leadership": [
                {{"name": "Name", "title": "Title", "background": "Info"}}
            ],
            "key_people": [
                {{"name": "Name", "role": "Role", "info": "Background"}}
            ],
            "products": ["Product1", "Product2"],
            "technologies": ["Tech1", "Tech2"],
            "recent_news": [
                {{"title": "News title", "summary": "Summary", "date": "Date"}}
            ],
            "confidence_score": 0.85
        }}

        Be thorough but accurate. If information is not clearly stated, mark confidence appropriately.
        """
        
        try:
            response = await self._call_claude(prompt)
            intel_data = json.loads(response)
            
            return CompanyIntelligence(
                name=intel_data.get('name', domain),
                industry=intel_data.get('industry', 'Unknown'),
                description=intel_data.get('description', ''),
                size=intel_data.get('size'),
                founded=intel_data.get('founded'),
                location=intel_data.get('location'),
                funding=intel_data.get('funding'),
                leadership=intel_data.get('leadership', []),
                key_people=intel_data.get('key_people', []),
                products=intel_data.get('products', []),
                technologies=intel_data.get('technologies', []),
                recent_news=intel_data.get('recent_news', []),
                confidence_score=intel_data.get('confidence_score', 0.5)
            )
            
        except Exception as e:
            logger.error(f"Claude company analysis failed: {e}")
            return self._create_fallback_company_intel(domain)

    async def extract_person_intelligence(self, first_name: str, last_name: str, email: str, company_intel: CompanyIntelligence) -> PersonIntelligence:
        """Use company context to intelligently search for person information"""
        
        logger.info(f" Starting intelligent person analysis for {first_name} {last_name} at {company_intel.name}")
        
        if not self.claude_client:
            return self._create_fallback_person_intel(first_name, last_name, email)
        
        # For now, create basic person intelligence with company context
        # This would be enhanced with actual search implementation
        
        person_intel = PersonIntelligence(
            name=f"{first_name} {last_name}",
            title="Unknown",
            company_context={
                "company_name": company_intel.name,
                "industry": company_intel.industry,
                "company_size": company_intel.size
            },
            professional_background={},
            connections={},
            confidence_score=0.6
        )
        
        return person_intel

    async def _intelligent_rate_limit(self, domain: str):
        """Intelligent rate limiting based on domain characteristics"""
        
        now = time.time()
        if domain in self.rate_limiter:
            last_request = self.rate_limiter[domain]
            time_since = now - last_request
            
            # Intelligent delay based on domain type
            if domain.endswith('.com'):
                min_delay = 3
            elif domain.endswith('.ai') or domain.endswith('.io'):
                min_delay = 5  # Tech companies might have better protection
            else:
                min_delay = 2
            
            if time_since < min_delay:
                sleep_time = min_delay - time_since + random.uniform(1, 3)
                logger.info(f" Intelligent rate limiting: sleeping {sleep_time:.1f}s for {domain}")
                await asyncio.sleep(sleep_time)
        
        self.rate_limiter[domain] = now

    async def _call_claude(self, prompt: str) -> str:
        """Call Claude Opus 4 with intelligent error handling"""
        
        try:
            message = self.claude_client.messages.create(
                model="claude-3-opus-20240229",
                max_tokens=4000,
                temperature=0.3,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            response_text = message.content[0].text
            logger.info(f" Claude response received ({len(response_text)} chars)")
            
            # Try to extract JSON from Claude's response
            # Sometimes Claude wraps JSON in markdown or adds explanations
            import re
            
            # Look for JSON block in markdown format
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                return json_match.group(1)
            
            # Look for JSON block without markdown
            json_match = re.search(r'(\{.*\})', response_text, re.DOTALL)
            if json_match:
                return json_match.group(1)
            
            # If no JSON found, log the raw response and return empty JSON
            logger.warning(f" No JSON found in Claude response. Raw response: {response_text[:500]}...")
            return "{}"
            
        except Exception as e:
            logger.error(f"Claude API call failed: {e}")
            raise

    def _create_fallback_plan(self, domain: str) -> DomainIntelligencePlan:
        """Create fallback plan when Claude is not available"""
        
        common_paths = ['/about', '/about-us', '/company', '/team', '/leadership', '/our-team']
        primary_urls = [f"https://{domain}{path}" for path in common_paths[:3]]
        fallback_urls = [f"https://{domain}{path}" for path in common_paths[3:]]
        
        return DomainIntelligencePlan(
            domain=domain,
            primary_urls=primary_urls,
            fallback_urls=fallback_urls,
            expected_company_info={},
            navigation_strategy="Basic path enumeration",
            scraping_priorities=["name", "description", "team"]
        )

    def _create_fallback_company_intel(self, domain: str) -> CompanyIntelligence:
        """Create fallback company intelligence"""
        
        return CompanyIntelligence(
            name=domain.replace('.com', '').replace('.', ' ').title(),
            industry="Unknown",
            description="",
            size=None,
            founded=None,
            location=None,
            funding=None,
            leadership=[],
            key_people=[],
            products=[],
            technologies=[],
            recent_news=[],
            confidence_score=0.1
        )

    def _create_fallback_person_intel(self, first_name: str, last_name: str, email: str) -> PersonIntelligence:
        """Create fallback person intelligence"""
        
        return PersonIntelligence(
            name=f"{first_name} {last_name}",
            title="Unknown",
            company_context={},
            professional_background={},
            connections={},
            confidence_score=0.1
        )

    async def close(self):
        """Close the session"""
        if self.session:
            await self.session.close()

# Global instance
claude_intelligence = ClaudeIntelligentAugmentation()

async def get_claude_intelligent_augmentation(email: str, name: str = None) -> Dict:
    """Main function for Claude intelligent augmentation"""
    
    if not claude_intelligence.session:
        await claude_intelligence.initialize()
    
    try:
        # Extract domain from email
        domain = email.split('@')[1] if '@' in email else None
        if not domain:
            return {"error": "Invalid email format"}
        
        # Extract person name
        first_name, last_name = "", ""
        if name:
            parts = name.split()
            first_name = parts[0] if parts else ""
            last_name = " ".join(parts[1:]) if len(parts) > 1 else ""
        
        # Step 1: Get comprehensive company intelligence
        logger.info(f" Starting Claude Opus 4 intelligent augmentation for {email}")
        company_intel = await claude_intelligence.extract_company_intelligence(domain)
        
        # Step 2: Use company context for person intelligence
        person_intel = await claude_intelligence.extract_person_intelligence(
            first_name, last_name, email, company_intel
        )
        
        # Format response
        return {
            "success": True,
            "method": "claude_opus_4_intelligent_augmentation",
            "person": {
                "name": person_intel.name,
                "title": person_intel.title,
                "professional_background": person_intel.professional_background,
                "connections": person_intel.connections,
                "confidence": person_intel.confidence_score
            },
            "company": {
                "name": company_intel.name,
                "industry": company_intel.industry,
                "description": company_intel.description,
                "size": company_intel.size,
                "founded": company_intel.founded,
                "location": company_intel.location,
                "funding": company_intel.funding,
                "leadership": company_intel.leadership,
                "key_people": company_intel.key_people,
                "products": company_intel.products,
                "technologies": company_intel.technologies,
                "recent_news": company_intel.recent_news,
                "confidence": company_intel.confidence_score
            },
            "intelligence_summary": {
                "method": "claude_opus_4_intelligent_orchestration",
                "person_confidence": person_intel.confidence_score,
                "company_confidence": company_intel.confidence_score,
                "overall_confidence": (person_intel.confidence_score + company_intel.confidence_score) / 2
            }
        }
        
    except Exception as e:
        logger.error(f"Claude intelligent augmentation failed for {email}: {e}")
        return {"error": str(e), "method": "claude_opus_4_intelligent_augmentation"} 

================================================================================
FILE: intelligence/deep_augmentation.py
SIZE: 18,227 bytes
LINES: 464
================================================================================

# File: intelligence/deep_augmentation.py
"""
Deep Augmentation System
========================
Enriches knowledge tree with supporting evidence and temporal evolution
"""

import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict
import numpy as np
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class AugmentedNode:
    """Augmented knowledge tree node with deep context"""
    node_id: str
    node_type: str  # insight, relationship, topic, entity
    original_content: Dict
    supporting_evidence: List[Dict]
    temporal_evolution: List[Dict]
    causality_chain: List[Dict]
    patterns: List[Dict]
    hidden_connections: List[Dict]
    confidence_score: float

class DeepAugmentationEngine:
    """Enriches knowledge tree with deep context and evidence"""
    
    # Singleton pattern for easy access from routes
    _instance = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    """Enriches knowledge tree with deep context and evidence"""
    
    def __init__(self):
        self.augmented_nodes = {}
        
    async def augment_knowledge_tree(self, knowledge_tree: Dict, emails: List[Dict]) -> Dict:
        """
        Perform deep augmentation pass on knowledge tree
        
        For each node:
        1. Extract supporting evidence from emails
        2. Build temporal evolution timeline
        3. Identify causality chains
        4. Extract patterns and learnings
        5. Find hidden connections
        """
        logger.info("Starting deep augmentation pass")
        
        # Create email index for fast lookup
        email_index = self._build_email_index(emails)
        
        # Augment each insight
        augmented_insights = {}
        for analyst_type, insights in knowledge_tree['insights'].items():
            augmented_insights[analyst_type] = await self._augment_insights(
                insights, email_index, analyst_type
            )
        
        # Augment relationships
        augmented_relationships = await self._augment_relationships(
            knowledge_tree['relationships'], email_index
        )
        
        # Augment topics with knowledge accumulation
        augmented_topics = await self._augment_topics(
            knowledge_tree['topics'], email_index
        )
        
        # Build cross-references and hidden connections
        hidden_connections = self._discover_hidden_connections(
            augmented_insights, augmented_relationships, augmented_topics
        )
        
        # Construct augmented tree
        augmented_tree = {
            'original_tree': knowledge_tree,
            'augmented_insights': augmented_insights,
            'augmented_relationships': augmented_relationships,
            'augmented_topics': augmented_topics,
            'hidden_connections': hidden_connections,
            'augmentation_metadata': {
                'timestamp': datetime.utcnow().isoformat(),
                'email_count': len(emails),
                'augmentation_depth': 'deep',
                'confidence': self._calculate_overall_confidence(augmented_insights)
            }
        }
        
        return augmented_tree
    
    def _build_email_index(self, emails: List[Dict]) -> Dict:
        """Build indices for fast email lookup"""
        index = {
            'by_id': {email['id']: email for email in emails},
            'by_date': defaultdict(list),
            'by_sender': defaultdict(list),
            'by_thread': defaultdict(list),
            'by_topic': defaultdict(list)
        }
        
        for email in emails:
            # Index by date
            if email.get('email_date'):
                date_key = email['email_date'][:10]  # YYYY-MM-DD
                index['by_date'][date_key].append(email)
            
            # Index by sender
            if email.get('sender'):
                index['by_sender'][email['sender'].lower()].append(email)
            
            # Index by thread
            if email.get('thread_id'):
                index['by_thread'][email['thread_id']].append(email)
            
            # Index by topics
            if email.get('topics'):
                for topic in email['topics']:
                    index['by_topic'][topic].append(email)
        
        return index
    
    async def _augment_insights(self, insights: Dict, email_index: Dict, analyst_type: str) -> Dict:
        """Augment insights with evidence and evolution"""
        augmented = {}
        
        for insight_category, insight_list in insights.items():
            if not isinstance(insight_list, list):
                continue
                
            augmented_list = []
            for insight in insight_list:
                if isinstance(insight, dict):
                    augmented_insight = await self._augment_single_insight(
                        insight, email_index, analyst_type, insight_category
                    )
                    augmented_list.append(augmented_insight)
            
            augmented[insight_category] = augmented_list
        
        return augmented
    
    async def _augment_single_insight(self, insight: Dict, email_index: Dict, 
                                     analyst_type: str, category: str) -> Dict:
        """Augment a single insight with deep context"""
        # Extract supporting evidence
        evidence = self._extract_supporting_evidence(insight, email_index)
        
        # Build temporal evolution
        evolution = self._build_temporal_evolution(insight, evidence, email_index)
        
        # Identify causality chains
        causality = self._identify_causality_chains(insight, evidence, email_index)
        
        # Extract patterns
        patterns = self._extract_patterns(insight, evidence, evolution)
        
        return {
            'original_insight': insight,
            'augmented_data': {
                'evidence': evidence,
                'temporal_evolution': evolution,
                'causality_chains': causality,
                'patterns': patterns,
                'confidence': self._calculate_insight_confidence(evidence, patterns)
            },
            'metadata': {
                'analyst_type': analyst_type,
                'category': category,
                'augmentation_timestamp': datetime.utcnow().isoformat()
            }
        }
    
    def _extract_supporting_evidence(self, insight: Dict, email_index: Dict) -> List[Dict]:
        """Extract email evidence supporting an insight"""
        evidence = []
        
        # Search emails for relevant content
        search_terms = self._extract_search_terms(insight)
        
        for term in search_terms:
            # Search in email content
            for email_list in email_index['by_topic'].values():
                for email in email_list:
                    if self._email_contains_evidence(email, term, insight):
                        evidence.append({
                            'email_id': email['id'],
                            'date': email.get('email_date'),
                            'sender': email.get('sender'),
                            'subject': email.get('subject'),
                            'relevant_excerpt': self._extract_relevant_excerpt(email, term),
                            'relevance_score': self._calculate_relevance_score(email, insight)
                        })
        
        # Sort by relevance
        evidence.sort(key=lambda x: x['relevance_score'], reverse=True)
        return evidence[:10]  # Top 10 pieces of evidence
    
    def _build_temporal_evolution(self, insight: Dict, evidence: List[Dict], 
                                 email_index: Dict) -> List[Dict]:
        """Build timeline showing how an insight evolved over time"""
        evolution = []
        
        # Group evidence by time periods
        time_buckets = defaultdict(list)
        for item in evidence:
            if item.get('date'):
                date = datetime.fromisoformat(item['date'])
                week_key = date.strftime('%Y-W%U')
                time_buckets[week_key].append(item)
        
        # Analyze evolution in each time period
        for week, items in sorted(time_buckets.items()):
            evolution.append({
                'period': week,
                'evidence_count': len(items),
                'key_developments': self._summarize_developments(items),
                'sentiment_shift': self._analyze_sentiment_shift(items),
                'momentum': self._calculate_momentum(items, week)
            })
        
        return evolution
    
    def _identify_causality_chains(self, insight: Dict, evidence: List[Dict], 
                                   email_index: Dict) -> List[Dict]:
        """Identify cause-and-effect relationships"""
        chains = []
        
        # Look for temporal sequences that suggest causation
        for i, item in enumerate(evidence[:-1]):
            for j, next_item in enumerate(evidence[i+1:], i+1):
                if self._suggests_causation(item, next_item):
                    chains.append({
                        'cause': item,
                        'effect': next_item,
                        'confidence': self._calculate_causation_confidence(item, next_item),
                        'time_delta': self._calculate_time_delta(item, next_item)
                    })
        
        return chains
    
    def _extract_patterns(self, insight: Dict, evidence: List[Dict], 
                         evolution: List[Dict]) -> List[Dict]:
        """Extract recurring patterns and learnings"""
        patterns = []
        
        # Communication patterns
        comm_patterns = self._analyze_communication_patterns(evidence)
        if comm_patterns:
            patterns.extend(comm_patterns)
        
        # Decision patterns
        decision_patterns = self._analyze_decision_patterns(insight, evidence)
        if decision_patterns:
            patterns.extend(decision_patterns)
        
        # Timing patterns
        timing_patterns = self._analyze_timing_patterns(evolution)
        if timing_patterns:
            patterns.extend(timing_patterns)
        
        return patterns
    
    async def _augment_relationships(self, relationships: List[Dict], 
                                    email_index: Dict) -> List[Dict]:
        """Augment relationships with communication patterns and evolution"""
        augmented = []
        
        for relationship in relationships:
            # Get all communications between parties
            communications = self._get_relationship_communications(relationship, email_index)
            
            # Analyze communication patterns
            patterns = self._analyze_relationship_patterns(communications)
            
            # Track relationship evolution
            evolution = self._track_relationship_evolution(communications)
            
            augmented.append({
                'original': relationship,
                'augmented_data': {
                    'communication_count': len(communications),
                    'patterns': patterns,
                    'evolution': evolution,
                    'health_score': self._calculate_relationship_health(patterns, evolution),
                    'collaboration_opportunities': self._identify_collaboration_opportunities(
                        relationship, communications
                    )
                }
            })
        
        return augmented
    
    async def _augment_topics(self, topics: List[str], email_index: Dict) -> List[Dict]:
        """Augment topics with knowledge accumulation and expertise mapping"""
        augmented = []
        
        for topic in topics:
            # Get all emails related to topic
            topic_emails = email_index['by_topic'].get(topic, [])
            
            # Build knowledge accumulation timeline
            knowledge_timeline = self._build_knowledge_timeline(topic, topic_emails)
            
            # Identify topic experts
            experts = self._identify_topic_experts(topic, topic_emails)
            
            # Extract key decisions and outcomes
            decisions = self._extract_topic_decisions(topic, topic_emails)
            
            augmented.append({
                'topic': topic,
                'augmented_data': {
                    'email_count': len(topic_emails),
                    'knowledge_timeline': knowledge_timeline,
                    'experts': experts,
                    'key_decisions': decisions,
                    'maturity_level': self._assess_topic_maturity(knowledge_timeline),
                    'strategic_importance': self._calculate_topic_importance(topic_emails)
                }
            })
        
        return augmented
    
    def _discover_hidden_connections(self, insights: Dict, relationships: List[Dict], 
                                    topics: List[Dict]) -> List[Dict]:
        """Discover non-obvious connections across different branches of knowledge tree"""
        connections = []
        
        # Cross-reference insights with relationships
        for analyst_type, insight_list in insights.items():
            for relationship in relationships:
                connection_strength = self._calculate_connection_strength(
                    insight_list, relationship
                )
                if connection_strength > 0.7:
                    connections.append({
                        'type': 'insight_relationship',
                        'source': {'type': 'insight', 'analyst': analyst_type},
                        'target': {'type': 'relationship', 'data': relationship['original']},
                        'strength': connection_strength,
                        'implications': self._analyze_connection_implications(
                            insight_list, relationship
                        )
                    })
        
        # Find topic intersections
        for i, topic1 in enumerate(topics):
            for topic2 in topics[i+1:]:
                intersection = self._find_topic_intersection(topic1, topic2)
                if intersection['strength'] > 0.5:
                    connections.append({
                        'type': 'topic_intersection',
                        'topics': [topic1['topic'], topic2['topic']],
                        'intersection': intersection,
                        'opportunities': self._identify_intersection_opportunities(
                            topic1, topic2, intersection
                        )
                    })
        
        return connections
    
    # Helper methods (implement based on specific needs)
    def _extract_search_terms(self, insight: Dict) -> List[str]:
        """Extract search terms from insight"""
        terms = []
        # Implementation
        return terms
    
    def _email_contains_evidence(self, email: Dict, term: str, insight: Dict) -> bool:
        """Check if email contains evidence for insight"""
        # Implementation
        return False
    
    def _extract_relevant_excerpt(self, email: Dict, term: str) -> str:
        """Extract relevant excerpt from email"""
        # Implementation
        return ""
    
    def _calculate_relevance_score(self, email: Dict, insight: Dict) -> float:
        """Calculate relevance score"""
        # Implementation
        return 0.0
    
    def _summarize_developments(self, items: List[Dict]) -> List[str]:
        """Summarize key developments"""
        # Implementation
        return []
    
    def _analyze_sentiment_shift(self, items: List[Dict]) -> Dict:
        """Analyze sentiment changes"""
        # Implementation
        return {}
    
    def _calculate_momentum(self, items: List[Dict], period: str) -> float:
        """Calculate momentum score"""
        # Implementation
        return 0.0
    
    def _suggests_causation(self, item1: Dict, item2: Dict) -> bool:
        """Check if items suggest causation"""
        # Implementation
        return False
    
    def _calculate_causation_confidence(self, cause: Dict, effect: Dict) -> float:
        """Calculate causation confidence"""
        # Implementation
        return 0.0
    
    def _calculate_time_delta(self, item1: Dict, item2: Dict) -> str:
        """Calculate time between items"""
        # Implementation
        return ""
    
    def _analyze_communication_patterns(self, evidence: List[Dict]) -> List[Dict]:
        """Analyze communication patterns"""
        # Implementation
        return []
    
    def _analyze_decision_patterns(self, insight: Dict, evidence: List[Dict]) -> List[Dict]:
        """Analyze decision patterns"""
        # Implementation
        return []
    
    def _analyze_timing_patterns(self, evolution: List[Dict]) -> List[Dict]:
        """Analyze timing patterns"""
        # Implementation
        return []
    
    def _calculate_insight_confidence(self, evidence: List[Dict], patterns: List[Dict]) -> float:
        """Calculate overall confidence"""
        # Implementation
        return 0.8
    
    def _calculate_overall_confidence(self, augmented_insights: Dict) -> float:
        """Calculate tree-wide confidence"""
        if not augmented_insights:
            return 0.5
            
        # Average confidence across all insight types
        total_confidence = 0.0
        count = 0
        
        for analyst_type, insights in augmented_insights.items():
            for category, items in insights.items():
                if isinstance(items, list):
                    for item in items:
                        if isinstance(item, dict) and 'confidence' in item:
                            total_confidence += item['confidence']
                            count += 1
        
        if count > 0:
            return total_confidence / count
        else:
            return 0.75  # Default confidence level

================================================================================
FILE: intelligence/enrichment_engine.py
SIZE: 12,891 bytes
LINES: 290
================================================================================

"""
Deep Intelligence Enrichment Engine
==================================
Comprehensive contact and company enrichment using multiple premium data sources.
"""

import asyncio
import aiohttp
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import json
import re

from utils.logging import structured_logger as logger

@dataclass
class DeepEnrichmentResult:
    """Comprehensive enrichment result with rich intelligence"""
    email: str
    person_intelligence: Dict[str, Any]
    company_intelligence: Dict[str, Any]
    network_intelligence: Dict[str, Any]
    content_intelligence: Dict[str, Any]
    financial_intelligence: Dict[str, Any]
    confidence_score: float
    enrichment_timestamp: datetime
    sources_used: List[str]

class DeepEnrichmentEngine:
    """Orchestrates deep intelligence gathering from premium sources"""
    
    def __init__(self):
        self.sources = {
            # Premium APIs (require paid subscriptions)
            "apollo": ApolloEnricher(),
            "clearbit": ClearbitEnricher(), 
            "pitchbook": PitchBookEnricher(),
            "crunchbase": CrunchbaseEnricher(),
            
            # Public data sources
            "wikipedia": WikipediaEnricher(),
            "sec_edgar": SECEnricher(),
            "patent_search": PatentEnricher(),
            "news_intelligence": NewsEnricher(),
            "social_deep": SocialMediaDeepEnricher(),
            
            # Company intelligence
            "builtwith": BuiltWithEnricher(),
            "similarweb": SimilarWebEnricher(),
            "glassdoor": GlassdoorEnricher()
        }
    
    async def enrich_contact_comprehensive(self, contact: Dict) -> DeepEnrichmentResult:
        """Perform comprehensive enrichment across all sources"""
        email = contact["email"]
        logger.info(f"Starting deep enrichment for {email}")
        
        # Phase 1: Basic identification
        basic_info = await self._get_basic_identification(contact)
        
        # Phase 2: Professional intelligence
        professional_intel = await self._gather_professional_intelligence(basic_info)
        
        # Phase 3: Company intelligence  
        company_intel = await self._gather_company_intelligence(basic_info)
        
        # Phase 4: Network analysis
        network_intel = await self._analyze_professional_network(basic_info)
        
        # Phase 5: Content & personality analysis
        content_intel = await self._analyze_content_and_personality(basic_info)
        
        # Phase 6: Financial intelligence
        financial_intel = await self._gather_financial_intelligence(basic_info)
        
        # Calculate comprehensive confidence
        confidence = self._calculate_comprehensive_confidence(
            basic_info, professional_intel, company_intel, 
            network_intel, content_intel, financial_intel
        )
        
        return DeepEnrichmentResult(
            email=email,
            person_intelligence=professional_intel,
            company_intelligence=company_intel,
            network_intelligence=network_intel,
            content_intelligence=content_intel,
            financial_intelligence=financial_intel,
            confidence_score=confidence,
            enrichment_timestamp=datetime.utcnow(),
            sources_used=self._get_successful_sources()
        )

class WikipediaEnricher:
    """Extract biographical data from Wikipedia"""
    
    async def enrich_person(self, name: str, company: str = None) -> Dict:
        """Get comprehensive biographical data"""
        try:
            # Search Wikipedia for the person
            search_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{name.replace(' ', '_')}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(search_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        return {
                            "biography": data.get("extract", ""),
                            "birth_date": self._extract_birth_date(data.get("extract", "")),
                            "education": self._extract_education(data.get("extract", "")),
                            "career_highlights": self._extract_career_highlights(data.get("extract", "")),
                            "awards": self._extract_awards(data.get("extract", "")),
                            "wikipedia_url": data.get("content_urls", {}).get("desktop", {}).get("page", ""),
                            "confidence": 0.9 if data.get("extract") else 0.1
                        }
        except Exception as e:
            logger.error(f"Wikipedia enrichment error: {e}")
            return {}

class CrunchbaseEnricher:
    """Professional and investment intelligence via Crunchbase"""
    
    def __init__(self):
        self.api_key = None  # Would be loaded from secure config
        
    async def enrich_person(self, name: str, company: str = None) -> Dict:
        """Get investment history, board positions, startup involvement"""
        if not self.api_key:
            return {"error": "Crunchbase API key not configured"}
            
        try:
            # Search for person in Crunchbase
            search_url = f"https://api.crunchbase.com/api/v4/searches/people"
            headers = {"X-CB-User-Key": self.api_key}
            params = {
                "query": name,
                "field_ids": [
                    "identifier", "name", "description", "image_url",
                    "jobs", "founded_organizations", "advisor_organizations",
                    "investor_investments", "board_members"
                ]
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(search_url, headers=headers, json=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        person_data = data.get("entities", [])
                        
                        if person_data:
                            person = person_data[0]["properties"]
                            
                            return {
                                "name": person.get("name"),
                                "description": person.get("description"),
                                "job_history": self._parse_job_history(person.get("jobs", [])),
                                "founded_companies": self._parse_founded_orgs(person.get("founded_organizations", [])),
                                "board_positions": self._parse_board_positions(person.get("board_members", [])),
                                "investment_activity": self._parse_investments(person.get("investor_investments", [])),
                                "advisory_roles": self._parse_advisory_roles(person.get("advisor_organizations", [])),
                                "crunchbase_url": f"https://www.crunchbase.com/person/{person.get('identifier', {}).get('permalink', '')}",
                                "confidence": 0.95
                            }
        except Exception as e:
            logger.error(f"Crunchbase enrichment error: {e}")
            return {}

class NewsEnricher:
    """Recent news, interviews, and media mentions"""
    
    async def enrich_person(self, name: str, company: str = None) -> Dict:
        """Get recent news mentions, interviews, thought leadership"""
        try:
            # Search multiple news APIs
            news_sources = await asyncio.gather(
                self._search_news_api(name, company),
                self._search_google_news(name, company),
                self._search_podcast_mentions(name)
            )
            
            all_articles = []
            for source in news_sources:
                all_articles.extend(source.get("articles", []))
            
            # Analyze content for personality insights
            personality_insights = self._analyze_personality_from_content(all_articles)
            
            return {
                "recent_mentions": all_articles[:20],  # Top 20 most recent
                "media_appearances": self._extract_media_appearances(all_articles),
                "thought_leadership": self._extract_thought_leadership(all_articles),
                "personality_insights": personality_insights,
                "speaking_style": self._analyze_speaking_style(all_articles),
                "hot_takes": self._extract_controversial_opinions(all_articles),
                "prediction_track_record": self._analyze_predictions(all_articles),
                "confidence": 0.8 if all_articles else 0.2
            }
        except Exception as e:
            logger.error(f"News enrichment error: {e}")
            return {}
    
    def _analyze_personality_from_content(self, articles: List[Dict]) -> Dict:
        """Analyze communication style and personality from content"""
        # Extract quotes and analyze patterns
        quotes = []
        for article in articles:
            quotes.extend(self._extract_quotes(article.get("content", "")))
        
        if not quotes:
            return {}
        
        return {
            "communication_style": self._analyze_communication_style(quotes),
            "decision_making_style": self._analyze_decision_patterns(quotes),
            "interests": self._extract_interests(quotes),
            "values": self._extract_values(quotes),
            "expertise_areas": self._identify_expertise_areas(quotes)
        }

class SocialMediaDeepEnricher:
    """Deep social media analysis for personality and style"""
    
    async def enrich_person(self, name: str, handles: Dict = None) -> Dict:
        """Analyze social media for personality, style, and network"""
        try:
            social_data = {}
            
            # LinkedIn deep analysis
            if handles and handles.get("linkedin"):
                linkedin_data = await self._analyze_linkedin_deep(handles["linkedin"])
                social_data["linkedin"] = linkedin_data
            
            # Twitter/X analysis
            if handles and handles.get("twitter"):
                twitter_data = await self._analyze_twitter_deep(handles["twitter"])
                social_data["twitter"] = twitter_data
            
            # Blog/Medium analysis
            blog_data = await self._find_and_analyze_blogs(name)
            if blog_data:
                social_data["blogs"] = blog_data
            
            # Aggregate insights
            aggregated_insights = self._aggregate_social_insights(social_data)
            
            return {
                "platforms": social_data,
                "personality_profile": aggregated_insights.get("personality"),
                "communication_patterns": aggregated_insights.get("communication"),
                "network_influence": aggregated_insights.get("influence"),
                "content_themes": aggregated_insights.get("themes"),
                "confidence": aggregated_insights.get("confidence", 0.6)
            }
        except Exception as e:
            logger.error(f"Social media enrichment error: {e}")
            return {}

class BuiltWithEnricher:
    """Company technology stack and digital presence"""
    
    async def enrich_company(self, domain: str) -> Dict:
        """Get comprehensive technology stack and digital intelligence"""
        try:
            # Get technology stack
            tech_url = f"https://api.builtwith.com/v20/api.json"
            params = {
                "KEY": self.api_key,  # Would be loaded from config
                "LOOKUP": domain
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(tech_url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        return {
                            "technology_stack": self._parse_tech_stack(data),
                            "digital_presence": self._analyze_digital_presence(data),
                            "tech_sophistication_score": self._calculate_tech_sophistication(data),
                            "vendor_relationships": self._extract_vendor_relationships(data),
                            "technical_capabilities": self._assess_technical_capabilities(data),
                            "confidence": 0.9
                        }
        except Exception as e:
            logger.error(f"BuiltWith enrichment error: {e}")
            return {}

# Additional enrichers for SEC filings, patents, etc. would follow similar patterns... 

================================================================================
FILE: intelligence/web_scraper.py
SIZE: 22,920 bytes
LINES: 559
================================================================================

"""
Claude Opus 4 Intelligent Web Intelligence System
=============================================== 
Uses Claude Opus 4 as the orchestrating brain to intelligently navigate websites,
extract comprehensive company intelligence, and perform targeted person searches.
"""

import asyncio
import aiohttp
import json
import re
from typing import Dict, Optional, List, Tuple
from urllib.parse import quote, urljoin, urlparse
import time
import anthropic
from dataclasses import dataclass
import random

from utils.logging import structured_logger as logger
from config.settings import ANTHROPIC_API_KEY

@dataclass
class DomainIntelligencePlan:
    """Claude-generated plan for domain intelligence gathering"""
    domain: str
    primary_urls: List[str]
    fallback_urls: List[str]
    expected_company_info: Dict[str, str]
    navigation_strategy: str
    scraping_priorities: List[str]

@dataclass
class CompanyIntelligence:
    """Comprehensive company intelligence extracted by Claude"""
    name: str
    industry: str
    description: str
    size: Optional[str]
    founded: Optional[str]
    location: Optional[str]
    funding: Optional[str]
    leadership: List[Dict]
    key_people: List[Dict]
    products: List[str]
    technologies: List[str]
    recent_news: List[Dict]
    confidence_score: float

@dataclass
class PersonIntelligence:
    """Person intelligence enhanced with company context"""
    name: str
    title: str
    company_context: Dict
    professional_background: Dict
    connections: Dict
    confidence_score: float

class ClaudeIntelligentAugmentation:
    """Claude Opus 4 powered intelligent augmentation system"""
    
    def __init__(self):
        self.claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None
        self.session = None
        self.rate_limiter = {}
        
    async def initialize(self):
        """Initialize HTTP session with intelligent headers"""
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=2)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        
        # Rotate user agents intelligently
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=headers
        )
        
        logger.info(" Claude Opus 4 Intelligent Augmentation initialized")

    async def create_domain_intelligence_plan(self, domain: str) -> DomainIntelligencePlan:
        """Use Claude to create an intelligent plan for domain exploration"""
        
        if not self.claude_client:
            logger.warning("Claude API not available, using fallback strategy")
            return self._create_fallback_plan(domain)
        
        prompt = f"""
        You are an expert web intelligence analyst. Create a comprehensive plan to gather intelligence about the company associated with domain: {domain}

        Analyze this domain and provide a strategic plan with:

        1. PRIMARY_URLS: Most likely URLs to contain company information (about, team, company, leadership pages)
        2. FALLBACK_URLS: Alternative URLs if primary ones fail
        3. EXPECTED_INFO: What types of company information we should expect to find
        4. NAVIGATION_STRATEGY: How to intelligently navigate the site
        5. SCRAPING_PRIORITIES: Order of information importance

        Consider:
        - Common website structures and patterns
        - Where companies typically place key information
        - How to avoid rate limiting and detection
        - Most efficient navigation paths

        Respond in JSON format:
        {{
            "primary_urls": ["url1", "url2", ...],
            "fallback_urls": ["url1", "url2", ...],
            "expected_company_info": {{
                "name": "likely_location_on_site",
                "industry": "where_to_find_this",
                "description": "typical_placement"
            }},
            "navigation_strategy": "step_by_step_approach",
            "scraping_priorities": ["most_important", "secondary", ...]
        }}
        """
        
        try:
            response = await self._call_claude(prompt)
            plan_data = json.loads(response)
            
            return DomainIntelligencePlan(
                domain=domain,
                primary_urls=[urljoin(f"https://{domain}", url) for url in plan_data.get('primary_urls', [])],
                fallback_urls=[urljoin(f"https://{domain}", url) for url in plan_data.get('fallback_urls', [])],
                expected_company_info=plan_data.get('expected_company_info', {}),
                navigation_strategy=plan_data.get('navigation_strategy', ''),
                scraping_priorities=plan_data.get('scraping_priorities', [])
            )
            
        except Exception as e:
            logger.error(f"Failed to create Claude intelligence plan: {e}")
            return self._create_fallback_plan(domain)

    async def extract_company_intelligence(self, domain: str) -> CompanyIntelligence:
        """Use Claude to intelligently extract comprehensive company information"""
        
        logger.info(f" Starting intelligent company analysis for {domain}")
        
        # Step 1: Create intelligent navigation plan
        plan = await self.create_domain_intelligence_plan(domain)
        logger.info(f" Created intelligence plan with {len(plan.primary_urls)} primary targets")
        
        # Step 2: Intelligently gather raw content
        content_map = await self._intelligent_content_gathering(plan)
        
        # Step 3: Use Claude to extract structured intelligence
        company_intel = await self._claude_company_analysis(domain, content_map, plan)
        
        return company_intel

    async def _intelligent_content_gathering(self, plan: DomainIntelligencePlan) -> Dict[str, str]:
        """Intelligently gather content using Claude's navigation strategy"""
        
        content_map = {}
        
        # Rate limiting strategy
        await self._intelligent_rate_limit(plan.domain)
        
        # Try primary URLs first
        for url in plan.primary_urls[:3]:  # Limit to top 3 to avoid detection
            try:
                logger.info(f" Intelligently fetching: {url}")
                
                content = await self._smart_fetch_content(url)
                if content and len(content.strip()) > 100:
                    content_map[url] = content
                    logger.info(f" Successfully gathered {len(content)} chars from {url}")
                    
                    # Intelligent delay between requests
                    await asyncio.sleep(random.uniform(2, 5))
                else:
                    logger.warning(f" No useful content from {url}")
                    
            except Exception as e:
                logger.warning(f"Failed to fetch {url}: {e}")
                continue
        
        # If primary URLs failed, try fallbacks
        if not content_map and plan.fallback_urls:
            logger.info(" Primary URLs failed, trying fallback strategy")
            for url in plan.fallback_urls[:2]:
                try:
                    content = await self._smart_fetch_content(url)
                    if content:
                        content_map[url] = content
                        break
                except Exception as e:
                    continue
        
        logger.info(f" Gathered content from {len(content_map)} URLs")
        return content_map

    async def _smart_fetch_content(self, url: str) -> Optional[str]:
        """Intelligently fetch content with anti-detection measures"""
        
        try:
            # Smart headers for this specific request
            headers = {
                'Referer': f"https://www.google.com/",
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            async with self.session.get(url, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # Basic content cleaning
                    # Remove scripts, styles, and extract main content
                    cleaned_content = self._clean_html_content(content)
                    return cleaned_content
                    
                elif response.status == 429:
                    logger.warning(f"Rate limited on {url}, backing off")
                    await asyncio.sleep(random.uniform(10, 20))
                    return None
                else:
                    logger.warning(f"HTTP {response.status} for {url}")
                    return None
                    
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return None

    def _clean_html_content(self, html: str) -> str:
        """Extract meaningful content from HTML"""
        
        # Remove scripts and styles
        html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
        
        # Remove HTML tags but keep text
        text = re.sub(r'<[^>]+>', ' ', html)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Limit content size for Claude analysis
        return text[:8000] if text else ""

    async def _claude_company_analysis(self, domain: str, content_map: Dict[str, str], plan: DomainIntelligencePlan) -> CompanyIntelligence:
        """Use Claude to extract structured company intelligence from gathered content"""
        
        if not self.claude_client:
            return self._create_fallback_company_intel(domain)
        
        # Combine all content for analysis
        combined_content = "\n\n".join([f"=== {url} ===\n{content}" for url, content in content_map.items()])
        
        prompt = f"""
        You are an expert company intelligence analyst. Analyze the following website content from {domain} and extract comprehensive company intelligence.

        WEBSITE CONTENT:
        {combined_content[:15000]}  # Limit for Claude context

        Extract and structure the following information:

        1. COMPANY_NAME: Official company name
        2. INDUSTRY: Primary industry/sector
        3. DESCRIPTION: Company description and mission
        4. SIZE: Employee count or size category
        5. FOUNDED: Founding year or date
        6. LOCATION: Headquarters location
        7. FUNDING: Funding status, investors, amounts
        8. LEADERSHIP: Key executives and leadership team
        9. KEY_PEOPLE: Important team members
        10. PRODUCTS: Main products or services
        11. TECHNOLOGIES: Technologies they use/develop
        12. RECENT_NEWS: Any recent developments or news
        13. CONFIDENCE: Your confidence in this analysis (0-1)

        Respond in JSON format:
        {{
            "name": "Company Name",
            "industry": "Industry",
            "description": "Description",
            "size": "Size",
            "founded": "Year",
            "location": "Location",
            "funding": "Funding info",
            "leadership": [
                {{"name": "Name", "title": "Title", "background": "Info"}}
            ],
            "key_people": [
                {{"name": "Name", "role": "Role", "info": "Background"}}
            ],
            "products": ["Product1", "Product2"],
            "technologies": ["Tech1", "Tech2"],
            "recent_news": [
                {{"title": "News title", "summary": "Summary", "date": "Date"}}
            ],
            "confidence_score": 0.85
        }}

        Be thorough but accurate. If information is not clearly stated, mark confidence appropriately.
        """
        
        try:
            response = await self._call_claude(prompt)
            intel_data = json.loads(response)
            
            return CompanyIntelligence(
                name=intel_data.get('name', domain),
                industry=intel_data.get('industry', 'Unknown'),
                description=intel_data.get('description', ''),
                size=intel_data.get('size'),
                founded=intel_data.get('founded'),
                location=intel_data.get('location'),
                funding=intel_data.get('funding'),
                leadership=intel_data.get('leadership', []),
                key_people=intel_data.get('key_people', []),
                products=intel_data.get('products', []),
                technologies=intel_data.get('technologies', []),
                recent_news=intel_data.get('recent_news', []),
                confidence_score=intel_data.get('confidence_score', 0.5)
            )
            
        except Exception as e:
            logger.error(f"Claude company analysis failed: {e}")
            return self._create_fallback_company_intel(domain)

    async def extract_person_intelligence(self, first_name: str, last_name: str, email: str, company_intel: CompanyIntelligence) -> PersonIntelligence:
        """Use company context to intelligently search for person information"""
        
        logger.info(f" Starting intelligent person analysis for {first_name} {last_name} at {company_intel.name}")
        
        if not self.claude_client:
            return self._create_fallback_person_intel(first_name, last_name, email)
        
        # Use Claude to create intelligent search strategies
        search_strategies = await self._create_person_search_strategies(first_name, last_name, company_intel)
        
        # Execute intelligent searches
        person_data = await self._execute_person_searches(search_strategies)
        
        # Use Claude to synthesize person intelligence
        person_intel = await self._claude_person_analysis(first_name, last_name, email, company_intel, person_data)
        
        return person_intel

    async def _create_person_search_strategies(self, first_name: str, last_name: str, company_intel: CompanyIntelligence) -> List[str]:
        """Use Claude to create intelligent person search strategies"""
        
        prompt = f"""
        Create intelligent search strategies to find information about:
        Name: {first_name} {last_name}
        Company: {company_intel.name}
        Industry: {company_intel.industry}
        Company Description: {company_intel.description}

        Based on the company context, create targeted search queries that would be most likely to find professional information about this person.

        Consider:
        - Company leadership structure
        - Industry-specific platforms
        - Professional networks
        - Company announcements or press releases
        - LinkedIn optimization
        - Company blog or team pages

        Return 5-7 strategic search queries as a JSON array:
        ["query1", "query2", ...]
        """
        
        try:
            response = await self._call_claude(prompt)
            strategies = json.loads(response)
            return strategies
        except Exception as e:
            logger.error(f"Failed to create person search strategies: {e}")
            return [
                f'"{first_name} {last_name}" {company_intel.name}',
                f'"{first_name} {last_name}" linkedin {company_intel.industry}',
                f'{first_name} {last_name} {company_intel.name.split()[0]}'
            ]

    async def _intelligent_rate_limit(self, domain: str):
        """Intelligent rate limiting based on domain characteristics"""
        
        now = time.time()
        if domain in self.rate_limiter:
            last_request = self.rate_limiter[domain]
            time_since = now - last_request
            
            # Intelligent delay based on domain type
            if domain.endswith('.com'):
                min_delay = 3
            elif domain.endswith('.ai') or domain.endswith('.io'):
                min_delay = 5  # Tech companies might have better protection
            else:
                min_delay = 2
            
            if time_since < min_delay:
                sleep_time = min_delay - time_since + random.uniform(1, 3)
                logger.info(f" Intelligent rate limiting: sleeping {sleep_time:.1f}s for {domain}")
                await asyncio.sleep(sleep_time)
        
        self.rate_limiter[domain] = now

    async def _call_claude(self, prompt: str) -> str:
        """Call Claude Opus 4 with intelligent error handling"""
        
        try:
            message = self.claude_client.messages.create(
                model="claude-3-opus-20240229",
                max_tokens=4000,
                temperature=0.3,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return message.content[0].text
            
        except Exception as e:
            logger.error(f"Claude API call failed: {e}")
            raise

    def _create_fallback_plan(self, domain: str) -> DomainIntelligencePlan:
        """Create fallback plan when Claude is not available"""
        
        common_paths = ['/about', '/about-us', '/company', '/team', '/leadership', '/our-team']
        primary_urls = [f"https://{domain}{path}" for path in common_paths[:3]]
        fallback_urls = [f"https://{domain}{path}" for path in common_paths[3:]]
        
        return DomainIntelligencePlan(
            domain=domain,
            primary_urls=primary_urls,
            fallback_urls=fallback_urls,
            expected_company_info={},
            navigation_strategy="Basic path enumeration",
            scraping_priorities=["name", "description", "team"]
        )

    def _create_fallback_company_intel(self, domain: str) -> CompanyIntelligence:
        """Create fallback company intelligence"""
        
        return CompanyIntelligence(
            name=domain.replace('.com', '').replace('.', ' ').title(),
            industry="Unknown",
            description="",
            size=None,
            founded=None,
            location=None,
            funding=None,
            leadership=[],
            key_people=[],
            products=[],
            technologies=[],
            recent_news=[],
            confidence_score=0.1
        )

    def _create_fallback_person_intel(self, first_name: str, last_name: str, email: str) -> PersonIntelligence:
        """Create fallback person intelligence"""
        
        return PersonIntelligence(
            name=f"{first_name} {last_name}",
            title="Unknown",
            company_context={},
            professional_background={},
            connections={},
            confidence_score=0.1
        )

    async def close(self):
        """Close the session"""
        if self.session:
            await self.session.close()

# Create global instance
claude_intelligence = ClaudeIntelligentAugmentation()

# Backward compatibility functions
async def get_search_intelligence(email: str, name: str = None) -> Dict:
    """Enhanced function that uses Claude intelligent augmentation"""
    
    if not claude_intelligence.session:
        await claude_intelligence.initialize()
    
    try:
        # Extract domain from email
        domain = email.split('@')[1] if '@' in email else None
        if not domain:
            return {"error": "Invalid email format"}
        
        # Extract person name
        first_name, last_name = "", ""
        if name:
            parts = name.split()
            first_name = parts[0] if parts else ""
            last_name = " ".join(parts[1:]) if len(parts) > 1 else ""
        
        # Step 1: Get comprehensive company intelligence
        logger.info(f" Starting Claude Opus 4 intelligent augmentation for {email}")
        company_intel = await claude_intelligence.extract_company_intelligence(domain)
        
        # Step 2: Use company context for person intelligence
        person_intel = await claude_intelligence.extract_person_intelligence(
            first_name, last_name, email, company_intel
        )
        
        # Format response
        return {
            "success": True,
            "method": "claude_opus_4_intelligent_augmentation",
            "person": {
                "name": person_intel.name,
                "title": person_intel.title,
                "professional_background": person_intel.professional_background,
                "connections": person_intel.connections,
                "confidence": person_intel.confidence_score
            },
            "company": {
                "name": company_intel.name,
                "industry": company_intel.industry,
                "description": company_intel.description,
                "size": company_intel.size,
                "founded": company_intel.founded,
                "location": company_intel.location,
                "funding": company_intel.funding,
                "leadership": company_intel.leadership,
                "key_people": company_intel.key_people,
                "products": company_intel.products,
                "technologies": company_intel.technologies,
                "recent_news": company_intel.recent_news,
                "confidence": company_intel.confidence_score
            },
            "intelligence_summary": {
                "method": "claude_opus_4_intelligent_orchestration",
                "person_confidence": person_intel.confidence_score,
                "company_confidence": company_intel.confidence_score,
                "overall_confidence": (person_intel.confidence_score + company_intel.confidence_score) / 2
            }
        }
        
    except Exception as e:
        logger.error(f"Claude intelligent augmentation failed for {email}: {e}")
        return {"error": str(e), "method": "claude_opus_4_intelligent_augmentation"}

# Keep existing functions for compatibility
get_web_intelligence = get_search_intelligence 

================================================================================
FILE: intelligence/calendar/__init__.py
SIZE: 211 bytes
LINES: 6
================================================================================

"""
Calendar Intelligence Module
==========================
This module provides functionality for analyzing calendar events and extracting
strategic intelligence from scheduling patterns and event details.
"""

================================================================================
FILE: intelligence/calendar/calendar_intelligence.py
SIZE: 21,789 bytes
LINES: 538
================================================================================

import logging
import json
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
import asyncio
from collections import defaultdict

logger = logging.getLogger(__name__)

@dataclass
class MeetingContext:
    """Rich context for an upcoming meeting"""
    event_id: str
    meeting_title: str
    start_time: datetime
    attendees: List[Dict]
    historical_context: Dict
    preparation_tasks: List[Dict]
    talking_points: List[str]
    relationship_insights: Dict
    strategic_importance: float
    predicted_outcomes: List[Dict]

class CalendarIntelligenceEngine:
    """Integrates calendar with knowledge tree for predictive meeting intelligence"""
    
    def __init__(self, storage_manager, claude_client):
        self.storage = storage_manager
        self.claude = claude_client
        
    async def analyze_upcoming_meetings(self, user_id: int, days_ahead: int = 7) -> List[MeetingContext]:
        """Analyze upcoming meetings with full context from knowledge tree"""
        try:
            # Get upcoming calendar events
            calendar_events = await self._fetch_upcoming_events(user_id, days_ahead)
            
            # Get knowledge tree
            knowledge_tree = self.storage.get_latest_knowledge_tree(user_id)
            
            # Analyze each meeting in parallel
            meeting_contexts = []
            analysis_tasks = []
            
            for event in calendar_events:
                task = self._analyze_single_meeting(event, knowledge_tree, user_id)
                analysis_tasks.append(task)
            
            results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, MeetingContext):
                    meeting_contexts.append(result)
                else:
                    logger.error(f"Meeting analysis error: {result}")
            
            # Sort by strategic importance and time
            meeting_contexts.sort(
                key=lambda m: (m.strategic_importance, -m.start_time.timestamp()),
                reverse=True
            )
            
            return meeting_contexts
            
        except Exception as e:
            logger.error(f"Error analyzing upcoming meetings: {str(e)}")
            return []
    
    async def _analyze_single_meeting(self, event: Dict, knowledge_tree: Dict, user_id: int) -> MeetingContext:
        """Deep analysis of a single meeting"""
        # Extract attendee emails
        attendee_emails = self._extract_attendee_emails(event)
        
        # Get historical context for each attendee
        historical_context = await self._gather_historical_context(
            attendee_emails, knowledge_tree, user_id
        )
        
        # Get relationship insights
        relationship_insights = await self._analyze_attendee_relationships(
            attendee_emails, knowledge_tree, user_id
        )
        
        # Generate meeting-specific insights using Claude
        meeting_insights = await self._generate_meeting_insights(
            event, historical_context, relationship_insights
        )
        
        # Create preparation tasks
        prep_tasks = self._generate_preparation_tasks(
            event, meeting_insights, historical_context
        )
        
        # Predict likely outcomes
        predicted_outcomes = await self._predict_meeting_outcomes(
            event, historical_context, relationship_insights
        )
        
        return MeetingContext(
            event_id=event['id'],
            meeting_title=event.get('summary', 'Untitled Meeting'),
            start_time=datetime.fromisoformat(event['start']['dateTime']),
            attendees=[{'email': email} for email in attendee_emails],  # Simplified attendees list
            historical_context=historical_context,
            preparation_tasks=prep_tasks,
            talking_points=meeting_insights.get('talking_points', []),
            relationship_insights=relationship_insights,
            strategic_importance=meeting_insights.get('importance_score', 0.5),
            predicted_outcomes=predicted_outcomes
        )
    
    async def _gather_historical_context(self, attendee_emails: List[str], 
                                       knowledge_tree: Dict, user_id: int) -> Dict:
        """Gather all historical context about meeting attendees"""
        context = {
            'previous_meetings': [],
            'email_threads': [],
            'shared_projects': [],
            'discussion_topics': [],
            'decision_history': [],
            'action_items_history': []
        }
        
        for email in attendee_emails:
            # Get previous meetings with this person
            prev_meetings = self.storage.get_meetings_with_attendee(user_id, email)
            context['previous_meetings'].extend(prev_meetings)
            
            # Get email threads
            email_threads = self.storage.get_email_threads_with_person(user_id, email)
            context['email_threads'].extend(email_threads)
            
            # Extract shared projects from knowledge tree
            if 'insights' in knowledge_tree:
                for insight_type, insights in knowledge_tree['insights'].items():
                    if isinstance(insights, dict):
                        for category, items in insights.items():
                            if isinstance(items, list):
                                for item in items:
                                    if self._involves_attendee(item, email):
                                        if 'project' in str(item).lower():
                                            context['shared_projects'].append(item)
                                        elif 'decision' in str(item).lower():
                                            context['decision_history'].append(item)
            
            # Get topics discussed with this person
            person_topics = self._extract_person_topics(email, knowledge_tree)
            context['discussion_topics'].extend(person_topics)
        
        # Deduplicate and sort by relevance
        for key in context:
            if isinstance(context[key], list):
                context[key] = self._deduplicate_and_rank(context[key])
        
        return context
    
    async def _analyze_attendee_relationships(self, attendee_emails: List[str], 
                                            knowledge_tree: Dict, user_id: int) -> Dict:
        """Analyze relationships between meeting attendees"""
        relationships = {
            'attendee_profiles': {},
            'influence_network': {},
            'collaboration_history': {},
            'communication_patterns': {},
            'potential_dynamics': []
        }
        
        # Get relationship data from graph database
        for email in attendee_emails:
            # Get person profile
            person = self.storage.get_person_by_email(user_id, email)
            if person:
                relationships['attendee_profiles'][email] = {
                    'name': person.get('name'),
                    'title': person.get('title'),
                    'company': person.get('company'),
                    'relationship_strength': person.get('engagement_score', 0),
                    'last_interaction': person.get('last_interaction')
                }
            
            # Get influence metrics from graph
            influence = self.storage.get_network_metrics(user_id, email)
            relationships['influence_network'][email] = influence
            
            # Get collaboration patterns
            collaborations = self._extract_collaboration_patterns(email, knowledge_tree)
            relationships['collaboration_history'][email] = collaborations
        
        # Analyze group dynamics
        if len(attendee_emails) > 2:
            relationships['potential_dynamics'] = await self._predict_group_dynamics(
                attendee_emails, relationships
            )
        
        return relationships
    
    async def _generate_meeting_insights(self, event: Dict, historical_context: Dict, 
                                       relationship_insights: Dict) -> Dict:
        """Use Claude to generate meeting-specific insights"""
        prompt = f"""Analyze this upcoming meeting and provide strategic insights:

Meeting: {event.get('summary')}
Time: {event.get('start')}
Attendees: {json.dumps(relationship_insights.get('attendee_profiles', {}))}

Historical Context:
- Previous meetings: {len(historical_context.get('previous_meetings', []))}
- Shared projects: {historical_context.get('shared_projects', [])}
- Recent decisions: {historical_context.get('decision_history', [])[-3:]}

Relationship Insights:
{json.dumps(relationship_insights, indent=2)}

Provide:
1. Strategic importance score (0-1)
2. Key talking points (list of 3-5)
3. Potential agenda items
4. Risks or sensitivities to be aware of
5. Opportunities to explore
6. Recommended preparation actions

Format as JSON."""

        response = await asyncio.to_thread(
            self.claude.messages.create,
            model="claude-3-opus-20240229",
            max_tokens=2000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            insights = json.loads(response.content[0].text)
            return insights
        except:
            return {
                'importance_score': 0.5,
                'talking_points': [],
                'opportunities': []
            }
    
    def _generate_preparation_tasks(self, event: Dict, insights: Dict, 
                                   historical_context: Dict) -> List[Dict]:
        """Generate specific preparation tasks for the meeting"""
        tasks = []
        
        # Review previous discussions
        if historical_context.get('email_threads'):
            tasks.append({
                'task': 'Review recent email threads with attendees',
                'priority': 'high',
                'time_estimate': '10 minutes',
                'resources': [thread['id'] for thread in historical_context['email_threads'][:3]]
            })
        
        # Prepare talking points
        if insights.get('talking_points'):
            tasks.append({
                'task': 'Prepare notes on key talking points',
                'priority': 'high',
                'time_estimate': '15 minutes',
                'details': insights['talking_points']
            })
        
        # Review shared projects
        if historical_context.get('shared_projects'):
            tasks.append({
                'task': 'Update on shared project status',
                'priority': 'medium',
                'time_estimate': '20 minutes',
                'projects': [p.get('name', 'Unknown') for p in historical_context['shared_projects']]
            })
        
        # Address sensitivities
        if insights.get('sensitivities'):
            tasks.append({
                'task': 'Review and prepare for sensitive topics',
                'priority': 'high',
                'time_estimate': '10 minutes',
                'details': insights['sensitivities']
            })
        
        return tasks
    
    async def _predict_meeting_outcomes(self, event: Dict, historical_context: Dict,
                                      relationship_insights: Dict) -> List[Dict]:
        """Predict likely meeting outcomes based on patterns"""
        predictions = []
        
        # Analyze historical meeting patterns
        if historical_context.get('previous_meetings'):
            patterns = self._analyze_meeting_patterns(historical_context['previous_meetings'])
            
            if patterns.get('typical_outcomes'):
                predictions.extend([
                    {
                        'outcome': outcome,
                        'probability': prob,
                        'based_on': 'historical patterns'
                    }
                    for outcome, prob in patterns['typical_outcomes'].items()
                ])
        
        # Consider relationship dynamics
        dynamics = relationship_insights.get('potential_dynamics', [])
        if dynamics:
            predictions.append({
                'outcome': 'Group dynamics assessment',
                'probability': 0.7,
                'details': dynamics,
                'based_on': 'relationship analysis'
            })
        
        return predictions
    
    async def generate_meeting_brief(self, meeting_context: MeetingContext) -> str:
        """Generate a comprehensive meeting brief"""
        brief_template = f"""
# Meeting Brief: {meeting_context.meeting_title}
**Date/Time**: {meeting_context.start_time.strftime('%Y-%m-%d %H:%M')}
**Strategic Importance**: {meeting_context.strategic_importance:.1%}

## Attendees
{self._format_attendee_summary(meeting_context.attendees)}

## Historical Context
{self._format_historical_summary(meeting_context.historical_context)}

## Key Talking Points
{self._format_talking_points(meeting_context.talking_points)}

## Preparation Tasks
{self._format_prep_tasks(meeting_context.preparation_tasks)}

## Relationship Dynamics
{self._format_relationship_insights(meeting_context.relationship_insights)}

## Predicted Outcomes
{self._format_predictions(meeting_context.predicted_outcomes)}
"""
        return brief_template
    
    def _extract_attendee_emails(self, event: Dict) -> List[str]:
        """Extract attendee emails from calendar event"""
        emails = []
        attendees = event.get('attendees', [])
        
        for attendee in attendees:
            if attendee.get('email'):
                emails.append(attendee['email'].lower())
        
        # Add organizer
        if event.get('organizer', {}).get('email'):
            emails.append(event['organizer']['email'].lower())
        
        return list(set(emails))
    
    def _involves_attendee(self, item: Dict, email: str) -> bool:
        """Check if an item involves a specific attendee"""
        item_str = json.dumps(item).lower()
        return email.lower() in item_str or email.split('@')[0].lower() in item_str
    
    def _extract_person_topics(self, email: str, knowledge_tree: Dict) -> List[str]:
        """Extract topics associated with a person"""
        topics = set()
        
        # Search through all topics in knowledge tree
        if 'topics' in knowledge_tree:
            for topic in knowledge_tree['topics']:
                if self._involves_attendee({'topic': topic}, email):
                    topics.add(topic)
        
        return list(topics)
    
    def _deduplicate_and_rank(self, items: List[Any]) -> List[Any]:
        """Deduplicate and rank items by relevance"""
        # Simple deduplication - enhance with similarity matching
        seen = set()
        unique_items = []
        
        for item in items:
            item_key = str(item)[:100]  # Use first 100 chars as key
            if item_key not in seen:
                seen.add(item_key)
                unique_items.append(item)
        
        return unique_items[:10]  # Return top 10
    
    def _extract_collaboration_patterns(self, email: str, knowledge_tree: Dict) -> Dict:
        """Extract collaboration patterns for a person"""
        patterns = {
            'projects_together': 0,
            'decisions_together': 0,
            'topics_overlap': [],
            'communication_frequency': 'unknown'
        }
        
        # Count collaborative instances in knowledge tree
        if 'insights' in knowledge_tree:
            for insight_type, insights in knowledge_tree['insights'].items():
                if 'project' in insight_type.lower():
                    patterns['projects_together'] += self._count_collaborations(insights, email)
                elif 'decision' in insight_type.lower():
                    patterns['decisions_together'] += self._count_collaborations(insights, email)
        
        return patterns
    
    def _count_collaborations(self, insights: Dict, email: str) -> int:
        """Count collaborative instances involving a person"""
        count = 0
        insights_str = json.dumps(insights).lower()
        email_username = email.split('@')[0].lower()
        
        if email.lower() in insights_str or email_username in insights_str:
            count += insights_str.count(email_username)
        
        return min(count, 10)  # Cap at 10
    
    async def _predict_group_dynamics(self, attendees: List[str], relationships: Dict) -> List[str]:
        """Predict group dynamics for meetings with multiple attendees"""
        dynamics = []
        
        # Check for hierarchy
        influence_scores = relationships.get('influence_network', {})
        if influence_scores:
            max_influence = max(influence_scores.values(), key=lambda x: x.get('influence_rank', 0))
            if max_influence.get('influence_rank', 0) > 0.7:
                dynamics.append("High-influence attendee present - may dominate discussion")
        
        # Check for potential conflicts
        # This would need more sophisticated analysis in production
        
        return dynamics
    
    async def _fetch_upcoming_events(self, user_id: int, days_ahead: int) -> List[Dict]:
        """Fetch upcoming calendar events"""
        # This would integrate with Google Calendar API
        # For now, return from database
        start_date = datetime.utcnow()
        end_date = start_date + timedelta(days=days_ahead)
        
        events = self.storage.get_calendar_events(
            user_id, 
            start_date=start_date,
            end_date=end_date
        )
        
        return [event for event in events if event.get('status') != 'cancelled']
    
    def _format_attendee_summary(self, attendees: List[Dict]) -> str:
        """Format attendee information for the brief"""
        lines = []
        for attendee in attendees:
            line = f"- **{attendee.get('name', 'Unknown')}**"
            if attendee.get('title'):
                line += f" - {attendee['title']}"
            if attendee.get('company'):
                line += f" at {attendee['company']}"
            lines.append(line)
        return '\n'.join(lines)
    
    def _format_historical_summary(self, context: Dict) -> str:
        """Format historical context for the brief"""
        summary = []
        
        if context.get('previous_meetings'):
            summary.append(f"- {len(context['previous_meetings'])} previous meetings")
        
        if context.get('shared_projects'):
            projects = [p.get('name', 'Unknown') for p in context['shared_projects'][:3]]
            summary.append(f"- Shared projects: {', '.join(projects)}")
        
        if context.get('discussion_topics'):
            topics = context['discussion_topics'][:5]
            summary.append(f"- Common topics: {', '.join(topics)}")
        
        return '\n'.join(summary) if summary else "No significant historical context found"
    
    def _format_talking_points(self, points: List[str]) -> str:
        """Format talking points for the brief"""
        if not points:
            return "No specific talking points identified"
        
        return '\n'.join([f"{i+1}. {point}" for i, point in enumerate(points)])
    
    def _format_prep_tasks(self, tasks: List[Dict]) -> str:
        """Format preparation tasks for the brief"""
        if not tasks:
            return "No specific preparation required"
        
        lines = []
        for task in tasks:
            line = f"- [ ] {task['task']} ({task['time_estimate']})"
            if task.get('priority') == 'high':
                line = f"- [ ] **{task['task']}** ({task['time_estimate']}) "
            lines.append(line)
        
        return '\n'.join(lines)
    
    def _format_relationship_insights(self, insights: Dict) -> str:
        """Format relationship insights for the brief"""
        lines = []
        
        # Add influence network summary
        if insights.get('influence_network'):
            high_influence = [
                email for email, metrics in insights['influence_network'].items()
                if metrics.get('influence_rank', 0) > 0.6
            ]
            if high_influence:
                lines.append(f"- High-influence attendees: {', '.join(high_influence)}")
        
        # Add collaboration history
        if insights.get('collaboration_history'):
            for email, history in insights['collaboration_history'].items():
                if history.get('projects_together', 0) > 0:
                    lines.append(f"- Collaborated with {email} on {history['projects_together']} projects")
        
        return '\n'.join(lines) if lines else "Standard professional relationships"
    
    def _format_predictions(self, predictions: List[Dict]) -> str:
        """Format predicted outcomes for the brief"""
        if not predictions:
            return "No specific predictions available"
        
        lines = []
        for pred in predictions:
            prob_str = f"{pred['probability']:.0%}" if pred.get('probability') else 'Likely'
            lines.append(f"- {pred['outcome']} ({prob_str})")
            if pred.get('details'):
                lines.append(f"  - {pred['details']}")
        
        return '\n'.join(lines)

# Usage example:
# storage_manager = StorageManager()
# claude_client = AnthropicClient()
# calendar_intelligence = CalendarIntelligenceEngine(storage_manager, claude_client)

================================================================================
FILE: intelligence/knowledge_tree/__init__.py
SIZE: 202 bytes
LINES: 6
================================================================================

"""
Knowledge Tree Intelligence Module
=================================
This module contains components for building, managing, and augmenting
knowledge trees in the strategic intelligence system.
"""

================================================================================
FILE: intelligence/knowledge_tree/augmentation_engine.py
SIZE: 11,751 bytes
LINES: 333
================================================================================

"""
Knowledge Tree Augmentation Engine
=================================
This module provides functionality for augmenting and enriching knowledge tree nodes
using AI-powered analysis and external data sources.
"""

from typing import Dict, List, Optional, Any, Set, Tuple
import uuid
from datetime import datetime
import json

from models.knowledge_tree import KnowledgeNode, RelationType
from utils.logging import get_logger
from utils.helpers import retry_with_backoff, extract_entities_from_text

logger = get_logger(__name__)


class AugmentationEngine:
    """
    Engine for augmenting and enriching knowledge tree nodes with additional intelligence.
    """
    
    def __init__(self, user_id: str):
        """
        Initialize the augmentation engine.
        
        Args:
            user_id: ID of the user owning this augmentation process
        """
        self.user_id = user_id
        self._llm_api_key = None  # Would be loaded from secure storage
        
    def augment_node(self, node: KnowledgeNode) -> KnowledgeNode:
        """
        Augment a knowledge node with additional information and connections.
        
        Args:
            node: Knowledge node to augment
            
        Returns:
            Augmented knowledge node
        """
        logger.info(f"Augmenting knowledge node: {node.node_id}")
        
        # Extract entities and concepts from node content
        extracted_data = self._extract_entities_and_concepts(node.content)
        
        # Add extracted entities as tags
        for tag in extracted_data.get("tags", []):
            node.add_tag(tag)
        
        # Add extracted metadata
        for key, value in extracted_data.get("metadata", {}).items():
            if key not in node.metadata:
                node.metadata[key] = value
        
        # Enrich with AI analysis (assuming this would call an LLM)
        ai_enrichment = self._generate_ai_enrichment(node)
        
        # Create new nodes and relationships based on AI analysis
        for related_fact in ai_enrichment.get("related_facts", []):
            # Create a new child node for the related fact
            child_node = KnowledgeNode(
                user_id=self.user_id,
                content=related_fact["content"],
                node_type="INFERRED",
                confidence=related_fact.get("confidence", 0.7),
                metadata={"inferred_from": node.node_id}
            )
            
            # Set source reference to original node
            source_ref = {
                "source_id": node.node_id,
                "source_type": "knowledge_node",
                "timestamp": datetime.utcnow().isoformat()
            }
            child_node.add_source(source_ref)
            
            # Save the child node
            child_node.save()
            
            # Connect the nodes
            relation_type = related_fact.get("relation_type", "RELATED_TO")
            node.add_relation(
                target_id=child_node.node_id,
                relation_type=relation_type,
                metadata={"confidence": related_fact.get("confidence", 0.7)}
            )
            
        # Save updated node
        node.save()
        
        return node
        
    def augment_branch(self, root_node_id: str, max_depth: int = 3) -> int:
        """
        Augment an entire branch of the knowledge tree.
        
        Args:
            root_node_id: ID of the root node to start from
            max_depth: Maximum depth to traverse
            
        Returns:
            Number of nodes augmented
        """
        # Load root node
        root_node = KnowledgeNode.load(self.user_id, root_node_id)
        if not root_node:
            logger.error(f"Root node not found: {root_node_id}")
            return 0
            
        # Track visited nodes to avoid cycles
        visited = set()
        augmented_count = 0
        
        def _augment_recursive(node: KnowledgeNode, current_depth: int) -> int:
            if node.node_id in visited:
                return 0
                
            if current_depth > max_depth:
                return 0
                
            # Mark as visited
            visited.add(node.node_id)
            
            # Augment the node
            self.augment_node(node)
            count = 1  # Count this node
            
            # Process children recursively
            relations = node.get_relations()
            for relation in relations:
                # Only traverse outgoing relations
                child_node = KnowledgeNode.load(self.user_id, relation['target_id'])
                if child_node:
                    count += _augment_recursive(child_node, current_depth + 1)
                    
            return count
            
        # Start recursive augmentation from root
        augmented_count = _augment_recursive(root_node, 1)
        
        logger.info(f"Augmented {augmented_count} nodes in branch starting from {root_node_id}")
        return augmented_count
        
    def find_cross_connections(self, node_ids: List[str], min_similarity: float = 0.7) -> List[Dict[str, Any]]:
        """
        Find potential connections between multiple nodes.
        
        Args:
            node_ids: List of node IDs to analyze
            min_similarity: Minimum similarity threshold
            
        Returns:
            List of potential connections
        """
        connections = []
        nodes = []
        
        # Load all nodes
        for node_id in node_ids:
            node = KnowledgeNode.load(self.user_id, node_id)
            if node:
                nodes.append(node)
        
        # Compare each node with every other
        for i in range(len(nodes)):
            for j in range(i + 1, len(nodes)):
                similarity = self._calculate_node_similarity(nodes[i], nodes[j])
                
                if similarity >= min_similarity:
                    connection = {
                        "source_id": nodes[i].node_id,
                        "target_id": nodes[j].node_id,
                        "similarity": similarity,
                        "suggested_relation": self._determine_relation_type(nodes[i], nodes[j])
                    }
                    connections.append(connection)
        
        return connections
                
    def _extract_entities_and_concepts(self, text: str) -> Dict[str, Any]:
        """
        Extract entities and concepts from text.
        
        Args:
            text: Text to analyze
            
        Returns:
            Dictionary with extracted data
        """
        # Extract basic entities using helper
        entities = extract_entities_from_text(text)
        
        # Convert to tags and metadata
        tags = []
        metadata = {}
        
        # Add emails as tags
        tags.extend([f"email:{email}" for email in entities.get("emails", [])])
        
        # Add URLs as metadata
        if entities.get("urls"):
            metadata["referenced_urls"] = entities.get("urls")
            
        # Add phone numbers as metadata
        if entities.get("phone_numbers"):
            metadata["referenced_phones"] = entities.get("phone_numbers")
            
        # Add dates as metadata
        if entities.get("dates"):
            metadata["referenced_dates"] = entities.get("dates")
            
        # In a real implementation, we would use NLP to extract:
        # - Named entities (people, organizations, locations)
        # - Key concepts and topics
        # - Sentiments and opinions
        
        # Simulate results for now
        sample_tags = ["finance", "strategy", "meeting", "quarterly", "project"]
        tags.extend(sample_tags[:2])  # Add a couple of sample tags
        
        return {
            "tags": tags,
            "metadata": metadata
        }
        
    @retry_with_backoff(max_retries=2)
    def _generate_ai_enrichment(self, node: KnowledgeNode) -> Dict[str, Any]:
        """
        Use AI to enrich node with related facts and insights.
        
        Args:
            node: Knowledge node to enrich
            
        Returns:
            Dictionary with AI-generated enrichments
        """
        # In a real implementation, this would call an LLM API
        # to generate insights, inferences, and related facts
        
        logger.info(f"Generating AI enrichment for node: {node.node_id}")
        
        # Simulate AI enrichment for demonstration
        content_preview = node.content[:30] + "..." if len(node.content) > 30 else node.content
        
        # Generate some related facts based on node type
        related_facts = []
        
        if node.node_type == "FACT":
            # For facts, generate possible implications
            related_facts = [
                {
                    "content": f"Based on the fact that {content_preview}, we can infer...",
                    "relation_type": "IMPLIES",
                    "confidence": 0.85
                },
                {
                    "content": f"The fact {content_preview} suggests a potential opportunity in...",
                    "relation_type": "SUGGESTS",
                    "confidence": 0.75
                }
            ]
        elif node.node_type == "INFERRED":
            # For inferences, generate related predictions
            related_facts = [
                {
                    "content": f"If the inference {content_preview} holds true, then...",
                    "relation_type": "LEADS_TO",
                    "confidence": 0.65
                }
            ]
        
        return {
            "related_facts": related_facts,
            "sentiment": "neutral",  # Simple sentiment analysis
            "priority_score": 0.6    # Importance score
        }
        
    def _calculate_node_similarity(self, node1: KnowledgeNode, node2: KnowledgeNode) -> float:
        """
        Calculate similarity between two knowledge nodes.
        
        Args:
            node1: First node
            node2: Second node
            
        Returns:
            Similarity score between 0.0 and 1.0
        """
        # In a real implementation, this would use advanced NLP techniques
        # like semantic similarity with embeddings
        
        # Simple implementation based on shared tags
        tags1 = set(node1.tags)
        tags2 = set(node2.tags)
        
        if not tags1 or not tags2:
            return 0.0
        
        # Jaccard similarity for tags
        intersection = len(tags1.intersection(tags2))
        union = len(tags1.union(tags2))
        
        tag_similarity = intersection / max(1, union)
        
        # Could combine with other similarity measures
        return tag_similarity
        
    def _determine_relation_type(self, source_node: KnowledgeNode, target_node: KnowledgeNode) -> str:
        """
        Determine the most appropriate relation type between nodes.
        
        Args:
            source_node: Source node
            target_node: Target node
            
        Returns:
            Suggested relation type
        """
        # This would use more sophisticated logic in a real implementation
        
        # Simple rules for now
        if source_node.node_type == "FACT" and target_node.node_type == "INFERRED":
            return "IMPLIES"
        elif source_node.node_type == "FACT" and target_node.node_type == "FACT":
            return "RELATED_TO"
        elif source_node.node_type == "INFERRED" and target_node.node_type == "PREDICTION":
            return "LEADS_TO"
        else:
            return "RELATED_TO"

================================================================================
FILE: intelligence/knowledge_tree/multidimensional_matrix.py
SIZE: 40,628 bytes
LINES: 985
================================================================================

"""
Multidimensional Knowledge Matrix
===============================
Builds a rich, interconnected matrix of the user's world that goes beyond
simple categorization to create deep conceptual understanding.
"""

import json
import asyncio
from typing import Dict, List, Optional, Any, Set, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass

from utils.logging import structured_logger as logger
from intelligence.analysts.base_analyst import BaseAnalyst
from intelligence.claude_analysis import (
    BusinessStrategyAnalyst, RelationshipDynamicsAnalyst, 
    TechnicalEvolutionAnalyst, MarketIntelligenceAnalyst, PredictiveAnalyst
)

@dataclass
class ConceptualFramework:
    """Represents a conceptual framework or mental model"""
    name: str
    description: str
    key_concepts: List[str]
    relationships: List[Dict]
    evidence: List[Dict]
    confidence: float
    domain: str

@dataclass
class ValueSystem:
    """Represents the user's values and priorities"""
    value_name: str
    importance: float
    manifestations: List[str]
    conflicts: List[str]
    evolution: List[Dict]

@dataclass
class DecisionPattern:
    """Represents how the user makes decisions"""
    context: str
    decision_factors: List[str]
    patterns: List[str]
    outcomes: List[Dict]
    confidence: float

class MultidimensionalKnowledgeMatrix:
    """
    Builds a rich, interconnected matrix of the user's world.
    
    This goes beyond simple categorization to understand:
    - How the user thinks about their world (conceptual frameworks)
    - What matters to them (value systems)
    - How they make decisions (decision patterns)
    - How different domains connect (cross-domain links)
    - Their unique perspective and mental models
    """
    
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.matrix = {
            "conceptual_frameworks": {},  # How you think about the world
            "value_systems": {},          # What matters to you
            "relationship_networks": {},  # Your connections and dynamics
            "decision_patterns": {},      # How you make choices
            "thematic_structures": {},    # Recurring themes and patterns
            "cross_domain_links": [],     # Connections between areas
            "temporal_evolution": {},     # How things change over time
            "opportunity_landscape": {},  # Strategic opportunities
            "risk_assessment": {},        # Potential risks and mitigation
            "influence_map": {}           # Spheres of influence and leverage
        }
        
        # Enhanced analysts with worldview-focused prompts
        self.worldview_analysts = {
            'conceptual_modeler': ConceptualModelingAnalyst(user_id),
            'pattern_recognition': PatternRecognitionAnalyst(user_id),
            'cross_domain_connector': CrossDomainConnector(user_id),
            'worldview_synthesizer': WorldviewSynthesizer(user_id),
            'strategic_insights': StrategicInsightsAnalyst(user_id)
        }
    
    async def build_multidimensional_matrix(self, user_id: int, time_window_days: int = 30) -> Dict:
        """Build comprehensive multidimensional matrix from emails"""
        logger.info(f"Building multidimensional knowledge matrix for user {user_id}")
        
        try:
            # First pass: Extract basic entities and facts
            initial_tree = await self._extract_initial_knowledge(user_id, time_window_days)
            
            # Second pass: Find patterns and relationships
            patterns = await self._identify_deep_patterns(initial_tree, user_id, time_window_days)
            
            # Third pass: Build conceptual frameworks
            frameworks = await self._build_conceptual_frameworks(initial_tree, patterns, user_id, time_window_days)
            
            # Fourth pass: Connect across domains
            cross_domain = await self._connect_domains(frameworks, user_id, time_window_days)
            
            # Fifth pass: Synthesize coherent worldview
            worldview = await self._synthesize_worldview(cross_domain, user_id, time_window_days)
            
            # Create hierarchical structure for navigation
            hierarchical_structure = self._create_hierarchical_structure(worldview)
            
            # Generate insights and recommendations
            insights = await self._generate_strategic_insights(worldview, user_id, time_window_days)
            
            final_matrix = {
                **worldview,
                "hierarchical_structure": hierarchical_structure,
                "strategic_insights": insights,
                "matrix_metadata": {
                    "build_timestamp": datetime.utcnow().isoformat(),
                    "time_window_days": time_window_days,
                    "analysis_depth": "multidimensional",
                    "user_id": user_id
                }
            }
            
            logger.info(f"Multidimensional matrix built successfully with {len(final_matrix)} top-level components")
            return final_matrix
            
        except Exception as e:
            logger.error(f"Multidimensional matrix building error: {str(e)}")
            return {"error": str(e), "status": "failed"}
    
    async def _extract_initial_knowledge(self, user_id: int, time_window_days: int) -> Dict:
        """First pass: Extract basic entities, facts, and relationships"""
        logger.info("Pass 1: Extracting initial knowledge and entities")
        
        from models.database import get_db_manager
        db_manager = get_db_manager()
        
        # Get emails for analysis
        emails = await self._get_emails_for_window(user_id, time_window_days)
        
        if not emails:
            return {"status": "no_data"}
        
        # Run basic analysis to extract entities and facts
        basic_analysts = {
            'business': BusinessStrategyAnalyst(),
            'relationships': RelationshipDynamicsAnalyst(),
            'technical': TechnicalEvolutionAnalyst(),
            'market': MarketIntelligenceAnalyst(),
            'predictive': PredictiveAnalyst()
        }
        
        # Run analyses in parallel
        analysis_tasks = []
        for analyst_name, analyst in basic_analysts.items():
            task = analyst.analyze_emails(emails)
            analysis_tasks.append(task)
        
        results = await asyncio.gather(*analysis_tasks)
        
        # Merge basic results
        initial_knowledge = {
            'entities': [],
            'relationships': [],
            'facts': [],
            'topics': set(),
            'timeframes': [],
            'email_context': emails[:10]  # Keep sample for context
        }
        
        for result in results:
            initial_knowledge['entities'].extend(result.entities)
            initial_knowledge['relationships'].extend(result.relationships)
            initial_knowledge['topics'].update(result.topics)
        
        initial_knowledge['topics'] = list(initial_knowledge['topics'])
        
        return initial_knowledge
    
    async def _identify_deep_patterns(self, initial_tree: Dict, user_id: int, time_window_days: int) -> Dict:
        """Second pass: Find deep patterns and recurring themes"""
        logger.info("Pass 2: Identifying deep patterns and themes")
        
        emails = await self._get_emails_for_window(user_id, time_window_days)
        
        # Use enhanced pattern recognition
        pattern_analyst = PatternRecognitionAnalyst(user_id)
        patterns = await pattern_analyst.analyze_patterns(emails, initial_tree)
        
        return patterns
    
    async def _build_conceptual_frameworks(self, initial_tree: Dict, patterns: Dict, user_id: int, time_window_days: int) -> Dict:
        """Third pass: Build conceptual frameworks and mental models"""
        logger.info("Pass 3: Building conceptual frameworks")
        
        emails = await self._get_emails_for_window(user_id, time_window_days)
        
        # Use conceptual modeling analyst
        frameworks_analyst = ConceptualModelingAnalyst(user_id)
        frameworks = await frameworks_analyst.build_frameworks(emails, initial_tree, patterns)
        
        return frameworks
    
    async def _connect_domains(self, frameworks: Dict, user_id: int, time_window_days: int) -> Dict:
        """Fourth pass: Connect across domains to find intersections"""
        logger.info("Pass 4: Connecting domains and finding intersections")
        
        emails = await self._get_emails_for_window(user_id, time_window_days)
        
        # Use cross-domain connector
        connector = CrossDomainConnector(user_id)
        cross_domain = await connector.connect_domains(emails, frameworks)
        
        return cross_domain
    
    async def _synthesize_worldview(self, cross_domain: Dict, user_id: int, time_window_days: int) -> Dict:
        """Fifth pass: Synthesize coherent worldview"""
        logger.info("Pass 5: Synthesizing coherent worldview")
        
        emails = await self._get_emails_for_window(user_id, time_window_days)
        
        # Use worldview synthesizer
        synthesizer = WorldviewSynthesizer(user_id)
        worldview = await synthesizer.synthesize_worldview(emails, cross_domain)
        
        return worldview
    
    def _create_hierarchical_structure(self, worldview: Dict) -> Dict:
        """Create navigable hierarchical structure from knowledge matrix"""
        logger.info("Creating hierarchical navigation structure")
        
        hierarchy = {}
        
        # Create top-level categories from domains
        for domain, content in worldview.get("thematic_structures", {}).items():
            hierarchy[domain] = {
                "subcategories": {},
                "content": content.get("summary", ""),
                "importance": content.get("importance", 0.5),
                "connections": content.get("connections", [])
            }
            
            # Add second-level categories
            for subcategory, details in content.get("components", {}).items():
                hierarchy[domain]["subcategories"][subcategory] = {
                    "items": {},
                    "content": details.get("summary", ""),
                    "insights": details.get("insights", [])
                }
                
                # Add leaf items
                for item_name, item_data in details.get("items", {}).items():
                    hierarchy[domain]["subcategories"][subcategory]["items"][item_name] = {
                        "data": item_data,
                        "evidence": item_data.get("evidence", []),
                        "confidence": item_data.get("confidence", 0.5)
                    }
        
        return hierarchy
    
    async def _generate_strategic_insights(self, worldview: Dict, user_id: int, time_window_days: int) -> Dict:
        """Generate strategic insights and recommendations"""
        logger.info("Generating strategic insights and recommendations")
        
        emails = await self._get_emails_for_window(user_id, time_window_days)
        
        # Use strategic insights generator
        insights_analyst = StrategicInsightsAnalyst(user_id)
        insights = await insights_analyst.generate_insights(emails, worldview)
        
        return insights
    
    async def _get_emails_for_window(self, user_id: int, days: int) -> List[Dict]:
        """Get emails within time window - using real emails, not mock data"""
        from models.database import get_db_manager
        from datetime import datetime, timedelta
        
        db_manager = get_db_manager()
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        # Get emails - this should now use real data since we fixed the OAuth issue
        emails = db_manager.get_user_emails(user_id, limit=1000)
        
        # Filter by date
        filtered_emails = []
        for email in emails:
            if email.email_date and email.email_date > cutoff_date:
                filtered_emails.append(email.to_dict())
        
        logger.info(f"Retrieved {len(filtered_emails)} real emails for analysis")
        return filtered_emails


# Enhanced specialized analysts for worldview construction

class ConceptualModelingAnalyst(BaseAnalyst):
    """Builds conceptual frameworks and mental models"""
    
    def __init__(self, user_id: int):
        super().__init__(user_id)
        self.analyst_name = "conceptual_modeling"
        self.analyst_description = "Conceptual frameworks and mental models analyst"
    
    def _prepare_email_context(self, emails: List[Dict]) -> str:
        """Prepare email context for Claude analysis"""
        context_parts = []
        for email in emails[:50]:  # Limit to prevent token overflow
            context_parts.append(f"""
Email ID: {email.get('id')}
Date: {email.get('email_date')}
From: {email.get('sender')} 
To: {email.get('recipients')}
Subject: {email.get('subject')}
Body: {email.get('body_text', '')[:1000]}...
---
""")
        return "\n".join(context_parts)
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """Generate conceptual frameworks intelligence"""
        emails = data.get('emails', [])
        initial_tree = data.get('initial_tree', {})
        patterns = data.get('patterns', {})
        
        return await self.build_frameworks(emails, initial_tree, patterns)
    
    async def build_frameworks(self, emails: List[Dict], initial_tree: Dict, patterns: Dict) -> Dict:
        """Build conceptual frameworks from email analysis"""
        
        system_prompt = """You are an expert in cognitive science and conceptual modeling. Your task is to understand how this person thinks about their world - their mental models, frameworks, and unique perspective.

Analyze the provided emails and extracted patterns to identify:

1. MENTAL MODELS: How do they conceptualize different domains (business, relationships, technology)?
2. DECISION FRAMEWORKS: What frameworks guide their thinking and choices?
3. WORLDVIEW PATTERNS: What consistent perspectives emerge across different contexts?
4. CONCEPTUAL CONNECTIONS: How do they link different ideas and domains?
5. UNIQUE LENS: What makes their perspective distinctive?

Focus on the WHY and HOW of their thinking, not just WHAT they think about.

Provide insights that would be revelatory - as if you deeply understand how they see their world in a way they might not have fully articulated themselves."""

        user_prompt = f"""
EMAILS FOR ANALYSIS:
{self._prepare_email_context(emails[:30])}

INITIAL KNOWLEDGE EXTRACTED:
{json.dumps(initial_tree, indent=2)}

PATTERNS IDENTIFIED:
{json.dumps(patterns, indent=2)}

Based on this data, identify the conceptual frameworks and mental models that guide this person's thinking. Focus on deep understanding rather than surface categorization.
"""

        response = await self._run_claude_analysis(system_prompt, user_prompt)
        return self._parse_frameworks_response(response)
    
    def _parse_frameworks_response(self, response: str) -> Dict:
        """Parse conceptual frameworks from Claude response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "conceptual_frameworks": self._extract_frameworks(response),
            "mental_models": self._extract_mental_models(response),
            "worldview_patterns": self._extract_worldview_patterns(response),
            "raw_analysis": response
        }
    
    def _extract_frameworks(self, response: str) -> List[Dict]:
        """Extract conceptual frameworks from response"""
        frameworks = []
        sections = response.split('\n\n')
        for section in sections:
            if 'framework' in section.lower() or 'model' in section.lower():
                frameworks.append({
                    "content": section.strip(),
                    "type": "conceptual_framework"
                })
        return frameworks
    
    def _extract_mental_models(self, response: str) -> List[Dict]:
        """Extract mental models from response"""
        models = []
        sections = response.split('\n\n')
        for section in sections:
            if 'mental model' in section.lower() or 'thinking' in section.lower():
                models.append({
                    "content": section.strip(),
                    "type": "mental_model"
                })
        return models
    
    def _extract_worldview_patterns(self, response: str) -> List[Dict]:
        """Extract worldview patterns from response"""
        patterns = []
        sections = response.split('\n\n')
        for section in sections:
            if 'worldview' in section.lower() or 'perspective' in section.lower():
                patterns.append({
                    "content": section.strip(),
                    "type": "worldview_pattern"
                })
        return patterns


class PatternRecognitionAnalyst(BaseAnalyst):
    """Identifies deep patterns and recurring themes"""
    
    def __init__(self, user_id: int):
        super().__init__(user_id)
        self.analyst_name = "pattern_recognition"
        self.analyst_description = "Deep patterns and recurring themes analyst"
    
    def _prepare_email_context(self, emails: List[Dict]) -> str:
        """Prepare email context for Claude analysis"""
        context_parts = []
        for email in emails[:50]:  # Limit to prevent token overflow
            context_parts.append(f"""
Email ID: {email.get('id')}
Date: {email.get('email_date')}
From: {email.get('sender')} 
To: {email.get('recipients')}
Subject: {email.get('subject')}
Body: {email.get('body_text', '')[:1000]}...
---
""")
        return "\n".join(context_parts)
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """Generate pattern recognition intelligence"""
        emails = data.get('emails', [])
        initial_tree = data.get('initial_tree', {})
        
        return await self.analyze_patterns(emails, initial_tree)
    
    async def analyze_patterns(self, emails: List[Dict], initial_tree: Dict) -> Dict:
        """Analyze deep patterns in communication and behavior"""
        
        system_prompt = """You are an expert pattern recognition analyst specializing in identifying deep, subtle patterns in human communication and behavior.

Your task is to find:
1. COMMUNICATION PATTERNS: Recurring styles, topics, and approaches
2. BEHAVIORAL PATTERNS: Consistent ways of approaching problems and decisions
3. TEMPORAL PATTERNS: How things evolve over time
4. RELATIONSHIP PATTERNS: Consistent dynamics with different people
5. STRATEGIC PATTERNS: Recurring strategic approaches and preferences

Look for patterns that are:
- Non-obvious but significant
- Consistent across different contexts
- Revelatory of deeper preferences and approaches
- Predictive of future behavior

Focus on patterns that provide strategic insight, not just descriptive categorization."""

        user_prompt = f"""
EMAILS FOR PATTERN ANALYSIS:
{self._prepare_email_context(emails[:40])}

INITIAL KNOWLEDGE BASE:
{json.dumps(initial_tree, indent=2)}

Identify deep patterns that reveal how this person operates, thinks, and approaches their world. Focus on patterns that would be strategically valuable to understand.
"""

        response = await self._run_claude_analysis(system_prompt, user_prompt)
        return self._parse_patterns_response(response)
    
    def _parse_patterns_response(self, response: str) -> Dict:
        """Parse patterns from Claude response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "communication_patterns": self._extract_communication_patterns(response),
            "behavioral_patterns": self._extract_behavioral_patterns(response),
            "temporal_patterns": self._extract_temporal_patterns(response),
            "relationship_patterns": self._extract_relationship_patterns(response),
            "strategic_patterns": self._extract_strategic_patterns(response),
            "raw_analysis": response
        }
    
    def _extract_communication_patterns(self, response: str) -> List[Dict]:
        """Extract communication patterns from response"""
        patterns = []
        sections = response.split('\n\n')
        for section in sections:
            if 'communication' in section.lower():
                patterns.append({
                    "content": section.strip(),
                    "type": "communication_pattern"
                })
        return patterns
    
    def _extract_behavioral_patterns(self, response: str) -> List[Dict]:
        """Extract behavioral patterns from response"""
        patterns = []
        sections = response.split('\n\n')
        for section in sections:
            if 'behavior' in section.lower() or 'approach' in section.lower():
                patterns.append({
                    "content": section.strip(),
                    "type": "behavioral_pattern"
                })
        return patterns
    
    def _extract_temporal_patterns(self, response: str) -> List[Dict]:
        """Extract temporal patterns from response"""
        patterns = []
        sections = response.split('\n\n')
        for section in sections:
            if 'temporal' in section.lower() or 'time' in section.lower():
                patterns.append({
                    "content": section.strip(),
                    "type": "temporal_pattern"
                })
        return patterns
    
    def _extract_relationship_patterns(self, response: str) -> List[Dict]:
        """Extract relationship patterns from response"""
        patterns = []
        sections = response.split('\n\n')
        for section in sections:
            if 'relationship' in section.lower():
                patterns.append({
                    "content": section.strip(),
                    "type": "relationship_pattern"
                })
        return patterns
    
    def _extract_strategic_patterns(self, response: str) -> List[Dict]:
        """Extract strategic patterns from response"""
        patterns = []
        sections = response.split('\n\n')
        for section in sections:
            if 'strategic' in section.lower() or 'strategy' in section.lower():
                patterns.append({
                    "content": section.strip(),
                    "type": "strategic_pattern"
                })
        return patterns


class CrossDomainConnector(BaseAnalyst):
    """Connects insights across different domains"""
    
    def __init__(self, user_id: int):
        super().__init__(user_id)
        self.analyst_name = "cross_domain_connector"
        self.analyst_description = "Cross-domain connections and systems thinking analyst"
    
    def _prepare_email_context(self, emails: List[Dict]) -> str:
        """Prepare email context for Claude analysis"""
        context_parts = []
        for email in emails[:50]:  # Limit to prevent token overflow
            context_parts.append(f"""
Email ID: {email.get('id')}
Date: {email.get('email_date')}
From: {email.get('sender')} 
To: {email.get('recipients')}
Subject: {email.get('subject')}
Body: {email.get('body_text', '')[:1000]}...
---
""")
        return "\n".join(context_parts)
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """Generate cross-domain intelligence"""
        emails = data.get('emails', [])
        frameworks = data.get('frameworks', {})
        
        return await self.connect_domains(emails, frameworks)
    
    async def connect_domains(self, emails: List[Dict], frameworks: Dict) -> Dict:
        """Find connections across different domains"""
        
        system_prompt = """You are an expert systems thinker specializing in finding connections across different domains and disciplines.

Your task is to identify how different areas of this person's life and work connect:
1. CROSS-DOMAIN INSIGHTS: How learnings from one area apply to another
2. SYSTEMIC CONNECTIONS: How different domains influence each other
3. LEVERAGE POINTS: Where action in one area creates impact in others
4. EMERGENT PROPERTIES: What emerges from the intersection of domains
5. STRATEGIC SYNTHESIS: How to leverage these connections strategically

Focus on non-obvious connections that create strategic advantage through integration."""

        user_prompt = f"""
EMAILS FOR ANALYSIS:
{self._prepare_email_context(emails[:30])}

CONCEPTUAL FRAMEWORKS IDENTIFIED:
{json.dumps(frameworks, indent=2)}

Identify the connections across different domains in this person's world. How do different areas of their life and work intersect and influence each other?
"""

        response = await self._run_claude_analysis(system_prompt, user_prompt)
        return self._parse_connections_response(response)
    
    def _parse_connections_response(self, response: str) -> Dict:
        """Parse cross-domain connections from response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "cross_domain_insights": self._extract_cross_domain_insights(response),
            "systemic_connections": self._extract_systemic_connections(response),
            "leverage_points": self._extract_leverage_points(response),
            "emergent_properties": self._extract_emergent_properties(response),
            "strategic_synthesis": self._extract_strategic_synthesis(response),
            "raw_analysis": response
        }
    
    def _extract_cross_domain_insights(self, response: str) -> List[Dict]:
        """Extract cross-domain insights from response"""
        insights = []
        sections = response.split('\n\n')
        for section in sections:
            if 'cross-domain' in section.lower() or 'across' in section.lower():
                insights.append({
                    "content": section.strip(),
                    "type": "cross_domain_insight"
                })
        return insights
    
    def _extract_systemic_connections(self, response: str) -> List[Dict]:
        """Extract systemic connections from response"""
        connections = []
        sections = response.split('\n\n')
        for section in sections:
            if 'systemic' in section.lower() or 'system' in section.lower():
                connections.append({
                    "content": section.strip(),
                    "type": "systemic_connection"
                })
        return connections
    
    def _extract_leverage_points(self, response: str) -> List[Dict]:
        """Extract leverage points from response"""
        points = []
        sections = response.split('\n\n')
        for section in sections:
            if 'leverage' in section.lower():
                points.append({
                    "content": section.strip(),
                    "type": "leverage_point"
                })
        return points
    
    def _extract_emergent_properties(self, response: str) -> List[Dict]:
        """Extract emergent properties from response"""
        properties = []
        sections = response.split('\n\n')
        for section in sections:
            if 'emergent' in section.lower():
                properties.append({
                    "content": section.strip(),
                    "type": "emergent_property"
                })
        return properties
    
    def _extract_strategic_synthesis(self, response: str) -> List[Dict]:
        """Extract strategic synthesis from response"""
        synthesis = []
        sections = response.split('\n\n')
        for section in sections:
            if 'synthesis' in section.lower() or 'strategic' in section.lower():
                synthesis.append({
                    "content": section.strip(),
                    "type": "strategic_synthesis"
                })
        return synthesis


class WorldviewSynthesizer(BaseAnalyst):
    """Synthesizes everything into a coherent worldview"""
    
    def __init__(self, user_id: int):
        super().__init__(user_id)
        self.analyst_name = "worldview_synthesizer"
        self.analyst_description = "Worldview synthesis and coherent understanding analyst"
    
    def _prepare_email_context(self, emails: List[Dict]) -> str:
        """Prepare email context for Claude analysis"""
        context_parts = []
        for email in emails[:50]:  # Limit to prevent token overflow
            context_parts.append(f"""
Email ID: {email.get('id')}
Date: {email.get('email_date')}
From: {email.get('sender')} 
To: {email.get('recipients')}
Subject: {email.get('subject')}
Body: {email.get('body_text', '')[:1000]}...
---
""")
        return "\n".join(context_parts)
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """Generate worldview synthesis intelligence"""
        emails = data.get('emails', [])
        cross_domain = data.get('cross_domain', {})
        
        return await self.synthesize_worldview(emails, cross_domain)
    
    async def synthesize_worldview(self, emails: List[Dict], cross_domain: Dict) -> Dict:
        """Synthesize everything into a coherent worldview"""
        
        system_prompt = """You are a master synthesizer who creates coherent worldviews from complex, multifaceted analysis.

Your task is to synthesize all the analysis into a unified understanding of how this person sees and operates in their world:

1. CORE WORLDVIEW: Their fundamental perspective on how the world works
2. VALUE ARCHITECTURE: The hierarchy of what matters to them and why
3. STRATEGIC PHILOSOPHY: Their approach to achieving goals and navigating challenges
4. RELATIONSHIP PHILOSOPHY: How they think about and manage relationships
5. LIFE DESIGN: How they structure and approach their life and work

Create a synthesis that feels complete, coherent, and revelatory - as if you truly understand this person's unique way of seeing and operating in the world."""

        user_prompt = f"""
EMAILS FOR SYNTHESIS:
{self._prepare_email_context(emails[:25])}

CROSS-DOMAIN ANALYSIS:
{json.dumps(cross_domain, indent=2)}

Synthesize all this analysis into a coherent worldview that captures how this person uniquely sees and operates in their world.
"""

        response = await self._run_claude_analysis(system_prompt, user_prompt)
        return self._parse_worldview_response(response)
    
    def _parse_worldview_response(self, response: str) -> Dict:
        """Parse synthesized worldview from response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "core_worldview": self._extract_core_worldview(response),
            "value_architecture": self._extract_value_architecture(response),
            "strategic_philosophy": self._extract_strategic_philosophy(response),
            "relationship_philosophy": self._extract_relationship_philosophy(response),
            "life_design": self._extract_life_design(response),
            "thematic_structures": self._extract_thematic_structures(response),
            "raw_analysis": response
        }
    
    def _extract_core_worldview(self, response: str) -> Dict:
        """Extract core worldview from response"""
        sections = response.split('\n\n')
        worldview = {}
        for i, section in enumerate(sections):
            if 'core' in section.lower() or 'worldview' in section.lower():
                worldview[f"core_belief_{i}"] = section.strip()
        return worldview
    
    def _extract_value_architecture(self, response: str) -> Dict:
        """Extract value architecture from response"""
        sections = response.split('\n\n')
        values = {}
        for i, section in enumerate(sections):
            if 'value' in section.lower():
                values[f"value_{i}"] = section.strip()
        return values
    
    def _extract_strategic_philosophy(self, response: str) -> Dict:
        """Extract strategic philosophy from response"""
        sections = response.split('\n\n')
        philosophy = {}
        for i, section in enumerate(sections):
            if 'strategic' in section.lower() or 'strategy' in section.lower():
                philosophy[f"strategic_principle_{i}"] = section.strip()
        return philosophy
    
    def _extract_relationship_philosophy(self, response: str) -> Dict:
        """Extract relationship philosophy from response"""
        sections = response.split('\n\n')
        philosophy = {}
        for i, section in enumerate(sections):
            if 'relationship' in section.lower():
                philosophy[f"relationship_principle_{i}"] = section.strip()
        return philosophy
    
    def _extract_life_design(self, response: str) -> Dict:
        """Extract life design from response"""
        sections = response.split('\n\n')
        design = {}
        for i, section in enumerate(sections):
            if 'life' in section.lower() or 'design' in section.lower():
                design[f"life_principle_{i}"] = section.strip()
        return design
    
    def _extract_thematic_structures(self, response: str) -> Dict:
        """Extract thematic structures from response"""
        sections = response.split('\n\n')
        themes = {}
        for i, section in enumerate(sections):
            if 'theme' in section.lower() or 'pattern' in section.lower():
                themes[f"theme_{i}"] = {
                    "summary": section.strip()[:200] + "..." if len(section.strip()) > 200 else section.strip(),
                    "full_content": section.strip(),
                    "importance": 0.7,  # Default importance
                    "connections": [],
                    "components": {}
                }
        return themes


class StrategicInsightsAnalyst(BaseAnalyst):
    """Generates strategic insights and recommendations"""
    
    def __init__(self, user_id: int):
        super().__init__(user_id)
        self.analyst_name = "strategic_insights"
        self.analyst_description = "Strategic insights and recommendations analyst"
    
    def _prepare_email_context(self, emails: List[Dict]) -> str:
        """Prepare email context for Claude analysis"""
        context_parts = []
        for email in emails[:50]:  # Limit to prevent token overflow
            context_parts.append(f"""
Email ID: {email.get('id')}
Date: {email.get('email_date')}
From: {email.get('sender')} 
To: {email.get('recipients')}
Subject: {email.get('subject')}
Body: {email.get('body_text', '')[:1000]}...
---
""")
        return "\n".join(context_parts)
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """Generate strategic insights intelligence"""
        emails = data.get('emails', [])
        worldview = data.get('worldview', {})
        
        return await self.generate_insights(emails, worldview)
    
    async def generate_insights(self, emails: List[Dict], worldview: Dict) -> Dict:
        """Generate strategic insights and recommendations"""
        
        system_prompt = """You are a strategic advisor who generates actionable insights from deep understanding.

Based on the comprehensive worldview analysis, provide:
1. KEY INSIGHTS: Most important revelations about how they operate
2. STRATEGIC OPPORTUNITIES: Specific opportunities aligned with their approach
3. POTENTIAL RISKS: Areas of vulnerability or potential challenges
4. RECOMMENDATIONS: Specific, actionable steps to leverage insights
5. OPTIMIZATION STRATEGIES: How to enhance their effectiveness

Focus on insights that are:
- Highly specific and actionable
- Aligned with their unique approach and values
- Strategically significant
- Non-obvious but important

Provide recommendations that feel like they come from someone who truly understands them."""

        user_prompt = f"""
RECENT COMMUNICATIONS:
{self._prepare_email_context(emails[:20])}

COMPREHENSIVE WORLDVIEW:
{json.dumps(worldview, indent=2)}

Generate strategic insights and specific recommendations based on this deep understanding of their worldview and approach.
"""

        response = await self._run_claude_analysis(system_prompt, user_prompt)
        return self._parse_insights_response(response)
    
    def _parse_insights_response(self, response: str) -> Dict:
        """Parse strategic insights from response"""
        try:
            # Try to extract JSON if present
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured response
        return {
            "key_insights": self._extract_key_insights(response),
            "strategic_opportunities": self._extract_opportunities(response),
            "potential_risks": self._extract_risks(response),
            "recommendations": self._extract_recommendations(response),
            "optimization_strategies": self._extract_optimization(response),
            "raw_analysis": response
        }
    
    def _extract_key_insights(self, response: str) -> List[Dict]:
        """Extract key insights from response"""
        insights = []
        sections = response.split('\n\n')
        for section in sections:
            if 'insight' in section.lower() or 'revelation' in section.lower():
                insights.append({
                    "content": section.strip(),
                    "type": "key_insight",
                    "confidence": 0.8
                })
        return insights
    
    def _extract_opportunities(self, response: str) -> List[Dict]:
        """Extract opportunities from response"""
        opportunities = []
        sections = response.split('\n\n')
        for section in sections:
            if 'opportunity' in section.lower():
                opportunities.append({
                    "content": section.strip(),
                    "type": "strategic_opportunity"
                })
        return opportunities
    
    def _extract_risks(self, response: str) -> List[Dict]:
        """Extract risks from response"""
        risks = []
        sections = response.split('\n\n')
        for section in sections:
            if 'risk' in section.lower() or 'vulnerability' in section.lower():
                risks.append({
                    "content": section.strip(),
                    "type": "potential_risk"
                })
        return risks
    
    def _extract_recommendations(self, response: str) -> List[Dict]:
        """Extract recommendations from response"""
        recommendations = []
        sections = response.split('\n\n')
        for section in sections:
            if 'recommend' in section.lower() or 'action' in section.lower():
                recommendations.append({
                    "content": section.strip(),
                    "type": "recommendation",
                    "priority": "medium"
                })
        return recommendations
    
    def _extract_optimization(self, response: str) -> List[Dict]:
        """Extract optimization strategies from response"""
        strategies = []
        sections = response.split('\n\n')
        for section in sections:
            if 'optimization' in section.lower() or 'enhance' in section.lower():
                strategies.append({
                    "content": section.strip(),
                    "type": "optimization_strategy"
                })
        return strategies 

================================================================================
FILE: intelligence/knowledge_tree/tree_builder.py
SIZE: 13,690 bytes
LINES: 403
================================================================================

"""
Knowledge Tree Builder
=====================
Constructs and manages knowledge trees from multiple data sources.
"""

from typing import Dict, List, Optional, Any, Set, Tuple
import uuid
from datetime import datetime

from models.knowledge_tree import KnowledgeNode, RelationType
from models.email import Email
from models.contact import Contact
from utils.logging import get_logger
from utils.helpers import extract_entities_from_text

logger = get_logger(__name__)


class KnowledgeTreeBuilder:
    """
    Responsible for building and managing knowledge trees from various data sources.
    """
    
    def __init__(self, user_id: str):
        """
        Initialize the tree builder.
        
        Args:
            user_id: ID of the user owning this knowledge tree
        """
        self.user_id = user_id
        
    def create_root_node(self, title: str, description: str = None) -> KnowledgeNode:
        """
        Create a root node for a new knowledge tree.
        
        Args:
            title: Title of the knowledge tree
            description: Optional description
            
        Returns:
            Root knowledge node
        """
        content = description or f"Root node for knowledge tree: {title}"
        
        root_node = KnowledgeNode(
            user_id=self.user_id,
            content=content,
            node_type="ROOT",
            tags=["root", title.lower().replace(" ", "-")],
            metadata={"title": title}
        )
        
        # Save the node
        root_node.save()
        
        logger.info(f"Created root node {root_node.node_id} for knowledge tree: {title}")
        return root_node
        
    def extract_from_email(self, email_id: str, parent_node_id: Optional[str] = None) -> List[KnowledgeNode]:
        """
        Extract knowledge from an email and add to the tree.
        
        Args:
            email_id: ID of the email to process
            parent_node_id: Optional parent node to attach to
            
        Returns:
            List of created knowledge nodes
        """
        # Load email
        email = Email.load(self.user_id, email_id)
        if not email:
            logger.error(f"Email not found: {email_id}")
            return []
            
        # Create facts from email content
        nodes = []
        
        # Create a node for the email itself
        email_node = KnowledgeNode(
            user_id=self.user_id,
            content=f"Email from {email.sender} about: {email.subject}",
            node_type="FACT",
            tags=["email", "communication"],
            metadata={
                "email_id": email_id,
                "sender": email.sender,
                "subject": email.subject,
                "date": email.date.isoformat() if email.date else None
            }
        )
        
        # Add email as source
        source_ref = {
            "source_id": email_id,
            "source_type": "email",
            "timestamp": datetime.utcnow().isoformat()
        }
        email_node.add_source(source_ref)
        
        # Save the node
        email_node.save()
        nodes.append(email_node)
        
        # Connect to parent if provided
        if parent_node_id:
            parent_node = KnowledgeNode.load(self.user_id, parent_node_id)
            if parent_node:
                parent_node.add_relation(
                    target_id=email_node.node_id, 
                    relation_type="CONTAINS",
                    metadata={"created_at": datetime.utcnow().isoformat()}
                )
                parent_node.save()
        
        # Extract key points from email body
        key_points = self._extract_key_points(email.body)
        
        # Create nodes for key points
        for point in key_points:
            point_node = KnowledgeNode(
                user_id=self.user_id,
                content=point["content"],
                node_type="FACT",
                confidence=point.get("confidence", 0.9),
                tags=point.get("tags", []),
                metadata={"extracted_from": email_id}
            )
            
            # Add email as source
            point_node.add_source(source_ref)
            
            # Save the node
            point_node.save()
            nodes.append(point_node)
            
            # Link to email node
            email_node.add_relation(
                target_id=point_node.node_id,
                relation_type="CONTAINS",
                metadata={"created_at": datetime.utcnow().isoformat()}
            )
            
        # Save email node with all its relations
        email_node.save()
        
        logger.info(f"Extracted {len(nodes)} knowledge nodes from email {email_id}")
        return nodes
    
    def extract_from_contact(self, contact_id: str, parent_node_id: Optional[str] = None) -> List[KnowledgeNode]:
        """
        Extract knowledge from a contact and add to the tree.
        
        Args:
            contact_id: ID of the contact to process
            parent_node_id: Optional parent node to attach to
            
        Returns:
            List of created knowledge nodes
        """
        # Load contact
        contact = Contact.load(self.user_id, contact_id)
        if not contact:
            logger.error(f"Contact not found: {contact_id}")
            return []
            
        nodes = []
        
        # Create a node for the contact
        contact_node = KnowledgeNode(
            user_id=self.user_id,
            content=f"Contact: {contact.name}",
            node_type="FACT",
            tags=["contact", "person"],
            metadata={
                "contact_id": contact_id,
                "name": contact.name,
                "email": contact.email,
                "organization": contact.organization,
                "title": contact.title
            }
        )
        
        # Add contact as source
        source_ref = {
            "source_id": contact_id,
            "source_type": "contact",
            "timestamp": datetime.utcnow().isoformat()
        }
        contact_node.add_source(source_ref)
        
        # Save the node
        contact_node.save()
        nodes.append(contact_node)
        
        # Connect to parent if provided
        if parent_node_id:
            parent_node = KnowledgeNode.load(self.user_id, parent_node_id)
            if parent_node:
                parent_node.add_relation(
                    target_id=contact_node.node_id, 
                    relation_type="RELATES_TO",
                    metadata={"created_at": datetime.utcnow().isoformat()}
                )
                parent_node.save()
        
        # Extract key facts about contact
        if contact.notes:
            key_facts = self._extract_key_points(contact.notes)
            
            # Create nodes for key facts
            for fact in key_facts:
                fact_node = KnowledgeNode(
                    user_id=self.user_id,
                    content=fact["content"],
                    node_type="FACT",
                    confidence=fact.get("confidence", 0.8),
                    tags=["contact-info"] + fact.get("tags", []),
                    metadata={"extracted_from": contact_id}
                )
                
                # Add contact as source
                fact_node.add_source(source_ref)
                
                # Save the node
                fact_node.save()
                nodes.append(fact_node)
                
                # Link to contact node
                contact_node.add_relation(
                    target_id=fact_node.node_id,
                    relation_type="RELATES_TO",
                    metadata={"created_at": datetime.utcnow().isoformat()}
                )
        
        # Save contact node with all relations
        contact_node.save()
        
        logger.info(f"Extracted {len(nodes)} knowledge nodes from contact {contact_id}")
        return nodes
        
    def merge_nodes(self, source_node_ids: List[str], new_content: str) -> KnowledgeNode:
        """
        Merge multiple nodes into a new consolidated node.
        
        Args:
            source_node_ids: IDs of nodes to merge
            new_content: Content for the merged node
            
        Returns:
            New consolidated node
        """
        # Load all source nodes
        source_nodes = []
        for node_id in source_node_ids:
            node = KnowledgeNode.load(self.user_id, node_id)
            if node:
                source_nodes.append(node)
                
        if not source_nodes:
            logger.error("No valid source nodes found for merging")
            return None
            
        # Collect tags from all source nodes
        all_tags = set()
        for node in source_nodes:
            all_tags.update(node.tags)
            
        # Create merged node
        merged_node = KnowledgeNode(
            user_id=self.user_id,
            content=new_content,
            node_type="INFERRED",
            tags=list(all_tags),
            metadata={"merged_from": source_node_ids}
        )
        
        # Add all original sources
        for node in source_nodes:
            for source in node.sources:
                merged_node.add_source(source)
        
        # Save the merged node
        merged_node.save()
        
        # Add relations from source nodes to merged node
        for node in source_nodes:
            node.add_relation(
                target_id=merged_node.node_id,
                relation_type="MERGED_INTO",
                metadata={"created_at": datetime.utcnow().isoformat()}
            )
            node.save()
            
        logger.info(f"Merged {len(source_nodes)} nodes into new node {merged_node.node_id}")
        return merged_node
        
    def get_tree_structure(self, root_node_id: str, max_depth: int = 5) -> Dict[str, Any]:
        """
        Get the structure of a knowledge tree starting from a root node.
        
        Args:
            root_node_id: ID of the root node
            max_depth: Maximum depth to traverse
            
        Returns:
            Dictionary representing the tree structure
        """
        # Load root node
        root_node = KnowledgeNode.load(self.user_id, root_node_id)
        if not root_node:
            logger.error(f"Root node not found: {root_node_id}")
            return {}
            
        # Keep track of visited nodes to avoid cycles
        visited = set()
        
        def build_subtree(node_id: str, depth: int) -> Dict[str, Any]:
            if depth > max_depth or node_id in visited:
                return None
                
            visited.add(node_id)
            node = KnowledgeNode.load(self.user_id, node_id)
            if not node:
                return None
                
            # Get outgoing relations
            relations = node.get_relations()
            
            # Build children recursively
            children = []
            for relation in relations:
                child_tree = build_subtree(relation['target_id'], depth + 1)
                if child_tree:
                    children.append({
                        "relation": relation['relation_type'],
                        "child": child_tree
                    })
            
            # Create node representation
            node_data = {
                "id": node.node_id,
                "content": node.content,
                "type": node.node_type,
                "tags": node.tags,
                "confidence": node.confidence
            }
            
            if children:
                node_data["children"] = children
                
            return node_data
            
        # Start building from root
        tree = build_subtree(root_node_id, 1)
        
        return tree
        
    def _extract_key_points(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract key points from text.
        
        Args:
            text: Text to analyze
            
        Returns:
            List of key points with content, confidence, and tags
        """
        # In a real implementation, this would use NLP to extract key points
        # For now, we'll use a simple approach based on paragraphs
        
        # Split into paragraphs and filter empty ones
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        key_points = []
        
        for i, paragraph in enumerate(paragraphs[:3]):  # Limit to first 3 paragraphs
            # Simple heuristic: first sentence as key point
            sentences = paragraph.split('.')
            if sentences:
                first_sentence = sentences[0].strip()
                if len(first_sentence) > 10:  # Minimum length check
                    # Extract entities from the paragraph
                    entities = extract_entities_from_text(paragraph)
                    
                    # Create tags from entities
                    tags = []
                    if entities.get("emails"):
                        tags.append("has-email")
                    if entities.get("urls"):
                        tags.append("has-url")
                    if entities.get("dates"):
                        tags.append("has-date")
                    
                    key_points.append({
                        "content": first_sentence,
                        "confidence": 0.8,
                        "tags": tags
                    })
        
        return key_points

================================================================================
FILE: intelligence/analysts/__init__.py
SIZE: 83 bytes
LINES: 2
================================================================================

# intelligence/analysts/__init__.py
# Intelligence analysts package initialization

================================================================================
FILE: intelligence/analysts/base_analyst.py
SIZE: 9,028 bytes
LINES: 278
================================================================================

# intelligence/analysts/base_analyst.py
"""
Base Intelligence Analyst
=====================
Abstract base class for all intelligence analysts powered by Claude Opus.
Ensures consistent interface and multi-tenant isolation.
"""

import json
import asyncio
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Union
from datetime import datetime

import anthropic
from utils.logging import structured_logger as logger
from storage.storage_manager import get_storage_manager
from config.settings import ANTHROPIC_API_KEY

class BaseAnalyst(ABC):
    """
    Base class for all intelligence analysts
    
    Provides common functionality for prompt construction,
    LLM interaction, and multi-tenant isolation.
    """
    
    def __init__(self, user_id: int, anthropic_api_key: str = None):
        """
        Initialize base analyst
        
        Args:
            user_id: User ID for multi-tenant isolation
            anthropic_api_key: Optional API key for Claude Opus
        """
        self.user_id = user_id
        self.api_key = anthropic_api_key or ANTHROPIC_API_KEY
        self.model = "claude-3-opus-20240229"
        self.client = None
        self.storage_manager = None
        
        # Required to be set by subclasses
        self.analyst_name = "base_analyst"
        self.analyst_description = "Generic intelligence analyst"
        self.max_tokens = 4000
        self.temperature = 0.3
    
    async def initialize(self) -> bool:
        """
        Initialize resources and connections
        
        Returns:
            True if initialization successful
        """
        try:
            # Initialize Claude Opus client
            self.client = anthropic.Anthropic(api_key=self.api_key)
            
            # Get storage manager for result persistence
            self.storage_manager = await get_storage_manager()
            
            return True
            
        except Exception as e:
            logger.error(
                f"Failed to initialize {self.analyst_name}",
                error=str(e),
                user_id=self.user_id
            )
            return False
    
    @abstractmethod
    async def generate_intelligence(self, data: Dict) -> Dict:
        """
        Generate intelligence analysis from provided data
        Must be implemented by subclasses
        
        Args:
            data: Dictionary with relevant data for analysis
            
        Returns:
            Dictionary with intelligence analysis results
        """
        pass
    
    async def analyze(self, input_data: Dict) -> Dict:
        """
        Public method to run analysis with standardized formatting
        
        Args:
            input_data: Dictionary of input data
            
        Returns:
            Dictionary with analysis results
        """
        try:
            # Initialize if needed
            if not self.client or not self.storage_manager:
                success = await self.initialize()
                if not success:
                    return self._create_error_response("Failed to initialize analyst")
            
            # Validate input data
            validation_error = self._validate_input(input_data)
            if validation_error:
                return self._create_error_response(validation_error)
            
            # Generate analysis
            start_time = datetime.utcnow()
            analysis_result = await self.generate_intelligence(input_data)
            end_time = datetime.utcnow()
            
            if not analysis_result:
                return self._create_error_response("No analysis results generated")
            
            # Format the result
            result = {
                "analyst_name": self.analyst_name,
                "analyst_description": self.analyst_description,
                "user_id": self.user_id,
                "timestamp": datetime.utcnow().isoformat(),
                "execution_time_ms": int((end_time - start_time).total_seconds() * 1000),
                "results": analysis_result,
                "status": "success",
                "input_data_summary": self._summarize_input(input_data)
            }
            
            # Store the result
            await self._store_analysis_result(result)
            
            return result
            
        except Exception as e:
            logger.error(
                f"Error in {self.analyst_name}",
                error=str(e),
                user_id=self.user_id
            )
            return self._create_error_response(str(e))
    
    async def _run_claude_analysis(self, system_prompt: str, user_prompt: str) -> str:
        """
        Run Claude Opus analysis with system and user prompts
        
        Args:
            system_prompt: System prompt defining the analysis context
            user_prompt: User prompt with specific instructions and data
            
        Returns:
            Claude's response text
        """
        try:
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            if not response or not response.content:
                logger.error(
                    f"Empty response from Claude Opus",
                    user_id=self.user_id,
                    analyst=self.analyst_name
                )
                return ""
                
            # Extract text content
            content = response.content[0].text
            return content
            
        except Exception as e:
            logger.error(
                f"Error running Claude Opus analysis",
                error=str(e),
                user_id=self.user_id,
                analyst=self.analyst_name
            )
            return ""
    
    async def _store_analysis_result(self, result: Dict) -> None:
        """
        Store analysis result in database
        
        Args:
            result: Analysis result dictionary
        """
        try:
            await self.storage_manager.store_intelligence_result(
                self.user_id,
                self.analyst_name,
                result
            )
        except Exception as e:
            logger.error(
                f"Failed to store analysis result",
                error=str(e),
                user_id=self.user_id,
                analyst=self.analyst_name
            )
    
    def _create_error_response(self, error_message: str) -> Dict:
        """
        Create standardized error response
        
        Args:
            error_message: Error description
            
        Returns:
            Error response dictionary
        """
        return {
            "analyst_name": self.analyst_name,
            "analyst_description": self.analyst_description,
            "user_id": self.user_id,
            "timestamp": datetime.utcnow().isoformat(),
            "status": "error",
            "error": error_message
        }
    
    def _validate_input(self, input_data: Dict) -> Optional[str]:
        """
        Validate input data for analysis
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Error message if validation fails, None if valid
        """
        # Basic validation - subclasses should override with specific validation
        if not input_data:
            return "No input data provided"
        return None
    
    def _summarize_input(self, input_data: Dict) -> Dict:
        """
        Create summary of input data for recording
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dictionary with input data summary
        """
        # Default implementation - subclasses should provide better summaries
        summary = {}
        
        for key, value in input_data.items():
            if isinstance(value, dict):
                summary[key] = "Dict with {} keys".format(len(value))
            elif isinstance(value, list):
                summary[key] = "List with {} items".format(len(value))
            elif isinstance(value, str) and len(value) > 100:
                summary[key] = "String of length {}".format(len(value))
            else:
                summary[key] = str(value)
                
        return summary
    
    def _format_json_for_prompt(self, data: Any) -> str:
        """
        Format data as pretty JSON for inclusion in prompt
        
        Args:
            data: Data to format
            
        Returns:
            Formatted JSON string
        """
        try:
            return json.dumps(data, indent=2)
        except Exception:
            return str(data)

================================================================================
FILE: intelligence/analysts/business_analyst.py
SIZE: 13,290 bytes
LINES: 351
================================================================================

# intelligence/analysts/business_analyst.py
"""
Business Intelligence Analyst
==========================
Specialized analyst for business insights and strategic recommendations.
Powered by Claude Opus with multi-tenant isolation.
"""

import json
from typing import Dict, List, Optional, Any
from datetime import datetime

from utils.logging import structured_logger as logger
from intelligence.analysts.base_analyst import BaseAnalyst

class BusinessIntelligenceAnalyst(BaseAnalyst):
    """
    Business intelligence analyst
    
    Analyzes contact and email data to extract business insights,
    relationships, and strategic opportunities.
    """
    
    def __init__(self, user_id: int, anthropic_api_key: str = None):
        """
        Initialize business analyst
        
        Args:
            user_id: User ID for multi-tenant isolation
            anthropic_api_key: Optional API key for Claude Opus
        """
        super().__init__(user_id, anthropic_api_key)
        self.analyst_name = "business_intelligence"
        self.analyst_description = "Business strategy and relationship analyst"
        
        # Customize Claude parameters
        self.max_tokens = 4000
        self.temperature = 0.2  # Lower temperature for more factual/strategic responses
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """
        Generate business intelligence from provided data
        
        Args:
            data: Dictionary containing:
                - contacts: List of enriched contact data
                - emails: List of relevant email exchanges
                - (optional) knowledge_tree: Knowledge tree data
                - (optional) specific_focus: Area of business to focus on
                
        Returns:
            Dictionary with business insights and recommendations
        """
        # Validate critical inputs
        if not data.get("contacts") and not data.get("emails"):
            logger.error(
                "Missing required data for business analysis",
                user_id=self.user_id
            )
            return {"error": "Insufficient data for business analysis"}
            
        try:
            # Extract data elements
            contacts = data.get("contacts", [])
            emails = data.get("emails", [])
            knowledge_tree = data.get("knowledge_tree", {})
            focus_area = data.get("specific_focus", "general business strategy")
            
            # Prepare specialized system prompt
            system_prompt = self._create_system_prompt(focus_area)
            
            # Prepare user prompt with data
            user_prompt = self._create_user_prompt(
                contacts=contacts,
                emails=emails,
                knowledge_tree=knowledge_tree,
                focus_area=focus_area
            )
            
            # Run Claude Opus analysis
            result_text = await self._run_claude_analysis(system_prompt, user_prompt)
            
            # Extract structured components
            structured_result = self._parse_claude_response(result_text)
            
            # Add metadata
            structured_result["focus_area"] = focus_area
            structured_result["analysis_timestamp"] = datetime.utcnow().isoformat()
            structured_result["data_points_analyzed"] = {
                "contacts": len(contacts),
                "emails": len(emails),
                "knowledge_nodes": len(knowledge_tree.get("nodes", [])) if isinstance(knowledge_tree.get("nodes"), list) else 0
            }
            
            return structured_result
            
        except Exception as e:
            logger.error(
                "Error generating business intelligence",
                error=str(e),
                user_id=self.user_id
            )
            return {"error": f"Business intelligence generation failed: {str(e)}"}
    
    def _create_system_prompt(self, focus_area: str) -> str:
        """
        Create system prompt for Claude Opus
        
        Args:
            focus_area: Area to focus analysis on
            
        Returns:
            System prompt string
        """
        return f"""You are an expert business intelligence analyst specializing in {focus_area}.

Your task is to analyze communication data, contact information, and knowledge graphs to extract valuable business insights and strategic recommendations.

Focus on:
1. Identifying key business relationships and their strategic importance
2. Detecting business opportunities and potential collaborations
3. Recognizing competitive threats and market positioning
4. Understanding organizational dynamics and decision-making patterns
5. Uncovering professional networks and spheres of influence

Structure your analysis with these sections:
- Executive Summary: Brief, high-impact overview of key findings
- Key Relationships: Analysis of most valuable business connections
- Strategic Opportunities: Business possibilities identified from the data
- Competitive Landscape: Market positioning and competitive dynamics
- Recommendations: Actionable strategic steps based on your analysis

Be concise, insightful, and focused on practical business value. 
Base all insights on the provided data and make reasonable inferences where appropriate.
Format your response as a combination of concise paragraphs and bullet points for readability.

Maintain absolute confidentiality and privacy in your analysis. 
Do not include any information that could be considered sensitive or proprietary.
"""
    
    def _create_user_prompt(
        self, 
        contacts: List[Dict], 
        emails: List[Dict],
        knowledge_tree: Dict,
        focus_area: str
    ) -> str:
        """
        Create user prompt with data for analysis
        
        Args:
            contacts: List of enriched contact data
            emails: List of relevant email exchanges
            knowledge_tree: Knowledge graph data
            focus_area: Area to focus analysis on
            
        Returns:
            Formatted user prompt
        """
        # Limit data volume to fit context window
        max_contacts = min(50, len(contacts))
        max_emails = min(30, len(emails))
        
        # Select most important contacts and emails
        selected_contacts = contacts[:max_contacts]
        selected_emails = emails[:max_emails]
        
        # Format prompt
        prompt = f"""Please analyze the following business data and provide strategic intelligence insights focused on {focus_area}.

## CONTACTS DATA
I'm providing information about {len(selected_contacts)} business contacts with their enriched profiles:

```json
{self._format_json_for_prompt(selected_contacts)}
```

## EMAIL COMMUNICATIONS
Here are {len(selected_emails)} relevant email exchanges that provide context on business relationships:

```json
{self._format_json_for_prompt(selected_emails)}
```
"""

        # Add knowledge tree if available
        if knowledge_tree and knowledge_tree.get("nodes"):
            prompt += f"""
## KNOWLEDGE RELATIONSHIPS
Here is a knowledge graph showing relationships between entities:

```json
{self._format_json_for_prompt(knowledge_tree)}
```
"""

        # Add specific instructions
        prompt += f"""
Based on this data, please provide:

1. An insightful business intelligence analysis
2. Strategic opportunities identified from these communications
3. Key relationship insights and their business implications
4. Actionable recommendations for business advantage

Focus specifically on {focus_area} aspects and organize your response according to the structure in your instructions.
"""

        return prompt
    
    def _parse_claude_response(self, response_text: str) -> Dict:
        """
        Parse Claude's response into structured components
        
        Args:
            response_text: Raw response from Claude
            
        Returns:
            Structured response dictionary
        """
        # Default structure
        result = {
            "executive_summary": "",
            "key_relationships": [],
            "strategic_opportunities": [],
            "competitive_landscape": "",
            "recommendations": []
        }
        
        if not response_text:
            return result
            
        # Extract executive summary
        if "Executive Summary" in response_text:
            parts = response_text.split("Executive Summary", 1)
            if len(parts) > 1:
                summary_section = parts[1].split("\n#", 1)[0].split("Key Relationships", 1)[0].strip()
                result["executive_summary"] = summary_section
        
        # Extract key relationships
        if "Key Relationships" in response_text:
            parts = response_text.split("Key Relationships", 1)
            if len(parts) > 1:
                relations_text = parts[1].split("\n#", 1)[0].split("Strategic Opportunities", 1)[0]
                # Extract bullet points
                relations = []
                for line in relations_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        relations.append(line.strip()[2:])
                if relations:
                    result["key_relationships"] = relations
                else:
                    result["key_relationships"] = [relations_text.strip()]
        
        # Extract strategic opportunities
        if "Strategic Opportunities" in response_text:
            parts = response_text.split("Strategic Opportunities", 1)
            if len(parts) > 1:
                opps_text = parts[1].split("\n#", 1)[0].split("Competitive Landscape", 1)[0]
                # Extract bullet points
                opps = []
                for line in opps_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        opps.append(line.strip()[2:])
                if opps:
                    result["strategic_opportunities"] = opps
                else:
                    result["strategic_opportunities"] = [opps_text.strip()]
        
        # Extract competitive landscape
        if "Competitive Landscape" in response_text:
            parts = response_text.split("Competitive Landscape", 1)
            if len(parts) > 1:
                landscape = parts[1].split("\n#", 1)[0].split("Recommendations", 1)[0].strip()
                result["competitive_landscape"] = landscape
        
        # Extract recommendations
        if "Recommendations" in response_text:
            parts = response_text.split("Recommendations", 1)
            if len(parts) > 1:
                recs_text = parts[1].strip()
                # Extract bullet points
                recs = []
                for line in recs_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        recs.append(line.strip()[2:])
                if recs:
                    result["recommendations"] = recs
                else:
                    result["recommendations"] = [recs_text.strip()]
        
        # Add full text as fallback
        result["full_analysis"] = response_text
        
        return result
    
    def _validate_input(self, input_data: Dict) -> Optional[str]:
        """
        Validate input data for business analysis
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Error message if validation fails, None if valid
        """
        if not input_data:
            return "No input data provided"
            
        if not input_data.get("contacts") and not input_data.get("emails"):
            return "Missing required data: need either contacts or emails"
            
        return None
    
    def _summarize_input(self, input_data: Dict) -> Dict:
        """
        Create summary of input data
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Input data summary
        """
        summary = {}
        
        if "contacts" in input_data:
            contacts = input_data["contacts"]
            summary["contacts_count"] = len(contacts)
            summary["contact_companies"] = list(set(
                c.get("company", "") for c in contacts 
                if c.get("company")
            ))[:5]  # List up to 5 unique companies
            
        if "emails" in input_data:
            emails = input_data["emails"]
            summary["emails_count"] = len(emails)
            summary["email_date_range"] = {
                "oldest": min((e.get("date", "") for e in emails), default="unknown"),
                "newest": max((e.get("date", "") for e in emails), default="unknown")
            }
            
        if "specific_focus" in input_data:
            summary["focus_area"] = input_data["specific_focus"]
            
        if "knowledge_tree" in input_data:
            tree = input_data["knowledge_tree"]
            summary["knowledge_nodes"] = len(tree.get("nodes", []))
            summary["knowledge_edges"] = len(tree.get("edges", []))
            
        return summary

================================================================================
FILE: intelligence/analysts/market_analyst.py
SIZE: 17,545 bytes
LINES: 441
================================================================================

# intelligence/analysts/market_analyst.py
"""
Market Intelligence Analyst
========================
Specialized analyst for market trends and competitive landscape analysis.
Powered by Claude Opus with multi-tenant isolation.
"""

import json
from typing import Dict, List, Optional, Any
from datetime import datetime

from utils.logging import structured_logger as logger
from intelligence.analysts.base_analyst import BaseAnalyst

class MarketIntelligenceAnalyst(BaseAnalyst):
    """
    Market intelligence analyst
    
    Analyzes communications and enriched contact data to extract market insights,
    competitive landscape, industry trends, and market positioning recommendations.
    """
    
    def __init__(self, user_id: int, anthropic_api_key: str = None):
        """
        Initialize market analyst
        
        Args:
            user_id: User ID for multi-tenant isolation
            anthropic_api_key: Optional API key for Claude Opus
        """
        super().__init__(user_id, anthropic_api_key)
        self.analyst_name = "market_intelligence"
        self.analyst_description = "Market trends and competitive landscape analyst"
        
        # Customize Claude parameters
        self.max_tokens = 4000
        self.temperature = 0.2  # Balanced for market analysis
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """
        Generate market intelligence from provided data
        
        Args:
            data: Dictionary containing:
                - contacts: List of enriched contact data with company info
                - emails: List of relevant email exchanges
                - (optional) market_data: Industry or market specific data
                - (optional) knowledge_tree: Knowledge tree data
                - (optional) specific_focus: Market area to focus on
                - (optional) competitors: List of specific competitors to analyze
                
        Returns:
            Dictionary with market insights and recommendations
        """
        # Validate critical inputs
        if not data.get("contacts") and not data.get("emails"):
            logger.error(
                "Missing required data for market analysis",
                user_id=self.user_id
            )
            return {"error": "Insufficient data for market analysis"}
            
        try:
            # Extract data elements
            contacts = data.get("contacts", [])
            emails = data.get("emails", [])
            market_data = data.get("market_data", [])
            knowledge_tree = data.get("knowledge_tree", {})
            focus_area = data.get("specific_focus", "industry trends")
            competitors = data.get("competitors", [])
            
            # Prepare specialized system prompt
            system_prompt = self._create_system_prompt(focus_area, competitors)
            
            # Prepare user prompt with data
            user_prompt = self._create_user_prompt(
                contacts=contacts,
                emails=emails,
                market_data=market_data,
                knowledge_tree=knowledge_tree,
                focus_area=focus_area,
                competitors=competitors
            )
            
            # Run Claude Opus analysis
            result_text = await self._run_claude_analysis(system_prompt, user_prompt)
            
            # Extract structured components
            structured_result = self._parse_claude_response(result_text)
            
            # Add metadata
            structured_result["focus_area"] = focus_area
            structured_result["analysis_timestamp"] = datetime.utcnow().isoformat()
            structured_result["data_points_analyzed"] = {
                "contacts": len(contacts),
                "emails": len(emails),
                "market_data_points": len(market_data),
                "competitors": competitors
            }
            
            return structured_result
            
        except Exception as e:
            logger.error(
                "Error generating market intelligence",
                error=str(e),
                user_id=self.user_id
            )
            return {"error": f"Market intelligence generation failed: {str(e)}"}
    
    def _create_system_prompt(self, focus_area: str, competitors: List[str]) -> str:
        """
        Create system prompt for Claude Opus
        
        Args:
            focus_area: Market area to focus on
            competitors: List of specific competitors to analyze
            
        Returns:
            System prompt string
        """
        competitor_context = ""
        if competitors:
            competitor_list = ", ".join(competitors)
            competitor_context = f" with particular attention to these competitors: {competitor_list}"
            
        return f"""You are an expert market intelligence analyst specializing in {focus_area}{competitor_context}.

Your task is to analyze contact data, communications, and market information to extract valuable market insights, competitive intelligence, and strategic positioning recommendations.

Focus on:
1. Identifying key market trends and industry dynamics
2. Analyzing competitive landscape and positioning
3. Recognizing market opportunities and threats
4. Understanding customer segments and needs
5. Evaluating market entry or expansion strategies

Structure your analysis with these sections:
- Market Overview: Brief summary of key market insights
- Industry Trends: Analysis of significant market movements and directions
- Competitive Landscape: Assessment of key players, their strengths and weaknesses
- Market Opportunities: Identified areas for growth or advantage
- Strategic Recommendations: Actionable market strategy recommendations

Be analytical, evidence-based, and focused on actionable market intelligence.
Base all insights on the provided data and make reasonable market inferences where appropriate.
Format your response as a combination of concise market analysis paragraphs and bullet points for readability.

Maintain absolute confidentiality and privacy in your analysis.
Do not include any information that could be considered sensitive or proprietary.
"""
    
    def _create_user_prompt(
        self, 
        contacts: List[Dict],
        emails: List[Dict],
        market_data: List[Dict],
        knowledge_tree: Dict,
        focus_area: str,
        competitors: List[str]
    ) -> str:
        """
        Create user prompt with data for analysis
        
        Args:
            contacts: List of enriched contact data
            emails: List of relevant email exchanges
            market_data: Industry or market specific data
            knowledge_tree: Knowledge graph data
            focus_area: Area to focus analysis on
            competitors: List of specific competitors to analyze
            
        Returns:
            Formatted user prompt
        """
        # Limit data volume to fit context window
        max_contacts = min(50, len(contacts))
        max_emails = min(30, len(emails))
        max_market = min(20, len(market_data))
        
        # Select most important data
        selected_contacts = contacts[:max_contacts]
        selected_emails = emails[:max_emails]
        selected_market = market_data[:max_market]
        
        # Format competitor focus
        competitor_focus = ""
        if competitors:
            competitor_list = ", ".join(competitors)
            competitor_focus = f"\nPay special attention to these competitors: {competitor_list}"
        
        # Format prompt
        prompt = f"""Please analyze the following data and provide market intelligence insights focused on {focus_area}.{competitor_focus}

## CONTACTS DATA
Here are {len(selected_contacts)} contacts with company information for market analysis:

```json
{self._format_json_for_prompt(selected_contacts)}
```

## EMAIL COMMUNICATIONS
Here are {len(selected_emails)} relevant email exchanges that provide context on market dynamics:

```json
{self._format_json_for_prompt(selected_emails)}
```
"""

        # Add market data if available
        if selected_market:
            prompt += f"""
## MARKET DATA
Here is specific market information for analysis:

```json
{self._format_json_for_prompt(selected_market)}
```
"""

        # Add knowledge tree if available
        if knowledge_tree and knowledge_tree.get("nodes"):
            prompt += f"""
## KNOWLEDGE NETWORK
Here is a knowledge graph showing market relationships:

```json
{self._format_json_for_prompt(knowledge_tree)}
```
"""

        # Add specific instructions
        prompt += f"""
Based on this data, please provide:

1. A market intelligence analysis highlighting key industry trends and dynamics
2. Competitive landscape assessment{" with focus on the specified competitors" if competitors else ""}
3. Identification of market opportunities and potential threats
4. Strategic recommendations for market positioning and advantage

Focus specifically on {focus_area} and organize your response according to the structure in your instructions.
"""

        return prompt
    
    def _parse_claude_response(self, response_text: str) -> Dict:
        """
        Parse Claude's response into structured components
        
        Args:
            response_text: Raw response from Claude
            
        Returns:
            Structured response dictionary
        """
        # Default structure
        result = {
            "market_overview": "",
            "industry_trends": [],
            "competitive_landscape": {},
            "market_opportunities": [],
            "strategic_recommendations": []
        }
        
        if not response_text:
            return result
            
        # Extract market overview
        if "Market Overview" in response_text:
            parts = response_text.split("Market Overview", 1)
            if len(parts) > 1:
                overview_section = parts[1].split("\n#", 1)[0].split("Industry Trends", 1)[0].strip()
                result["market_overview"] = overview_section
        
        # Extract industry trends
        if "Industry Trends" in response_text:
            parts = response_text.split("Industry Trends", 1)
            if len(parts) > 1:
                trends_text = parts[1].split("\n#", 1)[0].split("Competitive Landscape", 1)[0]
                # Extract bullet points
                trends = []
                for line in trends_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        trends.append(line.strip()[2:])
                if trends:
                    result["industry_trends"] = trends
                else:
                    result["industry_trends"] = [trends_text.strip()]
        
        # Extract competitive landscape
        # For this one, we'll try to structure it as a dictionary of competitors
        if "Competitive Landscape" in response_text:
            parts = response_text.split("Competitive Landscape", 1)
            if len(parts) > 1:
                landscape_text = parts[1].split("\n#", 1)[0].split("Market Opportunities", 1)[0]
                
                # Try to extract competitor-specific insights
                competitive_landscape = {}
                
                # Look for competitor names in bold or as subsections
                competitors = []
                for line in landscape_text.split("\n"):
                    # Check for markdown bold or subsection headers
                    if line.strip().startswith("**") and line.strip().endswith("**"):
                        competitor = line.strip().strip("**").strip(":")
                        competitors.append(competitor)
                    elif line.strip().startswith("### "):
                        competitor = line.strip()[4:].strip(":")
                        competitors.append(competitor)
                
                # If we found structured competitors, extract their data
                if competitors:
                    current_competitor = None
                    competitor_text = ""
                    
                    for line in landscape_text.split("\n"):
                        # Check for new competitor section
                        is_new_section = False
                        for competitor in competitors:
                            if line.strip().startswith(f"**{competitor}") or line.strip().startswith(f"### {competitor}"):
                                # Save previous competitor text if any
                                if current_competitor and competitor_text:
                                    competitive_landscape[current_competitor] = competitor_text.strip()
                                
                                # Start new competitor section
                                current_competitor = competitor
                                competitor_text = ""
                                is_new_section = True
                                break
                        
                        # If not a new section and we have a current competitor, add line to text
                        if not is_new_section and current_competitor:
                            competitor_text += line + "\n"
                    
                    # Save last competitor
                    if current_competitor and competitor_text:
                        competitive_landscape[current_competitor] = competitor_text.strip()
                
                # If we couldn't extract structured competitors, use whole text
                if not competitive_landscape:
                    result["competitive_landscape"] = {"overview": landscape_text.strip()}
                else:
                    result["competitive_landscape"] = competitive_landscape
        
        # Extract market opportunities
        if "Market Opportunities" in response_text:
            parts = response_text.split("Market Opportunities", 1)
            if len(parts) > 1:
                opps_text = parts[1].split("\n#", 1)[0].split("Strategic Recommendations", 1)[0]
                # Extract bullet points
                opps = []
                for line in opps_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        opps.append(line.strip()[2:])
                if opps:
                    result["market_opportunities"] = opps
                else:
                    result["market_opportunities"] = [opps_text.strip()]
        
        # Extract strategic recommendations
        if "Strategic Recommendations" in response_text:
            parts = response_text.split("Strategic Recommendations", 1)
            if len(parts) > 1:
                recs_text = parts[1].strip()
                # Extract bullet points
                recs = []
                for line in recs_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        recs.append(line.strip()[2:])
                if recs:
                    result["strategic_recommendations"] = recs
                else:
                    result["strategic_recommendations"] = [recs_text.strip()]
        
        # Add full text as fallback
        result["full_analysis"] = response_text
        
        return result
    
    def _validate_input(self, input_data: Dict) -> Optional[str]:
        """
        Validate input data for market analysis
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Error message if validation fails, None if valid
        """
        if not input_data:
            return "No input data provided"
            
        if not input_data.get("contacts") and not input_data.get("emails"):
            return "Missing required data: need either contacts or emails"
            
        return None
    
    def _summarize_input(self, input_data: Dict) -> Dict:
        """
        Create summary of input data
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Input data summary
        """
        summary = {}
        
        if "contacts" in input_data:
            contacts = input_data["contacts"]
            summary["contacts_count"] = len(contacts)
            
            # Count contacts by company for market coverage
            companies = {}
            for contact in contacts:
                company = contact.get("company", "Unknown")
                companies[company] = companies.get(company, 0) + 1
                
            summary["company_distribution"] = dict(sorted(
                companies.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5])  # Top 5 companies
            
        if "emails" in input_data:
            emails = input_data["emails"]
            summary["emails_count"] = len(emails)
            
        if "market_data" in input_data:
            market_data = input_data["market_data"]
            summary["market_data_points"] = len(market_data)
            
        if "specific_focus" in input_data:
            summary["focus_area"] = input_data["specific_focus"]
            
        if "competitors" in input_data:
            summary["competitors"] = input_data["competitors"]
            
        return summary

================================================================================
FILE: intelligence/analysts/predictive_analyst.py
SIZE: 15,815 bytes
LINES: 413
================================================================================

# intelligence/analysts/predictive_analyst.py
"""
Predictive Intelligence Analyst
============================
Specialized analyst for forecasting trends and future developments.
Powered by Claude Opus with multi-tenant isolation.
"""

import json
from typing import Dict, List, Optional, Any
from datetime import datetime

from utils.logging import structured_logger as logger
from intelligence.analysts.base_analyst import BaseAnalyst

class PredictiveIntelligenceAnalyst(BaseAnalyst):
    """
    Predictive intelligence analyst
    
    Analyzes historical data and trends to forecast future developments,
    identify emerging patterns, and provide predictive insights.
    """
    
    def __init__(self, user_id: int, anthropic_api_key: str = None):
        """
        Initialize predictive analyst
        
        Args:
            user_id: User ID for multi-tenant isolation
            anthropic_api_key: Optional API key for Claude Opus
        """
        super().__init__(user_id, anthropic_api_key)
        self.analyst_name = "predictive_intelligence"
        self.analyst_description = "Forecasting and predictive trend analyst"
        
        # Customize Claude parameters
        self.max_tokens = 4000
        self.temperature = 0.3  # Balanced temperature for prediction
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """
        Generate predictive intelligence from provided data
        
        Args:
            data: Dictionary containing:
                - emails: List of relevant email exchanges
                - contacts: List of enriched contacts
                - (optional) historical_data: Time series or historical information
                - (optional) knowledge_tree: Knowledge tree data
                - (optional) specific_focus: Area to focus predictions on
                - (optional) time_horizon: How far into future to predict
                
        Returns:
            Dictionary with predictive insights and recommendations
        """
        # Validate critical inputs
        if not data.get("emails") and not data.get("contacts"):
            logger.error(
                "Missing required data for predictive analysis",
                user_id=self.user_id
            )
            return {"error": "Insufficient data for predictive analysis"}
            
        try:
            # Extract data elements
            emails = data.get("emails", [])
            contacts = data.get("contacts", [])
            historical_data = data.get("historical_data", [])
            knowledge_tree = data.get("knowledge_tree", {})
            focus_area = data.get("specific_focus", "general trends")
            time_horizon = data.get("time_horizon", "next 6 months")
            
            # Prepare specialized system prompt
            system_prompt = self._create_system_prompt(focus_area, time_horizon)
            
            # Prepare user prompt with data
            user_prompt = self._create_user_prompt(
                emails=emails,
                contacts=contacts,
                historical_data=historical_data,
                knowledge_tree=knowledge_tree,
                focus_area=focus_area,
                time_horizon=time_horizon
            )
            
            # Run Claude Opus analysis
            result_text = await self._run_claude_analysis(system_prompt, user_prompt)
            
            # Extract structured components
            structured_result = self._parse_claude_response(result_text)
            
            # Add metadata
            structured_result["focus_area"] = focus_area
            structured_result["time_horizon"] = time_horizon
            structured_result["analysis_timestamp"] = datetime.utcnow().isoformat()
            structured_result["data_points_analyzed"] = {
                "emails": len(emails),
                "contacts": len(contacts),
                "historical_data_points": len(historical_data)
            }
            
            return structured_result
            
        except Exception as e:
            logger.error(
                "Error generating predictive intelligence",
                error=str(e),
                user_id=self.user_id
            )
            return {"error": f"Predictive intelligence generation failed: {str(e)}"}
    
    def _create_system_prompt(self, focus_area: str, time_horizon: str) -> str:
        """
        Create system prompt for Claude Opus
        
        Args:
            focus_area: Area to focus predictions on
            time_horizon: How far into future to predict
            
        Returns:
            System prompt string
        """
        return f"""You are an expert predictive intelligence analyst specializing in forecasting {focus_area} developments over the {time_horizon}.

Your task is to analyze communications, contact data, and historical information to identify patterns and forecast future trends and developments with a focus on practical, actionable intelligence.

Focus on:
1. Identifying established trends and their projected evolution
2. Detecting early signals of emerging developments
3. Forecasting likely future scenarios and their implications
4. Recognizing potential disruptions and inflection points
5. Evaluating patterns in communication and relationships that suggest future developments

Structure your analysis with these sections:
- Executive Summary: Brief overview of key predictive insights
- Current Trajectory: Analysis of established trends and their direction
- Emerging Signals: Early indicators of potential future developments
- Future Scenarios: Most likely developments over the specified time horizon
- Risk Factors: Potential disruptions or challenges to these predictions
- Recommendations: Actionable steps based on predictive analysis

Be balanced, evidence-based, and transparent about uncertainty levels.
Clearly distinguish between highly probable projections and more speculative forecasts.
Base all insights on the provided data and make reasonable inferences where appropriate.
Format your response as a combination of concise paragraphs and bullet points for readability.

Maintain absolute confidentiality and privacy in your analysis.
Do not include any information that could be considered sensitive or proprietary.
"""
    
    def _create_user_prompt(
        self, 
        emails: List[Dict],
        contacts: List[Dict],
        historical_data: List[Dict],
        knowledge_tree: Dict,
        focus_area: str,
        time_horizon: str
    ) -> str:
        """
        Create user prompt with data for analysis
        
        Args:
            emails: List of relevant email exchanges
            contacts: List of enriched contacts
            historical_data: Time series or historical information
            knowledge_tree: Knowledge graph data
            focus_area: Area to focus predictions on
            time_horizon: How far into future to predict
            
        Returns:
            Formatted user prompt
        """
        # Limit data volume to fit context window
        max_emails = min(30, len(emails))
        max_contacts = min(30, len(contacts))
        max_historical = min(20, len(historical_data))
        
        # Select most important data
        selected_emails = emails[:max_emails]
        selected_contacts = contacts[:max_contacts]
        selected_historical = historical_data[:max_historical]
        
        # Format prompt
        prompt = f"""Please analyze the following data and provide predictive insights for {focus_area} over the {time_horizon}.

## EMAIL COMMUNICATIONS
Here are {len(selected_emails)} relevant email exchanges that provide context on trends and developments:

```json
{self._format_json_for_prompt(selected_emails)}
```

## CONTACT DATA
Here are {len(selected_contacts)} contacts that provide context on the professional network:

```json
{self._format_json_for_prompt(selected_contacts)}
```
"""

        # Add historical data if available
        if selected_historical:
            prompt += f"""
## HISTORICAL DATA
Here is historical trend data for analysis:

```json
{self._format_json_for_prompt(selected_historical)}
```
"""

        # Add knowledge tree if available
        if knowledge_tree and knowledge_tree.get("nodes"):
            prompt += f"""
## KNOWLEDGE NETWORK
Here is a knowledge graph showing relationships:

```json
{self._format_json_for_prompt(knowledge_tree)}
```
"""

        # Add specific instructions
        prompt += f"""
Based on this data, please provide:

1. A predictive analysis for {focus_area} over the {time_horizon}
2. Identification of established trends and their likely evolution
3. Early signals of emerging developments
4. Most likely future scenarios and potential disruptions
5. Actionable recommendations based on these predictions

Organize your response according to the structure in your instructions.
"""

        return prompt
    
    def _parse_claude_response(self, response_text: str) -> Dict:
        """
        Parse Claude's response into structured components
        
        Args:
            response_text: Raw response from Claude
            
        Returns:
            Structured response dictionary
        """
        # Default structure
        result = {
            "executive_summary": "",
            "current_trajectory": [],
            "emerging_signals": [],
            "future_scenarios": [],
            "risk_factors": [],
            "recommendations": []
        }
        
        if not response_text:
            return result
            
        # Extract executive summary
        if "Executive Summary" in response_text:
            parts = response_text.split("Executive Summary", 1)
            if len(parts) > 1:
                summary_section = parts[1].split("\n#", 1)[0].split("Current Trajectory", 1)[0].strip()
                result["executive_summary"] = summary_section
        
        # Extract current trajectory
        if "Current Trajectory" in response_text:
            parts = response_text.split("Current Trajectory", 1)
            if len(parts) > 1:
                trajectory_text = parts[1].split("\n#", 1)[0].split("Emerging Signals", 1)[0]
                # Extract bullet points
                trajectories = []
                for line in trajectory_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        trajectories.append(line.strip()[2:])
                if trajectories:
                    result["current_trajectory"] = trajectories
                else:
                    result["current_trajectory"] = [trajectory_text.strip()]
        
        # Extract emerging signals
        if "Emerging Signals" in response_text:
            parts = response_text.split("Emerging Signals", 1)
            if len(parts) > 1:
                signals_text = parts[1].split("\n#", 1)[0].split("Future Scenarios", 1)[0]
                # Extract bullet points
                signals = []
                for line in signals_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        signals.append(line.strip()[2:])
                if signals:
                    result["emerging_signals"] = signals
                else:
                    result["emerging_signals"] = [signals_text.strip()]
        
        # Extract future scenarios
        if "Future Scenarios" in response_text:
            parts = response_text.split("Future Scenarios", 1)
            if len(parts) > 1:
                scenarios_text = parts[1].split("\n#", 1)[0].split("Risk Factors", 1)[0]
                # Extract bullet points
                scenarios = []
                for line in scenarios_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        scenarios.append(line.strip()[2:])
                if scenarios:
                    result["future_scenarios"] = scenarios
                else:
                    result["future_scenarios"] = [scenarios_text.strip()]
        
        # Extract risk factors
        if "Risk Factors" in response_text:
            parts = response_text.split("Risk Factors", 1)
            if len(parts) > 1:
                risks_text = parts[1].split("\n#", 1)[0].split("Recommendations", 1)[0]
                # Extract bullet points
                risks = []
                for line in risks_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        risks.append(line.strip()[2:])
                if risks:
                    result["risk_factors"] = risks
                else:
                    result["risk_factors"] = [risks_text.strip()]
        
        # Extract recommendations
        if "Recommendations" in response_text:
            parts = response_text.split("Recommendations", 1)
            if len(parts) > 1:
                recs_text = parts[1].strip()
                # Extract bullet points
                recs = []
                for line in recs_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        recs.append(line.strip()[2:])
                if recs:
                    result["recommendations"] = recs
                else:
                    result["recommendations"] = [recs_text.strip()]
        
        # Add full text as fallback
        result["full_analysis"] = response_text
        
        return result
    
    def _validate_input(self, input_data: Dict) -> Optional[str]:
        """
        Validate input data for predictive analysis
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Error message if validation fails, None if valid
        """
        if not input_data:
            return "No input data provided"
            
        if not input_data.get("emails") and not input_data.get("contacts"):
            return "Missing required data: need either emails or contacts"
            
        return None
    
    def _summarize_input(self, input_data: Dict) -> Dict:
        """
        Create summary of input data
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Input data summary
        """
        summary = {}
        
        if "emails" in input_data:
            emails = input_data["emails"]
            summary["emails_count"] = len(emails)
            
            # Get date range
            dates = [e.get("date") for e in emails if e.get("date")]
            if dates:
                summary["email_date_range"] = {
                    "earliest": min(dates),
                    "latest": max(dates)
                }
                
        if "contacts" in input_data:
            contacts = input_data["contacts"]
            summary["contacts_count"] = len(contacts)
            
        if "historical_data" in input_data:
            historical = input_data["historical_data"]
            summary["historical_data_points"] = len(historical)
            
            # Get types of historical data
            data_types = set()
            for item in historical:
                if "type" in item:
                    data_types.add(item["type"])
            
            summary["historical_data_types"] = list(data_types)
            
        if "specific_focus" in input_data:
            summary["focus_area"] = input_data["specific_focus"]
            
        if "time_horizon" in input_data:
            summary["time_horizon"] = input_data["time_horizon"]
            
        return summary

================================================================================
FILE: intelligence/analysts/relationship_analyst.py
SIZE: 14,440 bytes
LINES: 371
================================================================================

# intelligence/analysts/relationship_analyst.py
"""
Relationship Intelligence Analyst
==============================
Specialized analyst for relationship patterns and interpersonal dynamics.
Powered by Claude Opus with multi-tenant isolation.
"""

import json
from typing import Dict, List, Optional, Any
from datetime import datetime

from utils.logging import structured_logger as logger
from intelligence.analysts.base_analyst import BaseAnalyst

class RelationshipIntelligenceAnalyst(BaseAnalyst):
    """
    Relationship intelligence analyst
    
    Analyzes communication patterns, sentiment, and interpersonal dynamics
    to provide insights on professional relationships and network leverage.
    """
    
    def __init__(self, user_id: int, anthropic_api_key: str = None):
        """
        Initialize relationship analyst
        
        Args:
            user_id: User ID for multi-tenant isolation
            anthropic_api_key: Optional API key for Claude Opus
        """
        super().__init__(user_id, anthropic_api_key)
        self.analyst_name = "relationship_intelligence"
        self.analyst_description = "Relationship dynamics and network analysis specialist"
        
        # Customize Claude parameters
        self.max_tokens = 4000
        self.temperature = 0.25  # Slightly higher for insights on human dynamics
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """
        Generate relationship intelligence from provided data
        
        Args:
            data: Dictionary containing:
                - contacts: List of enriched contact data
                - emails: List of relevant email exchanges
                - (optional) knowledge_tree: Knowledge tree data
                - (optional) specific_focus: Relationship area or person to focus on
                
        Returns:
            Dictionary with relationship insights and recommendations
        """
        # Validate critical inputs
        if not data.get("contacts") and not data.get("emails"):
            logger.error(
                "Missing required data for relationship analysis",
                user_id=self.user_id
            )
            return {"error": "Insufficient data for relationship analysis"}
            
        try:
            # Extract data elements
            contacts = data.get("contacts", [])
            emails = data.get("emails", [])
            knowledge_tree = data.get("knowledge_tree", {})
            focus_area = data.get("specific_focus", "key relationships")
            
            # Prepare specialized system prompt
            system_prompt = self._create_system_prompt(focus_area)
            
            # Prepare user prompt with data
            user_prompt = self._create_user_prompt(
                contacts=contacts,
                emails=emails,
                knowledge_tree=knowledge_tree,
                focus_area=focus_area
            )
            
            # Run Claude Opus analysis
            result_text = await self._run_claude_analysis(system_prompt, user_prompt)
            
            # Extract structured components
            structured_result = self._parse_claude_response(result_text)
            
            # Add metadata
            structured_result["focus_area"] = focus_area
            structured_result["analysis_timestamp"] = datetime.utcnow().isoformat()
            structured_result["data_points_analyzed"] = {
                "contacts": len(contacts),
                "emails": len(emails),
                "knowledge_nodes": len(knowledge_tree.get("nodes", [])) if isinstance(knowledge_tree.get("nodes"), list) else 0
            }
            
            return structured_result
            
        except Exception as e:
            logger.error(
                "Error generating relationship intelligence",
                error=str(e),
                user_id=self.user_id
            )
            return {"error": f"Relationship intelligence generation failed: {str(e)}"}
    
    def _create_system_prompt(self, focus_area: str) -> str:
        """
        Create system prompt for Claude Opus
        
        Args:
            focus_area: Area to focus analysis on
            
        Returns:
            System prompt string
        """
        return f"""You are an expert relationship intelligence analyst specializing in interpersonal dynamics and professional networks.

Your task is to analyze communication patterns, sentiment in emails, and network connections to extract valuable insights about relationship dynamics and provide strategic recommendations for relationship management.

Focus on:
1. Identifying key relationship patterns and communication styles
2. Analyzing sentiment and emotional dynamics in communications
3. Detecting relationship strengths, weaknesses, and potential issues
4. Mapping spheres of influence and network leverage points
5. Recognizing trust levels and reciprocity in relationships

Structure your analysis with these sections:
- Key Relationship Insights: Overview of most significant relationship patterns
- Network Analysis: Mapping of connections, influence, and relationship clusters
- Communication Patterns: Analysis of communication styles, frequency, and effectiveness
- Relationship Opportunities: Areas to strengthen or leverage relationships
- Recommendations: Actionable steps to improve relationship outcomes

Be nuanced, empathetic, and practical in your analysis. Recognize the complexity of human relationships while providing clear, actionable insights.
Base all insights on the provided data and make reasonable inferences where appropriate.
Format your response as a combination of concise paragraphs and bullet points for readability.

Maintain absolute confidentiality and privacy in your analysis.
Do not include any information that could be considered sensitive or proprietary.
"""
    
    def _create_user_prompt(
        self, 
        contacts: List[Dict], 
        emails: List[Dict],
        knowledge_tree: Dict,
        focus_area: str
    ) -> str:
        """
        Create user prompt with data for analysis
        
        Args:
            contacts: List of enriched contact data
            emails: List of relevant email exchanges
            knowledge_tree: Knowledge graph data
            focus_area: Area to focus analysis on
            
        Returns:
            Formatted user prompt
        """
        # Limit data volume to fit context window
        max_contacts = min(50, len(contacts))
        max_emails = min(30, len(emails))
        
        # Select most important contacts and emails
        selected_contacts = contacts[:max_contacts]
        selected_emails = emails[:max_emails]
        
        # Format prompt
        prompt = f"""Please analyze the following relationship data and provide insights focused on {focus_area}.

## CONTACTS DATA
I'm providing information about {len(selected_contacts)} professional contacts with their profiles:

```json
{self._format_json_for_prompt(selected_contacts)}
```

## EMAIL COMMUNICATIONS
Here are {len(selected_emails)} relevant email exchanges that provide context on relationship dynamics:

```json
{self._format_json_for_prompt(selected_emails)}
```
"""

        # Add knowledge tree if available
        if knowledge_tree and knowledge_tree.get("nodes"):
            prompt += f"""
## RELATIONSHIP NETWORK
Here is a knowledge graph showing connections between people and entities:

```json
{self._format_json_for_prompt(knowledge_tree)}
```
"""

        # Add specific instructions
        prompt += f"""
Based on this data, please provide:

1. An insightful analysis of key relationship dynamics
2. Network analysis showing spheres of influence and connection patterns
3. Communication style analysis for key relationships
4. Opportunities to strengthen or leverage important relationships
5. Actionable recommendations for better relationship outcomes

Focus specifically on {focus_area} and organize your response according to the structure in your instructions.
"""

        return prompt
    
    def _parse_claude_response(self, response_text: str) -> Dict:
        """
        Parse Claude's response into structured components
        
        Args:
            response_text: Raw response from Claude
            
        Returns:
            Structured response dictionary
        """
        # Default structure
        result = {
            "key_relationship_insights": "",
            "network_analysis": "",
            "communication_patterns": [],
            "relationship_opportunities": [],
            "recommendations": []
        }
        
        if not response_text:
            return result
            
        # Extract key relationship insights
        if "Key Relationship Insights" in response_text:
            parts = response_text.split("Key Relationship Insights", 1)
            if len(parts) > 1:
                insights_section = parts[1].split("\n#", 1)[0].split("Network Analysis", 1)[0].strip()
                result["key_relationship_insights"] = insights_section
        
        # Extract network analysis
        if "Network Analysis" in response_text:
            parts = response_text.split("Network Analysis", 1)
            if len(parts) > 1:
                network_section = parts[1].split("\n#", 1)[0].split("Communication Patterns", 1)[0].strip()
                result["network_analysis"] = network_section
        
        # Extract communication patterns
        if "Communication Patterns" in response_text:
            parts = response_text.split("Communication Patterns", 1)
            if len(parts) > 1:
                patterns_text = parts[1].split("\n#", 1)[0].split("Relationship Opportunities", 1)[0]
                # Extract bullet points
                patterns = []
                for line in patterns_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        patterns.append(line.strip()[2:])
                if patterns:
                    result["communication_patterns"] = patterns
                else:
                    result["communication_patterns"] = [patterns_text.strip()]
        
        # Extract relationship opportunities
        if "Relationship Opportunities" in response_text:
            parts = response_text.split("Relationship Opportunities", 1)
            if len(parts) > 1:
                opps_text = parts[1].split("\n#", 1)[0].split("Recommendations", 1)[0]
                # Extract bullet points
                opps = []
                for line in opps_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        opps.append(line.strip()[2:])
                if opps:
                    result["relationship_opportunities"] = opps
                else:
                    result["relationship_opportunities"] = [opps_text.strip()]
        
        # Extract recommendations
        if "Recommendations" in response_text:
            parts = response_text.split("Recommendations", 1)
            if len(parts) > 1:
                recs_text = parts[1].strip()
                # Extract bullet points
                recs = []
                for line in recs_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        recs.append(line.strip()[2:])
                if recs:
                    result["recommendations"] = recs
                else:
                    result["recommendations"] = [recs_text.strip()]
        
        # Add full text as fallback
        result["full_analysis"] = response_text
        
        return result
    
    def _validate_input(self, input_data: Dict) -> Optional[str]:
        """
        Validate input data for relationship analysis
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Error message if validation fails, None if valid
        """
        if not input_data:
            return "No input data provided"
            
        if not input_data.get("contacts") and not input_data.get("emails"):
            return "Missing required data: need either contacts or emails"
            
        return None
    
    def _summarize_input(self, input_data: Dict) -> Dict:
        """
        Create summary of input data
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Input data summary
        """
        summary = {}
        
        if "contacts" in input_data:
            contacts = input_data["contacts"]
            summary["contacts_count"] = len(contacts)
            
            # Count contacts by company
            companies = {}
            for contact in contacts:
                company = contact.get("company", "Unknown")
                companies[company] = companies.get(company, 0) + 1
                
            summary["company_distribution"] = dict(sorted(
                companies.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5])  # Top 5 companies
            
        if "emails" in input_data:
            emails = input_data["emails"]
            summary["emails_count"] = len(emails)
            
            # Calculate communication frequency
            freq_by_person = {}
            for email in emails:
                sender = email.get("sender", "Unknown")
                freq_by_person[sender] = freq_by_person.get(sender, 0) + 1
                
                for recipient in email.get("recipients", []):
                    freq_by_person[recipient] = freq_by_person.get(recipient, 0) + 1
                    
            summary["top_communicators"] = dict(sorted(
                freq_by_person.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5])  # Top 5 communicators
            
        if "specific_focus" in input_data:
            summary["focus_area"] = input_data["specific_focus"]
            
        if "knowledge_tree" in input_data:
            tree = input_data["knowledge_tree"]
            summary["relationship_nodes"] = len(tree.get("nodes", []))
            summary["relationship_connections"] = len(tree.get("edges", []))
            
        return summary

================================================================================
FILE: intelligence/analysts/technical_analyst.py
SIZE: 14,238 bytes
LINES: 376
================================================================================

# intelligence/analysts/technical_analyst.py
"""
Technical Intelligence Analyst
===========================
Specialized analyst for technical insights and technology trends.
Powered by Claude Opus with multi-tenant isolation.
"""

import json
from typing import Dict, List, Optional, Any
from datetime import datetime

from utils.logging import structured_logger as logger
from intelligence.analysts.base_analyst import BaseAnalyst

class TechnicalIntelligenceAnalyst(BaseAnalyst):
    """
    Technical intelligence analyst
    
    Analyzes communications and documents to extract technical insights,
    identify technology trends, and provide technical strategy recommendations.
    """
    
    def __init__(self, user_id: int, anthropic_api_key: str = None):
        """
        Initialize technical analyst
        
        Args:
            user_id: User ID for multi-tenant isolation
            anthropic_api_key: Optional API key for Claude Opus
        """
        super().__init__(user_id, anthropic_api_key)
        self.analyst_name = "technical_intelligence"
        self.analyst_description = "Technical insights and technology trend analyst"
        
        # Customize Claude parameters
        self.max_tokens = 4000
        self.temperature = 0.1  # Lower temperature for more factual technical analysis
    
    async def generate_intelligence(self, data: Dict) -> Dict:
        """
        Generate technical intelligence from provided data
        
        Args:
            data: Dictionary containing:
                - emails: List of relevant email exchanges
                - (optional) documents: List of technical documents
                - (optional) knowledge_tree: Knowledge tree data
                - (optional) specific_focus: Technical area to focus on
                
        Returns:
            Dictionary with technical insights and recommendations
        """
        # Validate critical inputs
        if not data.get("emails") and not data.get("documents"):
            logger.error(
                "Missing required data for technical analysis",
                user_id=self.user_id
            )
            return {"error": "Insufficient data for technical analysis"}
            
        try:
            # Extract data elements
            emails = data.get("emails", [])
            documents = data.get("documents", [])
            knowledge_tree = data.get("knowledge_tree", {})
            focus_area = data.get("specific_focus", "technology trends")
            
            # Prepare specialized system prompt
            system_prompt = self._create_system_prompt(focus_area)
            
            # Prepare user prompt with data
            user_prompt = self._create_user_prompt(
                emails=emails,
                documents=documents,
                knowledge_tree=knowledge_tree,
                focus_area=focus_area
            )
            
            # Run Claude Opus analysis
            result_text = await self._run_claude_analysis(system_prompt, user_prompt)
            
            # Extract structured components
            structured_result = self._parse_claude_response(result_text)
            
            # Add metadata
            structured_result["focus_area"] = focus_area
            structured_result["analysis_timestamp"] = datetime.utcnow().isoformat()
            structured_result["data_points_analyzed"] = {
                "emails": len(emails),
                "documents": len(documents),
                "knowledge_nodes": len(knowledge_tree.get("nodes", [])) if isinstance(knowledge_tree.get("nodes"), list) else 0
            }
            
            return structured_result
            
        except Exception as e:
            logger.error(
                "Error generating technical intelligence",
                error=str(e),
                user_id=self.user_id
            )
            return {"error": f"Technical intelligence generation failed: {str(e)}"}
    
    def _create_system_prompt(self, focus_area: str) -> str:
        """
        Create system prompt for Claude Opus
        
        Args:
            focus_area: Technical area to focus on
            
        Returns:
            System prompt string
        """
        return f"""You are an expert technical intelligence analyst specializing in {focus_area}.

Your task is to analyze communications, documents, and knowledge graphs to extract valuable technical insights, identify technology trends, and provide strategic technical recommendations.

Focus on:
1. Identifying key technical topics and themes in communications
2. Detecting emerging technology trends and innovations
3. Recognizing technical challenges and potential solutions
4. Understanding technical dependencies and relationships
5. Evaluating technical strategies and architectural approaches

Structure your analysis with these sections:
- Technical Summary: Brief overview of key technical findings
- Key Technologies: Analysis of most significant technologies identified
- Technical Trends: Emerging patterns and directional shifts
- Technical Challenges: Identified issues and potential risks
- Recommendations: Actionable technical strategy recommendations

Be precise, evidence-based, and focused on practical technical value.
Base all insights on the provided data and make reasonable technical inferences where appropriate.
Format your response as a combination of concise technical paragraphs and bullet points for readability.

Maintain absolute confidentiality and privacy in your analysis.
Do not include any information that could be considered sensitive or proprietary.
"""
    
    def _create_user_prompt(
        self, 
        emails: List[Dict],
        documents: List[Dict],
        knowledge_tree: Dict,
        focus_area: str
    ) -> str:
        """
        Create user prompt with data for analysis
        
        Args:
            emails: List of relevant email exchanges
            documents: List of technical documents
            knowledge_tree: Knowledge graph data
            focus_area: Area to focus analysis on
            
        Returns:
            Formatted user prompt
        """
        # Limit data volume to fit context window
        max_emails = min(30, len(emails))
        max_documents = min(10, len(documents))
        
        # Select most important emails and documents
        selected_emails = emails[:max_emails]
        selected_documents = documents[:max_documents]
        
        # Format prompt
        prompt = f"""Please analyze the following technical data and provide insights focused on {focus_area}.

## EMAIL COMMUNICATIONS
Here are {len(selected_emails)} relevant email exchanges containing technical discussions:

```json
{self._format_json_for_prompt(selected_emails)}
```
"""

        # Add documents if available
        if selected_documents:
            prompt += f"""
## TECHNICAL DOCUMENTS
Here are {len(selected_documents)} technical documents for analysis:

```json
{self._format_json_for_prompt(selected_documents)}
```
"""

        # Add knowledge tree if available
        if knowledge_tree and knowledge_tree.get("nodes"):
            prompt += f"""
## TECHNICAL KNOWLEDGE GRAPH
Here is a knowledge graph showing technical relationships:

```json
{self._format_json_for_prompt(knowledge_tree)}
```
"""

        # Add specific instructions
        prompt += f"""
Based on this data, please provide:

1. A technical intelligence analysis identifying key technologies, trends, and patterns
2. An assessment of technical approaches and architectural decisions
3. Identification of potential technical challenges or risks
4. Technical strategy recommendations

Focus specifically on {focus_area} and organize your response according to the structure in your instructions.
"""

        return prompt
    
    def _parse_claude_response(self, response_text: str) -> Dict:
        """
        Parse Claude's response into structured components
        
        Args:
            response_text: Raw response from Claude
            
        Returns:
            Structured response dictionary
        """
        # Default structure
        result = {
            "technical_summary": "",
            "key_technologies": [],
            "technical_trends": [],
            "technical_challenges": [],
            "recommendations": []
        }
        
        if not response_text:
            return result
            
        # Extract technical summary
        if "Technical Summary" in response_text:
            parts = response_text.split("Technical Summary", 1)
            if len(parts) > 1:
                summary_section = parts[1].split("\n#", 1)[0].split("Key Technologies", 1)[0].strip()
                result["technical_summary"] = summary_section
        
        # Extract key technologies
        if "Key Technologies" in response_text:
            parts = response_text.split("Key Technologies", 1)
            if len(parts) > 1:
                tech_text = parts[1].split("\n#", 1)[0].split("Technical Trends", 1)[0]
                # Extract bullet points
                techs = []
                for line in tech_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        techs.append(line.strip()[2:])
                if techs:
                    result["key_technologies"] = techs
                else:
                    result["key_technologies"] = [tech_text.strip()]
        
        # Extract technical trends
        if "Technical Trends" in response_text:
            parts = response_text.split("Technical Trends", 1)
            if len(parts) > 1:
                trends_text = parts[1].split("\n#", 1)[0].split("Technical Challenges", 1)[0]
                # Extract bullet points
                trends = []
                for line in trends_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        trends.append(line.strip()[2:])
                if trends:
                    result["technical_trends"] = trends
                else:
                    result["technical_trends"] = [trends_text.strip()]
        
        # Extract technical challenges
        if "Technical Challenges" in response_text:
            parts = response_text.split("Technical Challenges", 1)
            if len(parts) > 1:
                challenges_text = parts[1].split("\n#", 1)[0].split("Recommendations", 1)[0]
                # Extract bullet points
                challenges = []
                for line in challenges_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        challenges.append(line.strip()[2:])
                if challenges:
                    result["technical_challenges"] = challenges
                else:
                    result["technical_challenges"] = [challenges_text.strip()]
        
        # Extract recommendations
        if "Recommendations" in response_text:
            parts = response_text.split("Recommendations", 1)
            if len(parts) > 1:
                recs_text = parts[1].strip()
                # Extract bullet points
                recs = []
                for line in recs_text.split("\n"):
                    if line.strip().startswith("- ") or line.strip().startswith("* "):
                        recs.append(line.strip()[2:])
                if recs:
                    result["recommendations"] = recs
                else:
                    result["recommendations"] = [recs_text.strip()]
        
        # Add full text as fallback
        result["full_analysis"] = response_text
        
        return result
    
    def _validate_input(self, input_data: Dict) -> Optional[str]:
        """
        Validate input data for technical analysis
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Error message if validation fails, None if valid
        """
        if not input_data:
            return "No input data provided"
            
        if not input_data.get("emails") and not input_data.get("documents"):
            return "Missing required data: need either emails or documents"
            
        return None
    
    def _summarize_input(self, input_data: Dict) -> Dict:
        """
        Create summary of input data
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Input data summary
        """
        summary = {}
        
        if "emails" in input_data:
            emails = input_data["emails"]
            summary["emails_count"] = len(emails)
            
            # Extract technical terms for summary
            tech_terms = set()
            tech_keywords = [
                "api", "architecture", "aws", "cloud", "code", "database", 
                "deployment", "development", "docker", "framework", "github", 
                "infrastructure", "kubernetes", "language", "microservices", 
                "platform", "programming", "python", "repository", "security", 
                "server", "service", "software", "system", "technology"
            ]
            
            for email in emails:
                content = email.get("body", "").lower()
                for term in tech_keywords:
                    if term in content:
                        tech_terms.add(term)
            
            summary["technical_terms"] = list(tech_terms)[:10]  # Top 10 terms
            
        if "documents" in input_data:
            documents = input_data["documents"]
            summary["documents_count"] = len(documents)
            
            # Categorize documents by type
            doc_types = {}
            for doc in documents:
                doc_type = doc.get("type", "unknown")
                doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
                
            summary["document_types"] = doc_types
            
        if "specific_focus" in input_data:
            summary["focus_area"] = input_data["specific_focus"]
            
        return summary

================================================================================
FILE: intelligence/web_enrichment/__init__.py
SIZE: 82 bytes
LINES: 2
================================================================================

# intelligence/web_enrichment/__init__.py
# Web enrichment package initialization

================================================================================
FILE: intelligence/web_enrichment/base_scraper.py
SIZE: 8,147 bytes
LINES: 271
================================================================================

# intelligence/web_enrichment/base_scraper.py
"""
Base Web Scraper
===============
Base class for all web scraping workers that enrich contact data.
Ensures proper multi-tenant isolation for user data.
"""

import asyncio
import aiohttp
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
import json
import time
from abc import ABC, abstractmethod

from playwright.async_api import async_playwright, Page, Browser
from utils.logging import structured_logger as logger

@dataclass
class EnrichmentResult:
    """Result of web enrichment for a contact"""
    email: str
    user_id: int
    source: str  # The source of enrichment (e.g., "linkedin", "twitter")
    data: Dict[str, Any] = field(default_factory=dict)
    raw_data: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 0.0
    enrichment_timestamp: datetime = None
    error: Optional[str] = None
    successful: bool = False


class BaseScraper(ABC):
    """
    Base class for web scrapers with consistent interface
    Implements multi-tenant isolation to ensure data privacy
    """
    
    def __init__(self, user_id: int = None, rate_limit: float = 2.0):
        """
        Initialize base scraper
        
        Args:
            user_id: ID of user for multi-tenant isolation
            rate_limit: Minimum seconds between requests to avoid rate limiting
        """
        self.user_id = user_id
        self.rate_limit = rate_limit
        self.last_request_time = 0
        self.browser = None
        self.context = None
        self.session = None
        self._initialized = False
        
    async def initialize(self) -> bool:
        """
        Initialize browser and session
        
        Returns:
            True if initialization successful
        """
        if self._initialized:
            return True
            
        try:
            # Create HTTP session for API requests
            self.session = aiohttp.ClientSession(
                headers=self._get_default_headers()
            )
            
            # Initialize playwright browser
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(
                headless=True,  # Use headless browser
            )
            
            # Create context with custom settings
            self.context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent=self._get_user_agent(),
                ignore_https_errors=True
            )
            
            self._initialized = True
            logger.info(f"{self.__class__.__name__} initialized", user_id=self.user_id)
            return True
            
        except Exception as e:
            logger.error(
                f"Failed to initialize {self.__class__.__name__}", 
                user_id=self.user_id,
                error=str(e)
            )
            await self.cleanup()
            return False
    
    async def cleanup(self) -> None:
        """Clean up resources when done"""
        try:
            if self.browser:
                await self.browser.close()
                self.browser = None
                
            if self.context:
                await self.context.close()
                self.context = None
                
            if self.session:
                await self.session.close()
                self.session = None
                
            self._initialized = False
            
        except Exception as e:
            logger.error(
                f"Error during {self.__class__.__name__} cleanup", 
                error=str(e),
                user_id=self.user_id
            )
    
    @abstractmethod
    async def enrich_contact(self, contact: Dict) -> EnrichmentResult:
        """
        Enrich a contact with web data
        Must be implemented by subclasses
        
        Args:
            contact: Contact data dictionary with at least 'email' field
            
        Returns:
            Enrichment result with data from web source
        """
        pass
    
    async def _new_page(self) -> Page:
        """
        Create a new browser page with rate limiting
        
        Returns:
            New playwright page
        """
        if not self._initialized:
            await self.initialize()
            
        # Rate limiting
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.rate_limit:
            await asyncio.sleep(self.rate_limit - time_since_last)
            
        # Update last request time
        self.last_request_time = time.time()
        
        # Create new page
        return await self.context.new_page()
    
    def _get_default_headers(self) -> Dict[str, str]:
        """
        Get default headers for requests
        
        Returns:
            Dictionary of HTTP headers
        """
        return {
            "User-Agent": self._get_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0",
            "TE": "trailers"
        }
    
    def _get_user_agent(self) -> str:
        """
        Get user agent string
        
        Returns:
            User agent string
        """
        return (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/119.0.0.0 Safari/537.36"
        )
    
    async def _take_screenshot(self, page: Page, name: str = "error") -> Optional[str]:
        """
        Take screenshot for debugging
        
        Args:
            page: Playwright page
            name: Screenshot name prefix
            
        Returns:
            Path to screenshot file
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"debug_{name}_{timestamp}.png"
            await page.screenshot(path=filename)
            return filename
        except Exception as e:
            logger.error("Failed to take screenshot", error=str(e))
            return None
    
    def _create_success_result(
        self,
        email: str,
        source: str,
        data: Dict[str, Any],
        raw_data: Dict[str, Any] = None,
        confidence: float = 1.0
    ) -> EnrichmentResult:
        """
        Create successful enrichment result
        
        Args:
            email: Contact email
            source: Source of the data
            data: Processed data
            raw_data: Raw data from source
            confidence: Confidence score (0.0-1.0)
            
        Returns:
            EnrichmentResult with successful status
        """
        return EnrichmentResult(
            email=email,
            user_id=self.user_id,
            source=source,
            data=data,
            raw_data=raw_data or {},
            confidence_score=confidence,
            enrichment_timestamp=datetime.utcnow(),
            successful=True
        )
    
    def _create_error_result(
        self,
        email: str,
        source: str,
        error: str
    ) -> EnrichmentResult:
        """
        Create error enrichment result
        
        Args:
            email: Contact email
            source: Source of the data
            error: Error message
            
        Returns:
            EnrichmentResult with error status
        """
        return EnrichmentResult(
            email=email,
            user_id=self.user_id,
            source=source,
            error=error,
            enrichment_timestamp=datetime.utcnow(),
            successful=False
        )

================================================================================
FILE: intelligence/web_enrichment/company_scraper.py
SIZE: 17,263 bytes
LINES: 456
================================================================================

# intelligence/web_enrichment/company_scraper.py
"""
Company Intelligence Scraper
==========================
Enriches company data from public web sources.
Ensures proper multi-tenant isolation for user data.
"""

import re
import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime

from utils.logging import structured_logger as logger
from intelligence.web_enrichment.base_scraper import BaseScraper, EnrichmentResult

class CompanyScraper(BaseScraper):
    """
    Company intelligence scraper for organizational data enrichment
    
    Extracts company information from publicly available sources
    to provide context on organizations where contacts work.
    """
    
    def __init__(self, user_id: int = None, rate_limit: float = 3.0):
        """
        Initialize company scraper
        
        Args:
            user_id: User ID for multi-tenant isolation
            rate_limit: Seconds between requests to avoid rate limiting
        """
        super().__init__(user_id, rate_limit)
        self.source = "company"
    
    async def enrich_contact(self, contact: Dict) -> EnrichmentResult:
        """
        Enrich contact with company data
        
        Args:
            contact: Contact information dictionary with at minimum:
                    - email: Email address
                    - company: (optional) Company name or None
                    
        Returns:
            EnrichmentResult with company data
        """
        if not contact.get("email"):
            return self._create_error_result(
                email=contact.get("email", "unknown"),
                source=self.source,
                error="Email is required for company enrichment"
            )
            
        try:
            # Initialize if needed
            if not self._initialized:
                success = await self.initialize()
                if not success:
                    return self._create_error_result(
                        email=contact["email"],
                        source=self.source,
                        error="Failed to initialize company scraper"
                    )
            
            # Extract email domain for company matching
            email = contact["email"]
            domain = email.split("@")[-1] if "@" in email else None
            
            # Try getting company name from contact data or LinkedIn enrichment
            company_name = (
                contact.get("company") or 
                contact.get("linkedin", {}).get("company") or 
                None
            )
            
            # Skip known generic domains
            if domain and self._is_generic_domain(domain):
                if not company_name:
                    return self._create_error_result(
                        email=email,
                        source=self.source,
                        error="Generic email domain with no company name provided"
                    )
            
            # If we have a domain but no company name, derive company from domain
            if domain and not company_name:
                company_name = self._domain_to_company_name(domain)
            
            if not company_name:
                return self._create_error_result(
                    email=email,
                    source=self.source,
                    error="Could not determine company name"
                )
            
            # Get company data
            company_data = await self._get_company_data(company_name, domain)
            
            if not company_data:
                return self._create_error_result(
                    email=email,
                    source=self.source,
                    error=f"No company data found for {company_name}"
                )
            
            # Format the data
            enriched_data = self._format_company_data(company_data)
            
            # Calculate confidence score based on data quality
            confidence = self._calculate_confidence(company_data, domain)
            
            return self._create_success_result(
                email=email,
                source=self.source,
                data=enriched_data,
                raw_data=company_data,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(
                "Company enrichment error",
                error=str(e),
                email=contact.get("email"),
                user_id=self.user_id
            )
            return self._create_error_result(
                email=contact["email"],
                source=self.source,
                error=f"Company enrichment failed: {str(e)}"
            )
    
    def _is_generic_domain(self, domain: str) -> bool:
        """
        Check if domain is a generic email provider
        
        Args:
            domain: Email domain
            
        Returns:
            True if generic provider
        """
        generic_domains = {
            "gmail.com", "yahoo.com", "hotmail.com", "outlook.com",
            "aol.com", "icloud.com", "mail.com", "protonmail.com",
            "zoho.com", "yandex.com", "gmx.com"
        }
        return domain.lower() in generic_domains
    
    def _domain_to_company_name(self, domain: str) -> str:
        """
        Convert domain to likely company name
        
        Args:
            domain: Email domain
            
        Returns:
            Company name derived from domain
        """
        # Strip TLD and common subdomains
        parts = domain.lower().split(".")
        
        # Remove TLD (.com, .org, etc)
        if len(parts) > 1:
            parts = parts[:-1]
            
        # Remove common subdomains
        if len(parts) > 1 and parts[0] in ["mail", "email", "corp", "www", "info"]:
            parts = parts[1:]
            
        # Join remaining parts and capitalize
        company_name = " ".join(parts).title()
        
        # Clean up
        company_name = re.sub(r'[-_]', ' ', company_name)
        
        return company_name
    
    async def _get_company_data(self, company_name: str, domain: str = None) -> Optional[Dict]:
        """
        Get company data from multiple sources
        
        Args:
            company_name: Company name
            domain: Company domain if known
            
        Returns:
            Company data or None if not found
        """
        # Try searching via structured search engines first
        company_data = await self._search_company_via_api(company_name, domain)
        
        # If that fails, fall back to web search
        if not company_data:
            company_data = await self._search_company_via_google(company_name, domain)
        
        return company_data
    
    async def _search_company_via_api(self, company_name: str, domain: str = None) -> Optional[Dict]:
        """
        Search for company using APIs
        
        Args:
            company_name: Company name
            domain: Company domain if known
            
        Returns:
            Company data dictionary if found
        """
        try:
            # Clearbit-like company API endpoint (placeholder)
            # In a real implementation, you would use a proper API service
            
            # Since we don't have actual API access, we'll simulate it
            # with a Google Knowledge Panel extraction instead
            
            search_query = f"{company_name} company"
            if domain:
                search_query += f" {domain}"
            
            encoded_query = search_query.replace(" ", "%20")
            
            # Use a page to search
            page = await self._new_page()
            
            try:
                # Search Google
                await page.goto(
                    f"https://www.google.com/search?q={encoded_query}",
                    wait_until="networkidle"
                )
                
                # Extract knowledge panel data
                company_data = await page.evaluate("""
                    () => {
                        // Helper to safely extract text
                        function safeText(selector) {
                            const el = document.querySelector(selector);
                            return el ? el.innerText.trim() : '';
                        }
                        
                        // Check if knowledge panel exists
                        const panel = document.querySelector('div[data-attrid], div.kp-header');
                        if (!panel) return null;
                        
                        // Extract company information
                        const name = safeText('h2[data-attrid="title"]') || safeText('div.kp-header h2');
                        
                        // Description
                        const description = safeText('[data-attrid="description"] > span, div[data-md]');
                        
                        // Type/industry/category
                        const type = safeText('[data-attrid*="category"] > span');
                        
                        // Founded data
                        const foundingDate = safeText('[data-attrid*="found"] > span');
                        
                        // Headquarters
                        const headquarters = safeText('[data-attrid*="locat"] > span');
                        
                        // Check for other common data attributes
                        const attributes = {};
                        document.querySelectorAll('[data-attrid]:not([data-attrid*="description"]):not([data-attrid="title"])').forEach(el => {
                            const key = el.getAttribute('data-attrid').split(':').pop();
                            const value = el.innerText.trim();
                            if (key && value) attributes[key] = value;
                        });
                        
                        // Website link
                        const websiteLink = document.querySelector('a[href^="https://"][data-attrid*="website"]');
                        const website = websiteLink ? websiteLink.href : '';
                        
                        return {
                            name,
                            description,
                            type,
                            founding_date: foundingDate,
                            headquarters,
                            website,
                            attributes,
                            source: 'google_knowledge_panel'
                        };
                    }
                """)
                
                return company_data
            
            finally:
                await page.close()
                
        except Exception as e:
            logger.error(
                f"Error in company API search", 
                error=str(e), 
                company=company_name,
                user_id=self.user_id
            )
            return None
    
    async def _search_company_via_google(self, company_name: str, domain: str = None) -> Optional[Dict]:
        """
        Search for company info via Google
        
        Args:
            company_name: Company name
            domain: Company domain if known
            
        Returns:
            Company data dictionary
        """
        try:
            # Build search query
            search_query = f"{company_name}"
            if domain:
                search_query += f" {domain}"
                
            search_query += " company about"
            
            # Encode query
            encoded_query = search_query.replace(" ", "+")
            
            # Get page
            page = await self._new_page()
            
            try:
                # Go to search results
                await page.goto(f"https://www.google.com/search?q={encoded_query}", wait_until="networkidle")
                
                # Extract snippet information
                company_data = await page.evaluate("""
                    () => {
                        const results = document.querySelectorAll('.g');
                        if (!results.length) return null;
                        
                        // Look for official site
                        let officialSite = null;
                        let description = '';
                        let headline = '';
                        
                        // Process search results
                        for (const result of results) {
                            const link = result.querySelector('a');
                            const url = link ? link.href : '';
                            const title = result.querySelector('h3') ? 
                                result.querySelector('h3').innerText : '';
                            
                            // Extract snippet text
                            const snippetEl = result.querySelector('div[style] > span');
                            const snippet = snippetEl ? snippetEl.innerText : '';
                            
                            // If this looks like an official site
                            if (url && title && (
                                title.toLowerCase().includes('official') ||
                                title.toLowerCase().includes('about us') ||
                                title.toLowerCase().includes('company')
                            )) {
                                officialSite = url;
                                description = snippet;
                                headline = title;
                                break;
                            }
                            
                            // Otherwise just use the first result
                            if (!officialSite && url) {
                                officialSite = url;
                                description = snippet;
                                headline = title;
                            }
                        }
                        
                        // Extract industry and size info if present
                        const industryMatch = description.match(/(?:industry|sector):\\s*([^\\.|,]+)/i);
                        const sizeMatch = description.match(/(?:employees|size|people):\\s*([^\\.|,]+)/i);
                        
                        return {
                            name: headline.replace(/- .+$/, '').trim(),
                            description,
                            website: officialSite,
                            industry: industryMatch ? industryMatch[1].trim() : '',
                            size: sizeMatch ? sizeMatch[1].trim() : '',
                            source: 'google_search'
                        };
                    }
                """)
                
                return company_data
                
            finally:
                await page.close()
                
        except Exception as e:
            logger.error(
                f"Error in company Google search", 
                error=str(e),
                company=company_name,
                user_id=self.user_id
            )
            return None
    
    def _calculate_confidence(self, data: Dict, domain: str = None) -> float:
        """
        Calculate confidence score for company data
        
        Args:
            data: Company data
            domain: Domain name if available
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        # Base confidence
        confidence = 0.5
        
        # If we have no data
        if not data:
            return 0.0
        
        # Name match with domain increases confidence
        if domain and data.get("website"):
            website = data["website"].lower()
            if domain.lower() in website:
                confidence += 0.3
                
        # Comprehensive data increases confidence
        if data.get("description") and len(data.get("description", "")) > 50:
            confidence += 0.1
            
        # More fields means higher confidence
        fields_count = sum(1 for k, v in data.items() if v and k != "source")
        confidence += min(0.2, fields_count * 0.02)
        
        # Cap at 1.0
        return min(1.0, confidence)
    
    def _format_company_data(self, data: Dict) -> Dict:
        """
        Format company data consistently
        
        Args:
            data: Raw company data
            
        Returns:
            Formatted company data
        """
        return {
            "name": data.get("name", ""),
            "description": data.get("description", ""),
            "website": data.get("website", ""),
            "industry": data.get("industry", data.get("type", "")),
            "founded": data.get("founding_date", ""),
            "headquarters": data.get("headquarters", ""),
            "size": data.get("size", ""),
            "data_source": data.get("source", "web")
        }

================================================================================
FILE: intelligence/web_enrichment/enrichment_orchestrator.py
SIZE: 9,803 bytes
LINES: 275
================================================================================

# intelligence/web_enrichment/enrichment_orchestrator.py
"""
Contact Enrichment Orchestrator
=============================
Coordinates contact enrichment activities across multiple web sources.
Ensures proper multi-tenant isolation for user data.
"""

import asyncio
import time
from typing import Dict, List, Optional, Any, Union
from datetime import datetime

from utils.logging import structured_logger as logger
from storage.storage_manager import get_storage_manager
from intelligence.web_enrichment.base_scraper import BaseScraper, EnrichmentResult
from intelligence.web_enrichment.linkedin_scraper import LinkedInScraper
from intelligence.web_enrichment.twitter_scraper import TwitterScraper

class EnrichmentOrchestrator:
    """
    Orchestrates the enrichment of contact data from multiple sources
    with multi-tenant isolation for user privacy
    """
    
    def __init__(self, user_id: int):
        """
        Initialize orchestrator for a specific user
        
        Args:
            user_id: User ID for multi-tenant isolation
        """
        self.user_id = user_id
        self.storage_manager = None
        self.scrapers = {}
        
    async def initialize(self) -> bool:
        """
        Initialize resources and connections
        
        Returns:
            True if initialization was successful
        """
        try:
            # Get storage manager
            self.storage_manager = await get_storage_manager()
            
            # Initialize scrapers
            self.scrapers = {
                'linkedin': LinkedInScraper(self.user_id),
                'twitter': TwitterScraper(self.user_id)
            }
            
            # Initialize each scraper
            for name, scraper in self.scrapers.items():
                await scraper.initialize()
                
            return True
            
        except Exception as e:
            logger.error("Failed to initialize enrichment orchestrator", 
                        user_id=self.user_id, error=str(e))
            return False
    
    async def cleanup(self) -> None:
        """Clean up resources"""
        for name, scraper in self.scrapers.items():
            try:
                await scraper.cleanup()
            except Exception as e:
                logger.error(f"Error during {name} scraper cleanup", 
                            user_id=self.user_id, error=str(e))
    
    async def enrich_contact(self, contact: Dict, sources: List[str] = None) -> Dict[str, EnrichmentResult]:
        """
        Enrich a single contact from specified sources
        
        Args:
            contact: Contact dictionary with at least 'email' field
            sources: List of sources to use (defaults to all)
            
        Returns:
            Dictionary of enrichment results by source
        """
        if not contact.get('email'):
            logger.error("Cannot enrich contact without email", user_id=self.user_id)
            return {}
        
        # Use all sources if not specified
        if not sources:
            sources = list(self.scrapers.keys())
            
        results = {}
        
        # Run each scraper
        for source in sources:
            if source not in self.scrapers:
                logger.warning(f"Unknown enrichment source: {source}", user_id=self.user_id)
                continue
                
            scraper = self.scrapers[source]
            try:
                result = await scraper.enrich_contact(contact)
                results[source] = result
                
                # Short delay between sources
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"Error enriching contact from {source}", 
                           user_id=self.user_id, email=contact.get('email'), error=str(e))
                
        return results
    
    async def enrich_contacts(
        self, 
        contacts: List[Dict], 
        sources: List[str] = None,
        concurrency: int = 2
    ) -> Dict[str, Dict[str, EnrichmentResult]]:
        """
        Enrich multiple contacts with rate limiting
        
        Args:
            contacts: List of contact dictionaries
            sources: List of sources to use (defaults to all)
            concurrency: Maximum number of concurrent enrichment operations
            
        Returns:
            Dictionary of enrichment results by contact email and source
        """
        # Initialize if not already done
        if not self.scrapers:
            success = await self.initialize()
            if not success:
                return {}
                
        # Validate contacts
        valid_contacts = [c for c in contacts if c.get('email')]
        
        if not valid_contacts:
            logger.warning("No valid contacts for enrichment", user_id=self.user_id)
            return {}
        
        # Process in batches for rate limiting
        results = {}
        semaphore = asyncio.Semaphore(concurrency)
        
        async def process_contact(contact: Dict) -> None:
            """Process a single contact with semaphore"""
            async with semaphore:
                email = contact.get('email')
                try:
                    contact_results = await self.enrich_contact(contact, sources)
                    results[email] = contact_results
                    
                    # Store in database if we have successful results
                    await self._store_enrichment_results(contact, contact_results)
                    
                    # Small delay to be nice to APIs
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"Failed to enrich contact", 
                               email=email, user_id=self.user_id, error=str(e))
        
        # Create tasks for all contacts
        tasks = [process_contact(contact) for contact in valid_contacts]
        
        # Run with progress tracking
        completed = 0
        total = len(tasks)
        
        for future in asyncio.as_completed(tasks):
            try:
                await future
                completed += 1
                
                # Log progress
                if completed % max(1, total // 10) == 0 or completed == total:
                    logger.info(f"Enrichment progress: {completed}/{total} contacts", 
                               user_id=self.user_id)
                    
            except Exception as e:
                logger.error("Error in contact enrichment task", 
                           user_id=self.user_id, error=str(e))
        
        return results
    
    async def _store_enrichment_results(
        self, 
        contact: Dict, 
        results: Dict[str, EnrichmentResult]
    ) -> None:
        """
        Store enrichment results in database
        
        Args:
            contact: Original contact data
            results: Enrichment results by source
        """
        if not results:
            return
            
        try:
            # Process only successful results
            data_to_store = {
                'enriched_at': datetime.utcnow().isoformat(),
                'email': contact.get('email')
            }
            
            # Merge data from each successful source
            for source, result in results.items():
                if result and result.successful:
                    data_to_store[source] = result.data
            
            # Only store if we have enrichment data
            if len(data_to_store) > 2:  # More than just timestamp and email
                # Calculate overall enrichment quality
                sources_count = sum(1 for _, r in results.items() if r and r.successful)
                enrichment_quality = min(1.0, sources_count / max(1, len(results)))
                
                data_to_store['enrichment_quality'] = enrichment_quality
                
                # Store in database
                await self.storage_manager.store_contact_enrichment(
                    self.user_id, 
                    contact.get('email'), 
                    data_to_store
                )
                
        except Exception as e:
            logger.error(f"Failed to store enrichment results", 
                       email=contact.get('email'), user_id=self.user_id, error=str(e))

# Convenience function for external calls
async def enrich_user_contacts(
    user_id: int, 
    contacts: List[Dict],
    sources: List[str] = None
) -> Dict:
    """
    Enrich contacts for a specific user
    
    Args:
        user_id: User ID for multi-tenant isolation
        contacts: List of contact dictionaries
        sources: List of sources to use
        
    Returns:
        Enrichment results
    """
    orchestrator = EnrichmentOrchestrator(user_id)
    try:
        await orchestrator.initialize()
        results = await orchestrator.enrich_contacts(contacts, sources)
        
        # Count success and failures
        success_count = sum(1 for email, res in results.items() 
                           for source, r in res.items() 
                           if r and r.successful)
        
        total_attempts = sum(1 for email, res in results.items() 
                            for source, _ in res.items())
        
        return {
            'status': 'completed',
            'success_rate': success_count / max(1, total_attempts) if total_attempts else 0,
            'contacts_processed': len(contacts),
            'successful_enrichments': success_count,
            'timestamp': datetime.utcnow().isoformat()
        }
    
    finally:
        await orchestrator.cleanup()

================================================================================
FILE: intelligence/web_enrichment/linkedin_scraper.py
SIZE: 18,337 bytes
LINES: 428
================================================================================

# intelligence/web_enrichment/linkedin_scraper.py
"""
LinkedIn Web Scraper
==================
Enriches contact data with LinkedIn profile information.
Uses a privacy-conscious approach for multi-tenant systems.
"""

import re
import asyncio
import time
from typing import Dict, List, Optional, Any

from playwright.async_api import Page
from utils.logging import structured_logger as logger
from intelligence.web_enrichment.base_scraper import BaseScraper, EnrichmentResult

class LinkedInScraper(BaseScraper):
    """
    LinkedIn web scraper for contact enrichment
    
    Extracts professional information about contacts from LinkedIn 
    while respecting website terms and user privacy.
    """
    
    def __init__(self, user_id: int = None, rate_limit: float = 3.0):
        """
        Initialize LinkedIn scraper
        
        Args:
            user_id: User ID for multi-tenant isolation
            rate_limit: Seconds between requests to avoid rate limiting
        """
        super().__init__(user_id, rate_limit)
        self.source = "linkedin"
    
    async def enrich_contact(self, contact: Dict) -> EnrichmentResult:
        """
        Enrich contact with LinkedIn data
        
        Args:
            contact: Contact information dictionary with at minimum:
                    - email: Email address
                    - name: (optional) Full name
                    - company: (optional) Company name
                    
        Returns:
            EnrichmentResult with LinkedIn profile data
        """
        if not contact.get("email"):
            return self._create_error_result(
                email=contact.get("email", "unknown"),
                source=self.source,
                error="Email is required for LinkedIn enrichment"
            )
            
        try:
            # Initialize if needed
            if not self._initialized:
                success = await self.initialize()
                if not success:
                    return self._create_error_result(
                        email=contact["email"],
                        source=self.source,
                        error="Failed to initialize LinkedIn scraper"
                    )
            
            # Build search query from available information
            name = contact.get("name", "")
            company = contact.get("company", "")
            domain = contact.get("domain", "")
            
            if not name:
                # Try to extract name from email if not provided
                email_parts = contact["email"].split("@")[0].split(".")
                if len(email_parts) >= 2:
                    name = " ".join([p.capitalize() for p in email_parts])
            
            if not company and domain and domain != "gmail.com":
                # Use domain as company name if available
                company = domain.split(".")[0].capitalize()
            
            # If we still don't have enough info, return error
            if not name:
                return self._create_error_result(
                    email=contact["email"],
                    source=self.source,
                    error="Insufficient information to search LinkedIn"
                )
                
            # Search for LinkedIn profile
            profile_data = await self._search_linkedin(name, company)
            
            if not profile_data:
                return self._create_error_result(
                    email=contact["email"], 
                    source=self.source,
                    error="No LinkedIn profile found"
                )
            
            # If we have a profile URL, get more details
            if profile_data.get("profile_url"):
                detailed_data = await self._extract_profile_details(profile_data["profile_url"])
                profile_data.update(detailed_data)
            
            # Calculate confidence score based on name match
            confidence = self._calculate_confidence_score(name, profile_data.get("name", ""))
            
            # Format the enriched data
            enriched_data = self._format_linkedin_data(profile_data)
            
            return self._create_success_result(
                email=contact["email"],
                source=self.source,
                data=enriched_data,
                raw_data=profile_data,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(
                "LinkedIn enrichment error",
                error=str(e),
                email=contact.get("email"),
                user_id=self.user_id
            )
            return self._create_error_result(
                email=contact["email"],
                source=self.source,
                error=f"LinkedIn enrichment failed: {str(e)}"
            )
    
    async def _search_linkedin(self, name: str, company: str = "") -> Optional[Dict]:
        """
        Search LinkedIn for a person
        
        Args:
            name: Person's name
            company: Optional company name for better targeting
            
        Returns:
            Dictionary with basic profile data or None if not found
        """
        try:
            # Create search query
            search_query = f"{name}"
            if company:
                search_query += f" {company}"
                
            # URL encode the query
            encoded_query = search_query.replace(" ", "%20")
                
            # Get new browser page with rate limiting
            page = await self._new_page()
            
            try:
                # Navigate to LinkedIn search
                search_url = f"https://www.linkedin.com/search/results/people/?keywords={encoded_query}"
                await page.goto(search_url, wait_until="networkidle")
                
                # Check if we're on login page (LinkedIn blocks unauthenticated searches)
                is_login_page = await page.evaluate("""
                    () => {
                        return Boolean(document.querySelector('form[data-id="sign-in-form"]'));
                    }
                """)
                
                if is_login_page:
                    logger.warning(
                        "LinkedIn requires authentication for search", 
                        user_id=self.user_id
                    )
                    
                    # Fallback to Google search for LinkedIn profile
                    await page.goto(
                        f"https://www.google.com/search?q={encoded_query}+linkedin", 
                        wait_until="networkidle"
                    )
                    
                    # Extract LinkedIn profile from Google search results
                    profile_data = await page.evaluate("""
                        () => {
                            const linkedInResults = Array.from(document.querySelectorAll('a[href*="linkedin.com/in/"]'));
                            if (linkedInResults.length === 0) return null;
                            
                            const firstResult = linkedInResults[0];
                            const url = firstResult.href;
                            const title = firstResult.querySelector('h3') 
                                ? firstResult.querySelector('h3').innerText 
                                : '';
                            const snippet = firstResult.parentElement.querySelector('div[style] div[style]') 
                                ? firstResult.parentElement.querySelector('div[style] div[style]').innerText
                                : '';
                                
                            return {
                                name: title.replace(' | LinkedIn', '').replace(' - LinkedIn', ''),
                                snippet: snippet,
                                profile_url: url
                            };
                        }
                    """)
                    
                    # Extract company and title from snippet
                    if profile_data and profile_data.get("snippet"):
                        snippet = profile_data.get("snippet", "")
                        
                        # Try to extract company and title from snippet
                        profile_data["position"] = ""
                        profile_data["company"] = ""
                        
                        position_match = re.search(r'(?:is|as|at)\s+([^\.]+?)(?:at|in|\.|\-)', snippet)
                        if position_match:
                            profile_data["position"] = position_match.group(1).strip()
                            
                        company_match = re.search(r'(?:at|@)\s+([^\.]+?)(?:in|\.|\-)', snippet)
                        if company_match:
                            profile_data["company"] = company_match.group(1).strip()
                    
                    return profile_data
                else:
                    # Extract data from LinkedIn search results
                    profile_data = await page.evaluate("""
                        () => {
                            const results = document.querySelectorAll('.reusable-search__result-container');
                            if (results.length === 0) return null;
                            
                            const firstResult = results[0];
                            const nameElement = firstResult.querySelector('.entity-result__title-text a');
                            const subtitleElement = firstResult.querySelector('.entity-result__primary-subtitle');
                            const locationElement = firstResult.querySelector('.entity-result__secondary-subtitle');
                            
                            if (!nameElement) return null;
                            
                            const profileUrl = nameElement.href;
                            const name = nameElement.innerText.trim();
                            const position = subtitleElement ? subtitleElement.innerText.trim() : '';
                            const location = locationElement ? locationElement.innerText.trim() : '';
                            
                            return {
                                name,
                                position,
                                location,
                                profile_url: profileUrl
                            };
                        }
                    """)
                    
                    return profile_data
            
            finally:
                # Always close the page to avoid memory leaks
                await page.close()
        
        except Exception as e:
            logger.error(
                "LinkedIn search error",
                error=str(e),
                name=name,
                company=company,
                user_id=self.user_id
            )
            return None
    
    async def _extract_profile_details(self, profile_url: str) -> Dict:
        """
        Extract more details from LinkedIn profile page
        
        Args:
            profile_url: URL to LinkedIn profile
            
        Returns:
            Dictionary with detailed profile data
        """
        try:
            # Get new page
            page = await self._new_page()
            
            try:
                # Navigate to profile
                await page.goto(profile_url, wait_until="networkidle")
                
                # Check if we hit a login wall
                is_login_wall = await page.evaluate("""
                    () => {
                        return Boolean(document.querySelector('.login__form'));
                    }
                """)
                
                if is_login_wall:
                    # We can only get limited info from preview
                    profile_preview = await page.evaluate("""
                        () => {
                            const name = document.querySelector('h1')?.innerText || '';
                            const headline = document.querySelector('h2')?.innerText || '';
                            
                            return {
                                full_name: name,
                                headline: headline,
                                limited_access: true
                            };
                        }
                    """)
                    
                    return profile_preview
                else:
                    # Extract more detailed information
                    detailed_data = await page.evaluate("""
                        () => {
                            // Helper to safely extract text
                            function safeText(selector) {
                                const element = document.querySelector(selector);
                                return element ? element.innerText.trim() : '';
                            }
                            
                            // Extract experience items
                            const experienceItems = Array.from(
                                document.querySelectorAll('.experience-section li')
                            ).map(item => {
                                const title = item.querySelector('.pv-entity__summary-info h3')?.innerText || '';
                                const company = item.querySelector('.pv-entity__secondary-title')?.innerText || '';
                                const dateRange = item.querySelector('.pv-entity__date-range span:not(:first-child)')?.innerText || '';
                                return { title, company, dateRange };
                            });
                            
                            // Extract education items
                            const educationItems = Array.from(
                                document.querySelectorAll('.education-section li')
                            ).map(item => {
                                const school = item.querySelector('h3')?.innerText || '';
                                const degree = item.querySelector('.pv-entity__secondary-title')?.innerText || '';
                                const years = item.querySelector('.pv-entity__dates span:not(:first-child)')?.innerText || '';
                                return { school, degree, years };
                            });
                            
                            // Extract skills
                            const skills = Array.from(
                                document.querySelectorAll('.pv-skill-categories-section li .pv-skill-category-entity__name-text')
                            ).map(skill => skill.innerText.trim());
                            
                            return {
                                about: safeText('.pv-about-section .pv-about__summary-text'),
                                experience: experienceItems.slice(0, 3), // Limit to recent 3
                                education: educationItems,
                                skills: skills.slice(0, 10), // Limit to top 10
                                connections: safeText('.pv-top-card--list .pv-top-card--list-bullet:first-child'),
                                activity_status: safeText('.pv-recent-activity-section-v2__headline h3')
                            };
                        }
                    """)
                    
                    return detailed_data
                    
            finally:
                # Always close the page
                await page.close()
                
        except Exception as e:
            logger.error(
                "LinkedIn profile details error", 
                error=str(e),
                profile_url=profile_url,
                user_id=self.user_id
            )
            return {}
    
    def _calculate_confidence_score(self, query_name: str, result_name: str) -> float:
        """
        Calculate confidence score based on name match
        
        Args:
            query_name: The name we searched for
            result_name: The name we found
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        if not query_name or not result_name:
            return 0.5  # Default moderate confidence
        
        # Normalize names
        query_name = query_name.lower().strip()
        result_name = result_name.lower().strip()
        
        # Exact match
        if query_name == result_name:
            return 1.0
            
        # Check if all query name parts are in result name
        query_parts = set(query_name.split())
        result_parts = set(result_name.split())
        
        # If all parts are contained, high confidence
        if query_parts.issubset(result_parts):
            return 0.9
            
        # Calculate how many parts match
        matching_parts = query_parts.intersection(result_parts)
        match_ratio = len(matching_parts) / max(len(query_parts), 1)
        
        # Scale from 0.3 to 0.8 based on match ratio
        confidence = 0.3 + (match_ratio * 0.5)
        
        return confidence
        
    def _format_linkedin_data(self, raw_data: Dict) -> Dict:
        """
        Format LinkedIn data into a consistent structure
        
        Args:
            raw_data: Raw LinkedIn data
            
        Returns:
            Consistently formatted LinkedIn profile data
        """
        return {
            "name": raw_data.get("name", ""),
            "position": raw_data.get("position", ""),
            "company": raw_data.get("company", ""),
            "location": raw_data.get("location", ""),
            "profile_url": raw_data.get("profile_url", ""),
            "headline": raw_data.get("headline", ""),
            "about": raw_data.get("about", ""),
            "experience": raw_data.get("experience", []),
            "education": raw_data.get("education", []),
            "skills": raw_data.get("skills", []),
            "connections": raw_data.get("connections", ""),
            "limited_data": raw_data.get("limited_access", False)
        }

================================================================================
FILE: intelligence/web_enrichment/twitter_scraper.py
SIZE: 15,159 bytes
LINES: 404
================================================================================

# intelligence/web_enrichment/twitter_scraper.py
"""
Twitter/X Web Scraper
===================
Enriches contact data with Twitter profile information.
Uses a privacy-conscious approach for multi-tenant systems.
"""

import re
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime

from utils.logging import structured_logger as logger
from intelligence.web_enrichment.base_scraper import BaseScraper, EnrichmentResult

class TwitterScraper(BaseScraper):
    """
    Twitter/X web scraper for contact enrichment
    
    Extracts social media information about contacts while
    respecting website terms and user privacy.
    """
    
    def __init__(self, user_id: int = None, rate_limit: float = 2.5):
        """
        Initialize Twitter scraper
        
        Args:
            user_id: User ID for multi-tenant isolation
            rate_limit: Seconds between requests to avoid rate limiting
        """
        super().__init__(user_id, rate_limit)
        self.source = "twitter"
    
    async def enrich_contact(self, contact: Dict) -> EnrichmentResult:
        """
        Enrich contact with Twitter data
        
        Args:
            contact: Contact information dictionary with at minimum:
                    - email: Email address
                    - name: (optional) Full name
                    
        Returns:
            EnrichmentResult with Twitter profile data
        """
        if not contact.get("email"):
            return self._create_error_result(
                email=contact.get("email", "unknown"),
                source=self.source,
                error="Email is required for Twitter enrichment"
            )
            
        try:
            # Initialize if needed
            if not self._initialized:
                success = await self.initialize()
                if not success:
                    return self._create_error_result(
                        email=contact["email"],
                        source=self.source,
                        error="Failed to initialize Twitter scraper"
                    )
            
            # Extract data points from contact
            email = contact["email"]
            name = contact.get("name", "")
            
            # Try to find Twitter handle
            twitter_handle = await self._find_twitter_handle(email, name)
            
            if not twitter_handle:
                return self._create_error_result(
                    email=email,
                    source=self.source,
                    error="Could not find Twitter handle"
                )
            
            # Get profile data
            profile_data = await self._get_profile_data(twitter_handle)
            
            if not profile_data:
                return self._create_error_result(
                    email=email,
                    source=self.source,
                    error=f"No Twitter profile found for handle @{twitter_handle}"
                )
            
            # Format the data
            enriched_data = self._format_twitter_data(profile_data)
            
            # Calculate confidence based on name match
            confidence = self._calculate_confidence(name, profile_data.get("name", ""))
            
            return self._create_success_result(
                email=email,
                source=self.source,
                data=enriched_data,
                raw_data=profile_data,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(
                "Twitter enrichment error",
                error=str(e),
                email=contact.get("email"),
                user_id=self.user_id
            )
            return self._create_error_result(
                email=contact["email"],
                source=self.source,
                error=f"Twitter enrichment failed: {str(e)}"
            )
    
    async def _find_twitter_handle(self, email: str, name: str) -> Optional[str]:
        """
        Try to find Twitter handle from email or name
        
        Args:
            email: Contact's email address
            name: Contact's name
            
        Returns:
            Twitter handle if found, None otherwise
        """
        possible_handles = []
        
        # Try email-based handles
        if email and "@" in email:
            username = email.split("@")[0]
            possible_handles.append(username)
            
            # Common variations
            if "." in username:
                possible_handles.append(username.replace(".", ""))
                
            # Firstname.lastname format
            if "." in username:
                parts = username.split(".")
                if len(parts) == 2:
                    possible_handles.append(parts[0])  # Just first name
                    possible_handles.append(f"{parts[0]}_{parts[1]}")  # firstname_lastname
        
        # Try name-based handles
        if name:
            # Clean and normalize name
            name_clean = re.sub(r'[^\w\s]', '', name).lower()
            name_parts = name_clean.split()
            
            if len(name_parts) >= 2:
                # First initial + lastname
                possible_handles.append(f"{name_parts[0][0]}{name_parts[-1]}")
                
                # First name + last name
                possible_handles.append(f"{name_parts[0]}{name_parts[-1]}")
                
                # First name + underscore + last name
                possible_handles.append(f"{name_parts[0]}_{name_parts[-1]}")
        
        # Try each handle
        for handle in possible_handles:
            if await self._check_twitter_handle(handle):
                return handle
                
        # If no match found, try Google search
        return await self._search_twitter_via_google(name)
    
    async def _check_twitter_handle(self, handle: str) -> bool:
        """
        Check if a Twitter handle exists
        
        Args:
            handle: Twitter handle to check
            
        Returns:
            True if handle exists
        """
        try:
            # Clean handle
            handle = handle.strip().lower()
            if handle.startswith("@"):
                handle = handle[1:]
                
            # Try direct profile access
            url = f"https://twitter.com/{handle}"
            
            async with self.session.head(url, allow_redirects=True) as response:
                return response.status == 200
                
        except Exception as e:
            logger.debug(
                f"Error checking Twitter handle {handle}",
                error=str(e)
            )
            return False
    
    async def _search_twitter_via_google(self, name: str) -> Optional[str]:
        """
        Search for Twitter profile using Google
        
        Args:
            name: Person's name
            
        Returns:
            Twitter handle if found
        """
        if not name:
            return None
            
        try:
            page = await self._new_page()
            
            try:
                # Encode search query
                encoded_name = name.replace(" ", "+")
                search_url = f"https://www.google.com/search?q={encoded_name}+twitter+profile"
                
                await page.goto(search_url, wait_until="networkidle")
                
                # Extract Twitter URLs from search results
                twitter_handle = await page.evaluate("""
                    () => {
                        const twitterLinks = Array.from(document.querySelectorAll('a[href*="twitter.com/"]'));
                        
                        for (const link of twitterLinks) {
                            const href = link.href;
                            if (href.includes('/status/')) continue; // Skip tweet links
                            
                            const match = href.match(/twitter\\.com\\/(\\w+)/);
                            if (match && match[1] && match[1] !== 'search' && match[1].length > 1) {
                                return match[1];
                            }
                        }
                        
                        return null;
                    }
                """)
                
                return twitter_handle
            
            finally:
                await page.close()
                
        except Exception as e:
            logger.error(f"Twitter Google search error: {str(e)}")
            return None
    
    async def _get_profile_data(self, handle: str) -> Optional[Dict]:
        """
        Get Twitter profile data
        
        Args:
            handle: Twitter handle
            
        Returns:
            Profile data dictionary
        """
        try:
            # Clean handle
            handle = handle.strip().lower()
            if handle.startswith("@"):
                handle = handle[1:]
                
            profile_url = f"https://twitter.com/{handle}"
            
            page = await self._new_page()
            
            try:
                await page.goto(profile_url, wait_until="domcontentloaded", timeout=30000)
                await page.wait_for_selector('main', timeout=10000)
                
                # Extract profile data
                profile_data = await page.evaluate("""
                    () => {
                        // Helper function for safe text extraction
                        function safeText(selector) {
                            const el = document.querySelector(selector);
                            return el ? el.innerText.trim() : '';
                        }
                        
                        // Extract basic profile info
                        const name = safeText('main div[data-testid="primaryColumn"] h2');
                        const handle = safeText('main div[data-testid="primaryColumn"] div[dir="ltr"] span');
                        const bio = safeText('main div[data-testid="primaryColumn"] div[data-testid="UserDescription"]');
                        const location = safeText('main div[data-testid="primaryColumn"] span[data-testid="UserLocation"]');
                        
                        // Get stats
                        const statsContainer = document.querySelector('main div[data-testid="primaryColumn"] div[role="navigation"]');
                        let followingCount = '';
                        let followersCount = '';
                        
                        if (statsContainer) {
                            const statElements = statsContainer.querySelectorAll('span');
                            if (statElements.length >= 2) {
                                followingCount = statElements[0].innerText;
                                followersCount = statElements[1].innerText;
                            }
                        }
                        
                        // Get profile image
                        const profileImg = document.querySelector('main div[data-testid="primaryColumn"] img[src*="profile_images"]');
                        const profileImageUrl = profileImg ? profileImg.src : '';
                        
                        // Check verification
                        const isVerified = Boolean(document.querySelector('main div[data-testid="primaryColumn"] svg[aria-label*="Verified"]'));
                        
                        // Extract joining date if present
                        const joinDate = safeText('main div[data-testid="primaryColumn"] span[data-testid="UserJoinDate"]');
                        
                        return {
                            name: name,
                            handle: handle,
                            bio: bio,
                            location: location,
                            followers: followersCount,
                            following: followingCount,
                            profile_image_url: profileImageUrl,
                            is_verified: isVerified,
                            join_date: joinDate,
                            profile_url: window.location.href
                        };
                    }
                """)
                
                return profile_data
                
            finally:
                await page.close()
                
        except Exception as e:
            logger.error(f"Error getting Twitter profile: {str(e)}")
            return None
    
    def _calculate_confidence(self, query_name: str, result_name: str) -> float:
        """
        Calculate confidence score of Twitter profile match
        
        Args:
            query_name: Name we searched for
            result_name: Name from Twitter profile
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        if not query_name or not result_name:
            return 0.5  # Default moderate confidence
        
        # Clean and normalize names
        query_clean = query_name.lower().strip()
        result_clean = result_name.lower().strip()
        
        # Exact match
        if query_clean == result_clean:
            return 0.95
        
        # Check if first and last name are present
        query_parts = query_clean.split()
        result_parts = result_clean.split()
        
        if len(query_parts) >= 2 and len(result_parts) >= 1:
            # Check if first name matches
            first_match = query_parts[0] == result_parts[0]
            
            # Check if last name is present
            last_match = query_parts[-1] in result_clean
            
            if first_match and last_match:
                return 0.9
            elif first_match:
                return 0.7
            elif last_match:
                return 0.6
        
        # Partial matching
        for part in query_parts:
            if len(part) > 2 and part in result_clean:
                return 0.6
        
        return 0.4  # Low confidence
    
    def _format_twitter_data(self, profile_data: Dict) -> Dict:
        """
        Format Twitter profile data consistently
        
        Args:
            profile_data: Raw profile data
            
        Returns:
            Formatted Twitter data
        """
        return {
            "name": profile_data.get("name", ""),
            "handle": profile_data.get("handle", "").replace("@", ""),
            "bio": profile_data.get("bio", ""),
            "location": profile_data.get("location", ""),
            "followers_count": profile_data.get("followers", ""),
            "following_count": profile_data.get("following", ""),
            "profile_image": profile_data.get("profile_image_url", ""),
            "verified": profile_data.get("is_verified", False),
            "join_date": profile_data.get("join_date", ""),
            "profile_url": profile_data.get("profile_url", "")
        }

================================================================================
FILE: intelligence/predictions/__init__.py
SIZE: 187 bytes
LINES: 6
================================================================================

"""
Predictions Module
=================
Provides predictive intelligence capabilities including relationship trajectories,
project outcomes, opportunity windows, and risk scenarios.
"""

================================================================================
FILE: intelligence/predictions/prediction_engine.py
SIZE: 31,842 bytes
LINES: 828
================================================================================

# File: chief_of_staff_ai/intelligence/predictive_engine.py
"""
Predictive Intelligence Engine
==============================
Analyzes patterns to predict outcomes and generate actionable recommendations
"""

import logging
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import DBSCAN
import asyncio
import json

logger = logging.getLogger(__name__)

@dataclass
class Prediction:
    """Individual prediction with confidence and evidence"""
    prediction_type: str
    subject: str
    outcome: str
    probability: float
    confidence: float
    time_horizon: str
    evidence: List[Dict]
    recommended_actions: List[Dict]
    risk_factors: List[str]

@dataclass
class ActionableInsight:
    """Actionable insight with specific recommendations"""
    insight_type: str
    title: str
    description: str
    urgency: str  # immediate, short_term, long_term
    impact: str   # high, medium, low
    actions: List[Dict]
    expected_outcome: str
    effort_required: str
    dependencies: List[str]

class PredictiveIntelligenceEngine:
    """Main engine for predictive analytics and recommendations"""
    
    def __init__(self, storage_manager, claude_client):
        self.storage = storage_manager
        self.claude = claude_client
        self.models = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize ML models for predictions"""
        # Relationship trajectory model
        self.models['relationship_trajectory'] = RandomForestRegressor(
            n_estimators=100,
            random_state=42
        )
        
        # Opportunity timing model
        self.models['opportunity_timing'] = RandomForestRegressor(
            n_estimators=50,
            random_state=42
        )
        
        # Risk detection model
        self.models['risk_detection'] = DBSCAN(
            eps=0.3,
            min_samples=2
        )
    
    async def generate_predictions(self, user_id: int, knowledge_tree: Dict) -> List[Prediction]:
        """Generate comprehensive predictions based on patterns"""
        predictions = []
        
        # Run different prediction types in parallel
        prediction_tasks = [
            self._predict_relationship_trajectories(user_id, knowledge_tree),
            self._predict_project_outcomes(user_id, knowledge_tree),
            self._predict_opportunity_windows(user_id, knowledge_tree),
            self._predict_risk_scenarios(user_id, knowledge_tree),
            self._predict_optimal_timing(user_id, knowledge_tree)
        ]
        
        results = await asyncio.gather(*prediction_tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                predictions.extend(result)
            else:
                logger.error(f"Prediction error: {result}")
        
        # Rank predictions by confidence and probability
        predictions.sort(
            key=lambda p: (p.confidence * p.probability),
            reverse=True
        )
        
        return predictions[:20]  # Top 20 predictions
    
    async def _predict_relationship_trajectories(self, user_id: int, 
                                               knowledge_tree: Dict) -> List[Prediction]:
        """Predict how relationships will evolve"""
        predictions = []
        
        # Get relationship data
        relationships = knowledge_tree.get('augmented_relationships', [])
        
        for rel in relationships[:10]:  # Top 10 relationships
            # Extract features for prediction
            features = self._extract_relationship_features(rel)
            
            if features:
                # Predict trajectory
                trajectory = self._predict_single_trajectory(features)
                
                # Use Claude for nuanced interpretation
                interpretation = await self._interpret_trajectory(rel, trajectory)
                
                predictions.append(Prediction(
                    prediction_type='relationship_trajectory',
                    subject=self._get_relationship_subject(rel),
                    outcome=interpretation['outcome'],
                    probability=trajectory['probability'],
                    confidence=trajectory['confidence'],
                    time_horizon=interpretation['time_horizon'],
                    evidence=self._gather_trajectory_evidence(rel),
                    recommended_actions=interpretation['actions'],
                    risk_factors=interpretation['risks']
                ))
        
        return predictions
    
    async def _predict_project_outcomes(self, user_id: int, 
                                      knowledge_tree: Dict) -> List[Prediction]:
        """Predict project success and timeline"""
        predictions = []
        
        # Extract project insights from knowledge tree
        project_insights = self._extract_project_insights(knowledge_tree)
        
        for project in project_insights:
            # Analyze project health indicators
            health_score = self._calculate_project_health(project)
            
            # Predict outcome
            outcome_prediction = await self._predict_project_success(project, health_score)
            
            predictions.append(Prediction(
                prediction_type='project_outcome',
                subject=project.get('name', 'Unknown Project'),
                outcome=outcome_prediction['outcome'],
                probability=outcome_prediction['probability'],
                confidence=health_score,
                time_horizon=outcome_prediction['timeline'],
                evidence=project.get('evidence', []),
                recommended_actions=outcome_prediction['actions'],
                risk_factors=outcome_prediction['risks']
            ))
        
        return predictions
    
    async def _predict_opportunity_windows(self, user_id: int, 
                                         knowledge_tree: Dict) -> List[Prediction]:
        """Predict optimal timing for opportunities"""
        predictions = []
        
        # Extract opportunities from insights
        opportunities = self._extract_opportunities(knowledge_tree)
        
        for opp in opportunities:
            # Analyze timing factors
            timing_analysis = await self._analyze_opportunity_timing(opp, knowledge_tree)
            
            if timing_analysis['confidence'] > 0.6:
                predictions.append(Prediction(
                    prediction_type='opportunity_window',
                    subject=opp['description'],
                    outcome=f"Optimal window: {timing_analysis['window']}",
                    probability=timing_analysis['success_probability'],
                    confidence=timing_analysis['confidence'],
                    time_horizon=timing_analysis['window'],
                    evidence=timing_analysis['supporting_evidence'],
                    recommended_actions=timing_analysis['preparation_steps'],
                    risk_factors=timing_analysis['risks']
                ))
        
        return predictions
    
    async def _predict_risk_scenarios(self, user_id: int, 
                                    knowledge_tree: Dict) -> List[Prediction]:
        """Predict potential risks and mitigation strategies"""
        predictions = []
        
        # Extract risk indicators
        risk_indicators = self._extract_risk_indicators(knowledge_tree)
        
        # Cluster similar risks
        if len(risk_indicators) > 2:
            risk_clusters = self._cluster_risks(risk_indicators)
        else:
            risk_clusters = [risk_indicators]
        
        for cluster in risk_clusters:
            # Analyze risk cluster
            risk_analysis = await self._analyze_risk_cluster(cluster)
            
            if risk_analysis['severity'] > 0.5:
                predictions.append(Prediction(
                    prediction_type='risk_scenario',
                    subject=risk_analysis['risk_category'],
                    outcome=risk_analysis['potential_impact'],
                    probability=risk_analysis['likelihood'],
                    confidence=risk_analysis['confidence'],
                    time_horizon=risk_analysis['time_frame'],
                    evidence=risk_analysis['indicators'],
                    recommended_actions=risk_analysis['mitigation_steps'],
                    risk_factors=risk_analysis['escalation_factors']
                ))
        
        return predictions
    
    async def _predict_optimal_timing(self, user_id: int, 
                                    knowledge_tree: Dict) -> List[Prediction]:
        """Predict optimal timing for various actions"""
        predictions = []
        
        # Get pending decisions and actions
        pending_items = self._extract_pending_items(knowledge_tree)
        
        for item in pending_items:
            # Analyze timing factors
            timing_prediction = await self._predict_action_timing(item, knowledge_tree)
            
            predictions.append(Prediction(
                prediction_type='optimal_timing',
                subject=item['description'],
                outcome=f"Best time: {timing_prediction['optimal_time']}",
                probability=timing_prediction['success_rate'],
                confidence=timing_prediction['confidence'],
                time_horizon=timing_prediction['optimal_time'],
                evidence=timing_prediction['timing_factors'],
                recommended_actions=timing_prediction['preparation'],
                risk_factors=timing_prediction['delay_risks']
            ))
        
        return predictions
    
    async def generate_actionable_insights(self, predictions: List[Prediction], 
                                         user_context: Dict) -> List[ActionableInsight]:
        """Convert predictions into actionable insights"""
        insights = []
        
        # Group predictions by urgency
        urgent_predictions = [p for p in predictions if self._is_urgent(p)]
        important_predictions = [p for p in predictions if self._is_important(p)]
        
        # Generate immediate actions
        for pred in urgent_predictions[:5]:
            insight = await self._create_actionable_insight(pred, 'immediate')
            insights.append(insight)
        
        # Generate strategic actions
        for pred in important_predictions[:5]:
            insight = await self._create_actionable_insight(pred, 'strategic')
            insights.append(insight)
        
        # Generate opportunity actions
        opportunity_predictions = [p for p in predictions 
                                 if p.prediction_type == 'opportunity_window']
        for pred in opportunity_predictions[:3]:
            insight = await self._create_actionable_insight(pred, 'opportunity')
            insights.append(insight)
        
        return insights
    
    async def _create_actionable_insight(self, prediction: Prediction, 
                                       insight_category: str) -> ActionableInsight:
        """Create a specific actionable insight from a prediction"""
        # Use Claude to generate actionable steps
        prompt = f"""Based on this prediction, create specific actionable steps:

Prediction: {prediction.outcome}
Subject: {prediction.subject}
Probability: {prediction.probability:.1%}
Evidence: {json.dumps(prediction.evidence[:3])}
Category: {insight_category}

Generate:
1. A clear, actionable title
2. Specific steps to take (3-5)
3. Expected outcome
4. Effort required (hours/days)
5. Any dependencies

Format as JSON."""

        response = await asyncio.to_thread(
            self.claude.messages.create,
            model="claude-3-opus-20240229",
            max_tokens=1000,
            temperature=0.4,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            insight_data = json.loads(response.content[0].text)
            
            return ActionableInsight(
                insight_type=prediction.prediction_type,
                title=insight_data.get('title', f"Action: {prediction.subject}"),
                description=prediction.outcome,
                urgency=self._determine_urgency(prediction, insight_category),
                impact=self._determine_impact(prediction),
                actions=insight_data.get('steps', []),
                expected_outcome=insight_data.get('expected_outcome', ''),
                effort_required=insight_data.get('effort_required', 'Unknown'),
                dependencies=insight_data.get('dependencies', [])
            )
        except:
            # Fallback insight
            return ActionableInsight(
                insight_type=prediction.prediction_type,
                title=f"Action needed: {prediction.subject}",
                description=prediction.outcome,
                urgency='medium',
                impact='medium',
                actions=prediction.recommended_actions,
                expected_outcome="Improved outcomes",
                effort_required="Moderate",
                dependencies=[]
            )
    
    def _extract_relationship_features(self, relationship: Dict) -> Optional[np.ndarray]:
        """Extract features for relationship trajectory prediction"""
        try:
            augmented_data = relationship.get('augmented_data', {})
            
            features = [
                augmented_data.get('communication_count', 0),
                len(augmented_data.get('patterns', [])),
                augmented_data.get('health_score', 0.5),
                self._calculate_momentum(augmented_data.get('evolution', [])),
                self._calculate_engagement_trend(augmented_data.get('patterns', []))
            ]
            
            return np.array(features).reshape(1, -1)
        except:
            return None
    
    def _predict_single_trajectory(self, features: np.ndarray) -> Dict:
        """Predict trajectory for a single relationship"""
        # This would use the trained model in production
        # For now, simulate based on features
        
        health_score = features[0][2]
        momentum = features[0][3]
        
        if health_score > 0.7 and momentum > 0:
            return {
                'direction': 'strengthening',
                'probability': 0.8,
                'confidence': 0.75
            }
        elif health_score < 0.3 or momentum < -0.2:
            return {
                'direction': 'weakening',
                'probability': 0.7,
                'confidence': 0.65
            }
        else:
            return {
                'direction': 'stable',
                'probability': 0.6,
                'confidence': 0.5
            }
    
    async def _interpret_trajectory(self, relationship: Dict, trajectory: Dict) -> Dict:
        """Use Claude to interpret trajectory prediction"""
        prompt = f"""Interpret this relationship trajectory prediction:

Relationship: {self._get_relationship_subject(relationship)}
Predicted Direction: {trajectory['direction']}
Probability: {trajectory['probability']:.1%}

Recent patterns: {relationship.get('augmented_data', {}).get('patterns', [])}

Provide:
1. Natural language outcome description
2. Time horizon (when this will manifest)
3. 3-5 specific recommended actions
4. Risk factors to watch for

Format as JSON."""

        response = await asyncio.to_thread(
            self.claude.messages.create,
            model="claude-3-opus-20240229",
            max_tokens=1000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            return json.loads(response.content[0].text)
        except:
            return {
                'outcome': f"Relationship is {trajectory['direction']}",
                'time_horizon': 'next 30 days',
                'actions': ['Schedule regular check-ins'],
                'risks': ['Communication gaps']
            }
    
    def _get_relationship_subject(self, relationship: Dict) -> str:
        """Extract subject name from relationship data"""
        original = relationship.get('original', {})
        if isinstance(original, dict):
            return original.get('name', 'Unknown Contact')
        return 'Unknown Contact'
    
    def _gather_trajectory_evidence(self, relationship: Dict) -> List[Dict]:
        """Gather evidence supporting trajectory prediction"""
        evidence = []
        augmented = relationship.get('augmented_data', {})
        
        # Communication patterns
        if augmented.get('patterns'):
            evidence.append({
                'type': 'communication_patterns',
                'data': augmented['patterns'][:3]
            })
        
        # Evolution data
        if augmented.get('evolution'):
            evidence.append({
                'type': 'relationship_evolution',
                'data': augmented['evolution'][-3:]
            })
        
        return evidence
    
    def _calculate_momentum(self, evolution: List[Dict]) -> float:
        """Calculate relationship momentum from evolution data"""
        if not evolution or len(evolution) < 2:
            return 0.0
        
        # Simple momentum: change in engagement over time
        recent = evolution[-1].get('evidence_count', 0)
        previous = evolution[-2].get('evidence_count', 0)
        
        return (recent - previous) / max(previous, 1)
    
    def _calculate_engagement_trend(self, patterns: List[Dict]) -> float:
        """Calculate engagement trend from patterns"""
        if not patterns:
            return 0.0
        
        # Count positive vs negative patterns
        positive = sum(1 for p in patterns if 'positive' in str(p).lower())
        negative = sum(1 for p in patterns if 'negative' in str(p).lower())
        
        total = positive + negative
        return (positive - negative) / max(total, 1)
    
    def _extract_project_insights(self, knowledge_tree: Dict) -> List[Dict]:
        """Extract project-related insights from knowledge tree"""
        projects = []
        
        # Look for project insights in different categories
        insights = knowledge_tree.get('insights', {})
        for analyst_type, categories in insights.items():
            if isinstance(categories, dict):
                for category, items in categories.items():
                    if 'project' in category.lower() and isinstance(items, list):
                        projects.extend(items)
        
        return projects
    
    def _calculate_project_health(self, project: Dict) -> float:
        """Calculate project health score"""
        health_indicators = {
            'has_timeline': 0.2,
            'has_milestones': 0.2,
            'has_team': 0.2,
            'recent_progress': 0.2,
            'no_blockers': 0.2
        }
        
        score = 0.0
        project_str = json.dumps(project).lower()
        
        if 'timeline' in project_str or 'deadline' in project_str:
            score += health_indicators['has_timeline']
        if 'milestone' in project_str:
            score += health_indicators['has_milestones']
        if 'team' in project_str or 'assigned' in project_str:
            score += health_indicators['has_team']
        if 'progress' in project_str or 'completed' in project_str:
            score += health_indicators['recent_progress']
        if 'blocker' not in project_str and 'blocked' not in project_str:
            score += health_indicators['no_blockers']
        
        return score
    
    async def _predict_project_success(self, project: Dict, health_score: float) -> Dict:
        """Predict project success probability"""
        # Use Claude for nuanced project analysis
        prompt = f"""Analyze this project and predict its success:

Project: {json.dumps(project)}
Health Score: {health_score:.1%}

Provide:
1. Success probability (0-1)
2. Expected timeline/outcome
3. Key success factors
4. Main risks
5. Recommended actions (3-5)

Format as JSON."""

        response = await asyncio.to_thread(
            self.claude.messages.create,
            model="claude-3-opus-20240229",
            max_tokens=1000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            result = json.loads(response.content[0].text)
            return {
                'outcome': result.get('outcome', 'Likely successful'),
                'probability': result.get('success_probability', health_score),
                'timeline': result.get('timeline', 'Next quarter'),
                'actions': result.get('recommended_actions', []),
                'risks': result.get('risks', [])
            }
        except:
            return {
                'outcome': 'Moderate success likely',
                'probability': health_score,
                'timeline': 'Next quarter',
                'actions': ['Review project milestones'],
                'risks': ['Resource constraints']
            }
    
    def _extract_opportunities(self, knowledge_tree: Dict) -> List[Dict]:
        """Extract opportunities from knowledge tree"""
        opportunities = []
        
        # Look in market intelligence and predictions
        insights = knowledge_tree.get('insights', {})
        
        if 'market_intelligence' in insights:
            market_opps = insights['market_intelligence'].get('opportunities', [])
            opportunities.extend(market_opps)
        
        if 'predictive' in insights:
            predicted_opps = insights['predictive'].get('opportunity_windows', [])
            opportunities.extend(predicted_opps)
        
        return opportunities
    
    async def _analyze_opportunity_timing(self, opportunity: Dict, 
                                        knowledge_tree: Dict) -> Dict:
        """Analyze optimal timing for an opportunity"""
        # Extract timing factors
        timing_factors = []
        
        # Market conditions
        market_insights = knowledge_tree.get('insights', {}).get('market_intelligence', {})
        if market_insights.get('timing_factors'):
            timing_factors.extend(market_insights['timing_factors'])
        
        # Relationship readiness
        relationships = knowledge_tree.get('augmented_relationships', [])
        relevant_relationships = self._find_relevant_relationships(opportunity, relationships)
        
        # Use Claude for timing analysis
        prompt = f"""Analyze the optimal timing for this opportunity:

Opportunity: {json.dumps(opportunity)}
Market Factors: {json.dumps(timing_factors[:3])}
Relationship Readiness: {len(relevant_relationships)} key relationships

Determine:
1. Optimal time window (specific timeframe)
2. Success probability if timed well
3. Confidence in timing prediction
4. Supporting evidence
5. Preparation steps needed
6. Risks of poor timing

Format as JSON."""

        response = await asyncio.to_thread(
            self.claude.messages.create,
            model="claude-3-opus-20240229",
            max_tokens=1000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            return json.loads(response.content[0].text)
        except:
            return {
                'window': 'Next 30-60 days',
                'success_probability': 0.6,
                'confidence': 0.5,
                'supporting_evidence': timing_factors[:3],
                'preparation_steps': ['Prepare proposal'],
                'risks': ['Market timing uncertainty']
            }
    
    def _find_relevant_relationships(self, opportunity: Dict, 
                                   relationships: List[Dict]) -> List[Dict]:
        """Find relationships relevant to an opportunity"""
        relevant = []
        opp_str = json.dumps(opportunity).lower()
        
        for rel in relationships:
            rel_str = json.dumps(rel).lower()
            # Simple keyword matching - enhance with NLP
            if any(keyword in rel_str for keyword in opp_str.split()[:5]):
                relevant.append(rel)
        
        return relevant[:5]
    
    def _extract_risk_indicators(self, knowledge_tree: Dict) -> List[Dict]:
        """Extract risk indicators from knowledge tree"""
        risks = []
        
        # From various insight categories
        insights = knowledge_tree.get('insights', {})
        
        for analyst_type, categories in insights.items():
            if isinstance(categories, dict):
                # Look for risk-related categories
                if 'risks' in categories:
                    risks.extend(categories['risks'])
                if 'risk_indicators' in categories:
                    risks.extend(categories['risk_indicators'])
        
        # From relationship analysis
        for rel in knowledge_tree.get('augmented_relationships', []):
            if rel.get('augmented_data', {}).get('health_score', 1) < 0.3:
                risks.append({
                    'type': 'relationship_risk',
                    'subject': self._get_relationship_subject(rel),
                    'indicator': 'declining health score'
                })
        
        return risks
    
    def _cluster_risks(self, risk_indicators: List[Dict]) -> List[List[Dict]]:
        """Cluster similar risks together"""
        # Simple clustering based on risk type
        clusters = defaultdict(list)
        
        for risk in risk_indicators:
            risk_type = risk.get('type', 'unknown')
            clusters[risk_type].append(risk)
        
        return list(clusters.values())
    
    async def _analyze_risk_cluster(self, cluster: List[Dict]) -> Dict:
        """Analyze a cluster of related risks"""
        if not cluster:
            return {'severity': 0, 'confidence': 0}
        
        # Use Claude for risk analysis
        prompt = f"""Analyze this cluster of related risks:

Risks: {json.dumps(cluster)}

Determine:
1. Risk category/type
2. Potential impact (description)
3. Likelihood (0-1)
4. Severity (0-1)
5. Time frame when risk might materialize
6. Key indicators to watch
7. Mitigation steps (3-5)
8. Escalation factors

Format as JSON."""

        response = await asyncio.to_thread(
            self.claude.messages.create,
            model="claude-3-opus-20240229",
            max_tokens=1000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            result = json.loads(response.content[0].text)
            return {
                'risk_category': result.get('risk_category', 'General Risk'),
                'potential_impact': result.get('potential_impact', 'Moderate impact'),
                'likelihood': result.get('likelihood', 0.5),
                'severity': result.get('severity', 0.5),
                'confidence': 0.7,  # Default confidence
                'time_frame': result.get('time_frame', 'Next quarter'),
                'indicators': cluster,
                'mitigation_steps': result.get('mitigation_steps', []),
                'escalation_factors': result.get('escalation_factors', [])
            }
        except:
            return {
                'risk_category': 'General Risk',
                'potential_impact': 'Potential negative impact',
                'likelihood': 0.5,
                'severity': 0.5,
                'confidence': 0.5,
                'time_frame': 'Near future',
                'indicators': cluster,
                'mitigation_steps': ['Monitor closely'],
                'escalation_factors': []
            }
    
    def _extract_pending_items(self, knowledge_tree: Dict) -> List[Dict]:
        """Extract pending decisions and actions"""
        pending = []
        
        # From insights
        insights = knowledge_tree.get('insights', {})
        for analyst_type, categories in insights.items():
            if isinstance(categories, dict):
                if 'decisions' in categories:
                    pending.extend([d for d in categories['decisions'] 
                                  if 'pending' in str(d).lower()])
                if 'action_required' in categories:
                    pending.extend(categories['action_required'])
        
        return pending
    
    async def _predict_action_timing(self, item: Dict, knowledge_tree: Dict) -> Dict:
        """Predict optimal timing for an action"""
        # Analyze various timing factors
        prompt = f"""Analyze optimal timing for this action:

Action/Decision: {json.dumps(item)}

Consider:
- Current momentum in related areas
- Resource availability
- Market conditions
- Relationship readiness

Provide:
1. Optimal time (specific)
2. Success rate if done at optimal time
3. Confidence in prediction
4. Key timing factors
5. Preparation needed
6. Risks of delay

Format as JSON."""

        response = await asyncio.to_thread(
            self.claude.messages.create,
            model="claude-3-opus-20240229",
            max_tokens=1000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        try:
            return json.loads(response.content[0].text)
        except:
            return {
                'optimal_time': 'Within 2 weeks',
                'success_rate': 0.7,
                'confidence': 0.6,
                'timing_factors': ['Current momentum'],
                'preparation': ['Gather resources'],
                'delay_risks': ['Lost opportunity']
            }
    
    def _is_urgent(self, prediction: Prediction) -> bool:
        """Check if a prediction requires urgent action"""
        urgent_indicators = [
            prediction.time_horizon in ['immediate', 'next 7 days', 'this week'],
            prediction.prediction_type == 'risk_scenario' and prediction.probability > 0.7,
            any('urgent' in action.get('priority', '').lower() 
                for action in prediction.recommended_actions)
        ]
        return any(urgent_indicators)
    
    def _is_important(self, prediction: Prediction) -> bool:
        """Check if a prediction is strategically important"""
        return (
            prediction.confidence > 0.7 and 
            prediction.probability > 0.6 and
            prediction.prediction_type in ['opportunity_window', 'relationship_trajectory']
        )
    
    def _determine_urgency(self, prediction: Prediction, category: str) -> str:
        """Determine urgency level for an insight"""
        if category == 'immediate' or self._is_urgent(prediction):
            return 'immediate'
        elif prediction.time_horizon in ['next 30 days', 'this month']:
            return 'short_term'
        else:
            return 'long_term'
    
    def _determine_impact(self, prediction: Prediction) -> str:
        """Determine impact level for an insight"""
        if prediction.probability > 0.8 and prediction.confidence > 0.7:
            return 'high'
        elif prediction.probability > 0.6 or prediction.confidence > 0.6:
            return 'medium'
        else:
            return 'low'

# Usage example:
# from storage.storage_manager import StorageManager
# from some_claude_client_module import AnthropicClient
# 
# storage_manager = StorageManager()
# claude_client = AnthropicClient()
# predictive_engine = PredictiveIntelligenceEngine(storage_manager, claude_client)

================================================================================
EXTRACTION SUMMARY
================================================================================
Total files extracted: 27
Total lines of code: 10,622
Output file: /Users/oudiantebi/Session42 Dropbox/Oudi Antebi/Mac (3)/Documents/MyCode/COS1/NEW-COS/intelligence_code_extracted.txt

Files included:
  1. intelligence/__init__.py
  2. intelligence/claude_analysis.py
  3. intelligence/claude_intelligent_augmentation.py
  4. intelligence/deep_augmentation.py
  5. intelligence/enrichment_engine.py
  6. intelligence/web_scraper.py
  7. intelligence/calendar/__init__.py
  8. intelligence/calendar/calendar_intelligence.py
  9. intelligence/knowledge_tree/__init__.py
 10. intelligence/knowledge_tree/augmentation_engine.py
 11. intelligence/knowledge_tree/multidimensional_matrix.py
 12. intelligence/knowledge_tree/tree_builder.py
 13. intelligence/analysts/__init__.py
 14. intelligence/analysts/base_analyst.py
 15. intelligence/analysts/business_analyst.py
 16. intelligence/analysts/market_analyst.py
 17. intelligence/analysts/predictive_analyst.py
 18. intelligence/analysts/relationship_analyst.py
 19. intelligence/analysts/technical_analyst.py
 20. intelligence/web_enrichment/__init__.py
 21. intelligence/web_enrichment/base_scraper.py
 22. intelligence/web_enrichment/company_scraper.py
 23. intelligence/web_enrichment/enrichment_orchestrator.py
 24. intelligence/web_enrichment/linkedin_scraper.py
 25. intelligence/web_enrichment/twitter_scraper.py
 26. intelligence/predictions/__init__.py
 27. intelligence/predictions/prediction_engine.py
================================================================================
